[{"id_": "e9c33efd-92d5-410e-b10e-ba9866553abc", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission.", "original_text": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b268d149-3134-412d-87ed-2d3f6929521a", "node_type": "1", "metadata": {"window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. ", "original_text": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. "}, "hash": "5a248640b7f74806fd29898d73cbdf1d909382d5fe7555a33b951f32ec23c8f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b268d149-3134-412d-87ed-2d3f6929521a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. ", "original_text": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9c33efd-92d5-410e-b10e-ba9866553abc", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission.", "original_text": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection. "}, "hash": "474f83288a21c5f5beb4508819d7ab3d474bf8121d960f78a671574df45173a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d3149ff-d436-48c0-a128-9a35a6ab5a5c", "node_type": "1", "metadata": {"window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). ", "original_text": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. "}, "hash": "6e418f08fb18199fd0dc23d1a181ff9014397bca88462fc40ed7d8ee96e47eae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. ", "mimetype": "text/plain", "start_char_idx": 414, "end_char_idx": 584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d3149ff-d436-48c0-a128-9a35a6ab5a5c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). ", "original_text": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b268d149-3134-412d-87ed-2d3f6929521a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. ", "original_text": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. "}, "hash": "bc409e47528319047dfe8a98580ec628d4e5485e4c586899d7594d9d329362b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63d25d4c-813f-4393-bd98-c78829b24068", "node_type": "1", "metadata": {"window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n", "original_text": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes."}, "hash": "33e8d54c2cdb5b9c6bc1b6dbf71742731bc2ee9322be5c6755db949ceddc41a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. ", "mimetype": "text/plain", "start_char_idx": 584, "end_char_idx": 715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63d25d4c-813f-4393-bd98-c78829b24068", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n", "original_text": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d3149ff-d436-48c0-a128-9a35a6ab5a5c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). ", "original_text": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. "}, "hash": "fb24f139aadaddfefed3cd116e7b1b3ab230f4244a6fac6a52440239273f8622", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d47df7e-2929-4845-9897-26e36c922e92", "node_type": "1", "metadata": {"window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . ", "original_text": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. "}, "hash": "6a71ade258242d5fd4d8cf41ac8d81c3ac38d7c57a57888809b980e95be3616f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes.", "mimetype": "text/plain", "start_char_idx": 715, "end_char_idx": 966, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d47df7e-2929-4845-9897-26e36c922e92", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . ", "original_text": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63d25d4c-813f-4393-bd98-c78829b24068", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n", "original_text": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes."}, "hash": "44961ba069ad7e5d2f47376bf73f65b3b69e72935530bb0d1dcd4bf697a02137", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0f69e3c-dfe0-41d2-ab10-e77ee8933f06", "node_type": "1", "metadata": {"window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  . ", "original_text": "The code to reproduce our results is made available as part of the submission."}, "hash": "001670b0091cae674f41aeb6f1223cc1c8ae09977b83c9caba2c97db846f2d2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. ", "mimetype": "text/plain", "start_char_idx": 966, "end_char_idx": 1303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a0f69e3c-dfe0-41d2-ab10-e77ee8933f06", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  . ", "original_text": "The code to reproduce our results is made available as part of the submission."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d47df7e-2929-4845-9897-26e36c922e92", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . ", "original_text": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. "}, "hash": "078a657ee552fd635064469869e5e628ca2269254ec459c13ff5c8a980d13a5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0167d904-ca9c-4956-9cf0-e19fb4bef6f8", "node_type": "1", "metadata": {"window": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  . ", "original_text": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. "}, "hash": "bcac2a952ee50b5e3da450e7686b4d703a1ca7689e6d19e7bc13a60c311e7bb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The code to reproduce our results is made available as part of the submission.", "mimetype": "text/plain", "start_char_idx": 1303, "end_char_idx": 1381, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0167d904-ca9c-4956-9cf0-e19fb4bef6f8", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  . ", "original_text": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0f69e3c-dfe0-41d2-ab10-e77ee8933f06", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "# Distribution and volume based scoring for Isolation Forests\n\n1st Hichem Dhouib\nPorsche Digital GmbH\nBerlin\nhichem.dhouib@porsche.digital\n\n2nd Alissa Wilms\nFreie Universit\u00e4t Berlin\nPorsche Digital GmbH\nBerlin\nalissa.wilms@porsche.digital\n\n3rd Paul Boes\nPorsche Digital GmbH\nBerlin\npaul.boes@porsche.digital\n\n**Abstract\u2014We make two contributions to the *Isolation Forest* method for anomaly and outlier detection.  The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  . ", "original_text": "The code to reproduce our results is made available as part of the submission."}, "hash": "13a871f7a2cffa3e6e7387ceb4eea1f5f629ddbef5266533d0a8274323bbd11f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "497b520e-41ba-4e4d-a476-3c0ad3f02b8d", "node_type": "1", "metadata": {"window": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). ", "original_text": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). "}, "hash": "752fea520df0fadd88dc62d4a6b3f147af89925d5735015803a6f13f129e4188", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. ", "mimetype": "text/plain", "start_char_idx": 1381, "end_char_idx": 1651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "497b520e-41ba-4e4d-a476-3c0ad3f02b8d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). ", "original_text": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0167d904-ca9c-4956-9cf0-e19fb4bef6f8", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators.  This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  . ", "original_text": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies. "}, "hash": "6e55f4da98a8917a6782108a64ed643319c171115d04b4b9a443c858dc8a0759", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6618a002-ea89-405b-af36-1ffec59292e3", "node_type": "1", "metadata": {"window": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). ", "original_text": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n"}, "hash": "11f5e2f01b69a99dfafbb0118de031ea46393f6bd7d966997a53cbb257c73b19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). ", "mimetype": "text/plain", "start_char_idx": 1651, "end_char_idx": 1962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6618a002-ea89-405b-af36-1ffec59292e3", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). ", "original_text": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "497b520e-41ba-4e4d-a476-3c0ad3f02b8d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution.  The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). ", "original_text": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic). "}, "hash": "c7f33ecad1decef777785b49861dfa27e0772b96f299099d647e8ceb1d458f9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50e75e82-1b61-4503-b50a-df2829198e2f", "node_type": "1", "metadata": {"window": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. ", "original_text": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . "}, "hash": "98b5166211dbea1d92f5f09560d796ace8f5caf97f7b43905c78f1f382173da1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n", "mimetype": "text/plain", "start_char_idx": 1962, "end_char_idx": 2268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50e75e82-1b61-4503-b50a-df2829198e2f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. ", "original_text": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6618a002-ea89-405b-af36-1ffec59292e3", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree\u2019s leaf nodes. **\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). ", "original_text": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n"}, "hash": "f5b4a27b8a37c82260ea111e1bb63dd0da42acf67d87a31e8d0a9f84838101de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "369d8837-2b12-45cc-a37a-3882bf89433d", "node_type": "1", "metadata": {"window": "The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. ", "original_text": ". "}, "hash": "359768529ce2f021b330d3deb45a23a167127be8cd3c8da0beaaa45f7ef88df8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . ", "mimetype": "text/plain", "start_char_idx": 2268, "end_char_idx": 2563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "369d8837-2b12-45cc-a37a-3882bf89433d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50e75e82-1b61-4503-b50a-df2829198e2f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n**We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive \u201dADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants.  The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. ", "original_text": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), . "}, "hash": "110efeb92e660c76d714fe9fe42441951d857d5d45cbc5599ddd03bcf744ec08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20d1c67d-4555-4e01-bd80-062165d2fb9d", "node_type": "1", "metadata": {"window": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. ", "original_text": ". "}, "hash": "5452aca486291523fdb7ec66b01d4a2377a9565054d25135b4b3b183c4cb5407", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 2561, "end_char_idx": 2563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20d1c67d-4555-4e01-bd80-062165d2fb9d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "369d8837-2b12-45cc-a37a-3882bf89433d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The code to reproduce our results is made available as part of the submission. **\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. ", "original_text": ". "}, "hash": "d3e630d9482b0ffc8d0aa132d75de49adcc6aa7f6b3ecdc0249dfbc67680cf8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dee250b5-bc2b-48ac-b165-e0d7478b46ba", "node_type": "1", "metadata": {"window": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n", "original_text": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). "}, "hash": "0dc968560b18287a064eaafb9fa4c73eb6e19a0ce0a1ea9d38ff7d6d952f6959", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 2563, "end_char_idx": 2565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dee250b5-bc2b-48ac-b165-e0d7478b46ba", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n", "original_text": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20d1c67d-4555-4e01-bd80-062165d2fb9d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n## I. INTRODUCTION\n\nIsolation Forest (IF) [8], [10] is one of the most commonly used machine learning methods for anomaly detection, due to its combination of short fitting and evaluation times with high performance across a variety of different types of anomalies.  Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. ", "original_text": ". "}, "hash": "9ebabfc52cc1dfc13d53e71870c16c472bee7be99ebfe2c890b92f71d96bc57e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5e7919c-9e3a-4918-a925-85c562bab85f", "node_type": "1", "metadata": {"window": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. ", "original_text": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). "}, "hash": "1fa1abe4d683b5aa0141e6c5d6315505267c45b52fed1af14807864eedc05673", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). ", "mimetype": "text/plain", "start_char_idx": 2567, "end_char_idx": 2603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5e7919c-9e3a-4918-a925-85c562bab85f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. ", "original_text": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dee250b5-bc2b-48ac-b165-e0d7478b46ba", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Abstractly, we can view IF as an ensemble-based anomaly detection method, in which a given (and in general unlabeled) training data set X \u2282 R^d is used to fit an ensemble (E_i)^n_{i=1} of n estimators E_i according to some fitting algorithm that we denote as A (and that in general would be non-deterministic).  This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n", "original_text": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x). "}, "hash": "25ba298cc8b0df6adb8c9e029ef3afcb17f02e8889fdbf074ad0855811692295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd07cd09-2854-4528-815b-31deece69004", "node_type": "1", "metadata": {"window": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection. ", "original_text": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. "}, "hash": "16cf9275b613bf99ede6e128912a8143e393c87e0750ba99dc6d83d0173756d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). ", "mimetype": "text/plain", "start_char_idx": 2603, "end_char_idx": 2742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd07cd09-2854-4528-815b-31deece69004", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection. ", "original_text": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5e7919c-9e3a-4918-a925-85c562bab85f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This ensemble is then used to classify any given point x \u2208 R^d as outlier or inlier based on the following two steps:\n\nFirst, an *anomaly score* is obtained for each estimator based on a given per-estimator scoring function \u03c6 such that \u03c6(E_i, x) \u2208 R is the anomaly score given to x by the i-th estimator.\n\n Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. ", "original_text": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4). "}, "hash": "6099f8342d7130b8a2d392ea655a4d7a06b0a5a0ccf38cdc244e09834ea08951", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "660b21ae-5e87-4d92-86fa-40251b3f86d0", "node_type": "1", "metadata": {"window": ".  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest. ", "original_text": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. "}, "hash": "1d13b4168ead203ddfcc5c700fd2363b0e81d99b30651355e4eca9fb1c190cae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. ", "mimetype": "text/plain", "start_char_idx": 2742, "end_char_idx": 2972, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "660b21ae-5e87-4d92-86fa-40251b3f86d0", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest. ", "original_text": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd07cd09-2854-4528-815b-31deece69004", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Second, the individual scores of the estimators are then aggregated into a final score via an *aggregation function* h: R^n \u2192 R such that the IF algorithm classifies x as an anomaly if\nh(\u03c6(x)) \u2265 \u03c4,\n\nwhere \u03c4 \u2208 R is some threshold that is specified as part of the input and \u03c6 = (\u03c6_1(x), \u03c6_2(x), .  .  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection. ", "original_text": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached. "}, "hash": "07ccd1b89655871ebe40a1be8788566ad2affabd3c37eea16ef038cc98c4e56c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9262061-7a83-4cbf-89d1-21cfa3a50f66", "node_type": "1", "metadata": {"window": ".  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized.", "original_text": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. "}, "hash": "6a70c07ab7cf5b6d828b60134729bf2a197eacf97c1f2e7973f06c9ad58f4fb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. ", "mimetype": "text/plain", "start_char_idx": 2972, "end_char_idx": 3245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9262061-7a83-4cbf-89d1-21cfa3a50f66", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized.", "original_text": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "660b21ae-5e87-4d92-86fa-40251b3f86d0", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  .  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest. ", "original_text": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees. "}, "hash": "a7dff3174ba06e9985e7e1c8f916f7a4f29dd2e99c2f3cef4e5a816fafc310ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b90d92f9-d508-4940-abdf-29d2921e33c2", "node_type": "1", "metadata": {"window": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. ", "original_text": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n"}, "hash": "d878a41917eeec5392bbcc02d2a2656ec117026a1a05a46a935c47d9d698685f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. ", "mimetype": "text/plain", "start_char_idx": 3245, "end_char_idx": 3735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b90d92f9-d508-4940-abdf-29d2921e33c2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. ", "original_text": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9262061-7a83-4cbf-89d1-21cfa3a50f66", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  , \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized.", "original_text": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering. "}, "hash": "363d4e766ee12b2b94be1166f1bb208f4387fda11b0c3fd55e2d7cf04a50dcea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44833dca-d3f7-4586-bb76-31deeaf6bb52", "node_type": "1", "metadata": {"window": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". ", "original_text": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. "}, "hash": "7b916e6571676675749152f3769c63bbb405f6507785bc17970b2105b5cfbafb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n", "mimetype": "text/plain", "start_char_idx": 3735, "end_char_idx": 3854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44833dca-d3f7-4586-bb76-31deeaf6bb52", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". ", "original_text": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b90d92f9-d508-4940-abdf-29d2921e33c2", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ", \u03c6_n(x)) with \u03c6_i(x) := \u03c6(E_i, x).  See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. ", "original_text": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n"}, "hash": "90c30259ac445343ce295ff694512c78c3e97ed15b28963f31f96ade8fd4352b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e406ef66-441a-41f4-b81f-6f809b07e587", "node_type": "1", "metadata": {"window": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". ", "original_text": "The top red boxes indicate the flowchart of an ensemble based anomaly detection. "}, "hash": "38f928fea6d5bb91935b7232eb01227c7646b69a031ea1f1215aff0c060cb218", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. ", "mimetype": "text/plain", "start_char_idx": 3854, "end_char_idx": 4121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e406ef66-441a-41f4-b81f-6f809b07e587", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". ", "original_text": "The top red boxes indicate the flowchart of an ensemble based anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44833dca-d3f7-4586-bb76-31deeaf6bb52", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "See Fig.1 for an overview of this process.^1\n\nWe can hence specify any ensemble-based anomaly detection method by a tuple (n, A, \u03c6, h, \u03c4).  In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". ", "original_text": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework. "}, "hash": "5de0ef9067f512f5dab7e49555ecc15c2164a45ce481cdf04e0b99bd4ae79d28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "734fd8d9-8530-486a-a030-ebc676bdb54d", "node_type": "1", "metadata": {"window": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". ", "original_text": "The grey boxes underneath show the specifics of the Isolation Forest. "}, "hash": "aebadbf7d232dd4a542965efc42c35bc2430b787b68db0bee97b47d5fb823028", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top red boxes indicate the flowchart of an ensemble based anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 4121, "end_char_idx": 4202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "734fd8d9-8530-486a-a030-ebc676bdb54d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". ", "original_text": "The grey boxes underneath show the specifics of the Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e406ef66-441a-41f4-b81f-6f809b07e587", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In the specific case of the IF, the fitting algorithm A_IF fits a random forest, in which each tree is grown randomly until either a leaf node contains only a single point in a subsampling set Y \u2286 X or a maximal depth is reached.  The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". ", "original_text": "The top red boxes indicate the flowchart of an ensemble based anomaly detection. "}, "hash": "bcb7efd7b4b1e346929e8395f42f48d70c70c4f044c31d6c5fd4219eee0db82f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6c17ab2-e9ec-4354-877d-2b66cd5d2b4d", "node_type": "1", "metadata": {"window": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes.", "original_text": "At the bottom in white with red border are the contributions of this paper visualized."}, "hash": "a7de0f3aeabc700dbfc69c473839a459285c687c264b4b2a6b1627169b4ab3ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The grey boxes underneath show the specifics of the Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 4202, "end_char_idx": 4272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6c17ab2-e9ec-4354-877d-2b66cd5d2b4d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes.", "original_text": "At the bottom in white with red border are the contributions of this paper visualized."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "734fd8d9-8530-486a-a030-ebc676bdb54d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The per-estimator scoring function \u03c6^IF is then defined such that\n\n\u03c6^IF(E_i, x) = d_i(x) / c_{|Y|} ,\n\nwhere d_i(x) equals the depth of the leaf node associated to point x and c_{|Y|} is a constant that only depends on the size of the subsampling set used to fit the trees.  Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". ", "original_text": "The grey boxes underneath show the specifics of the Isolation Forest. "}, "hash": "2611aa5093f7a819c07a85dfdeac3bced84dce38257a20f8b3090b477882bdbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d7bce4a-5bc3-4093-8b5a-7f588a188ce7", "node_type": "1", "metadata": {"window": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. ", "original_text": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. "}, "hash": "8094598ee180c627de541cc0ce9a76611de465315c8eaad76785ad40e86f6dc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the bottom in white with red border are the contributions of this paper visualized.", "mimetype": "text/plain", "start_char_idx": 4272, "end_char_idx": 4358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d7bce4a-5bc3-4093-8b5a-7f588a188ce7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. ", "original_text": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6c17ab2-e9ec-4354-877d-2b66cd5d2b4d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Finally, the aggregation function h is defined as^2\n\nh_{IF}(x) = 2^{\u2212 \\frac{\\sum_{i=1}^n \\phi_i(x)}{n}} ,\n\nthat is, the aggregate score is simply the sample mean of the per-estimator scores combined with an outer \u201cnormalization\u201d function that ensures the score lies in the interval [0, 1], anomalies correspond to points with higher anomaly score, and the\n\n---\n\u00b9In general, the co-domains of both of these functions could be higher-dimensional, as could be \u03c4, with an appropriate ordering.  However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes.", "original_text": "At the bottom in white with red border are the contributions of this paper visualized."}, "hash": "9bdd418e3493cf956c2ecb5be148aac3890ba8d354aaa98903d5fea3afa55806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8ba89c4-6c92-4566-9660-b36c9255242e", "node_type": "1", "metadata": {"window": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n", "original_text": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". "}, "hash": "28a61e3d48ee848fc1d9b3ddca22b0792f8f980f0116c9c4428488488e126ea2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 4358, "end_char_idx": 4476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8ba89c4-6c92-4566-9660-b36c9255242e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n", "original_text": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d7bce4a-5bc3-4093-8b5a-7f588a188ce7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "However, for the sake of simplicity, we limit our presentation to real-valued scoring and aggregation functions only.\n\n \u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. ", "original_text": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection. "}, "hash": "62b40073b0c2945998ac4a53b6b8680df949dc8aae919f76f885923e7f51f460", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f633c69-3976-4379-9989-b6005b190bc1", "node_type": "1", "metadata": {"window": "The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. ", "original_text": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". "}, "hash": "7106aa5cc4dde62c1c4ea9a6996fe8f2d6f94d1e8542902c1045729389d8aad8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". ", "mimetype": "text/plain", "start_char_idx": 4476, "end_char_idx": 4730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f633c69-3976-4379-9989-b6005b190bc1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. ", "original_text": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8ba89c4-6c92-4566-9660-b36c9255242e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "\u00b2The authors in [8] refer to h_{IF} as the \"scoring function\", however, given the distinction between the scoring and the aggregation step that we make in this paper we felt the need to move away from that terminology\n\n---\n\n***\n**Figure 1: Overview of our framework.  The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n", "original_text": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\". "}, "hash": "d113d874f91f9caba9e455a3a20f0f53df5317d4cadb04c0e8f7d1c9610e563a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "033c22d9-35d7-420b-ae57-c4524d063c45", "node_type": "1", "metadata": {"window": "The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. ", "original_text": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". "}, "hash": "22039dfd4aba273b26bc1e16332d829dc1a34b6270b9f02086668f023cd80142", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". ", "mimetype": "text/plain", "start_char_idx": 4730, "end_char_idx": 4928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "033c22d9-35d7-420b-ae57-c4524d063c45", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. ", "original_text": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f633c69-3976-4379-9989-b6005b190bc1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The top red boxes indicate the flowchart of an ensemble based anomaly detection.  The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. ", "original_text": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\". "}, "hash": "bc5ca0d1e90cc1c940c9aa478f42af04c47556235fb6e80be06e8c288d3eaafa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6051fae-088b-4763-a4c0-aebefe88742c", "node_type": "1", "metadata": {"window": "At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. ", "original_text": "Arrows connect the boxes to show the flow of information and processes."}, "hash": "b1fd7104591e65dca5a6b183fd34d7c11bdd312e4826792d173077083edb7328", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". ", "mimetype": "text/plain", "start_char_idx": 4928, "end_char_idx": 5084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6051fae-088b-4763-a4c0-aebefe88742c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. ", "original_text": "Arrows connect the boxes to show the flow of information and processes."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "033c22d9-35d7-420b-ae57-c4524d063c45", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The grey boxes underneath show the specifics of the Isolation Forest.  At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. ", "original_text": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\". "}, "hash": "a95ae0d589917e6ad800977116848015fbd2981eff433883cd324423ee1f70fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d075c0e-2f58-4476-a3c4-a2418d395d88", "node_type": "1", "metadata": {"window": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n", "original_text": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. "}, "hash": "835807925c2d4e092e0b3dfb37e1d8d4cf6bd71c5e4ef270fa2a8776c1b31a65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Arrows connect the boxes to show the flow of information and processes.", "mimetype": "text/plain", "start_char_idx": 5084, "end_char_idx": 5155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d075c0e-2f58-4476-a3c4-a2418d395d88", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n", "original_text": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6051fae-088b-4763-a4c0-aebefe88742c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "At the bottom in white with red border are the contributions of this paper visualized. **\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. ", "original_text": "Arrows connect the boxes to show the flow of information and processes."}, "hash": "b06496f5c512d7eb694d02e4c43cb0ad7301832763e33f276acb65aa79bbc695", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad3bfdcb-45e3-437a-8b3d-1e06c4847e2e", "node_type": "1", "metadata": {"window": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. ", "original_text": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n"}, "hash": "dfa7d2b13511c717c3cf615492c8fdfb5c0d8dcd8e4bb0d0d640ecd30c248f94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. ", "mimetype": "text/plain", "start_char_idx": 5155, "end_char_idx": 5241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad3bfdcb-45e3-437a-8b3d-1e06c4847e2e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. ", "original_text": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d075c0e-2f58-4476-a3c4-a2418d395d88", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n*Figure 1 Description: The figure shows a flowchart illustrating the process of ensemble-based anomaly detection.  The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n", "original_text": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF. "}, "hash": "ec595fd9a3012d8236afe431e4987dade73def619de442bd91086d908fa71c42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e85c0672-7e2f-471c-9c87-c3b028d01d86", "node_type": "1", "metadata": {"window": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. ", "original_text": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. "}, "hash": "f9663ad0cab4eb13f2f49bcb52c7b68e8c0a6ff4af26d5f43641bafd60d01dda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n", "mimetype": "text/plain", "start_char_idx": 5241, "end_char_idx": 5397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e85c0672-7e2f-471c-9c87-c3b028d01d86", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. ", "original_text": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad3bfdcb-45e3-437a-8b3d-1e06c4847e2e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The top row consists of red boxes with white text, showing the general workflow: \"Ensemble based AD\" leads to \"Raw data X\", which goes to \"Fitting Algorithm A\", then \"Scoring function \u03c6\", \"Aggregation function h\", and finally a \"Prediction h(\u03c6(x)) \u2265 \u03c4\".  The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. ", "original_text": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n"}, "hash": "c95420e14921988761a383be08dc34267948797b16ef3299f2be2b011cca8eae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28276f56-469d-4007-8adc-368d1a2504da", "node_type": "1", "metadata": {"window": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees. ", "original_text": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. "}, "hash": "2f57975f96954c9c7354d3908af9e139819b2c32d2c73fbd58f0185ad3b2dd77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. ", "mimetype": "text/plain", "start_char_idx": 5397, "end_char_idx": 5501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28276f56-469d-4007-8adc-368d1a2504da", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees. ", "original_text": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e85c0672-7e2f-471c-9c87-c3b028d01d86", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The second row, in grey boxes, specifies the components for the standard \"Isolation Forest\": The fitting algorithm is \"A_IF\", the scoring function is \"\u03c6^IF\", and the aggregation function is \"h_IF\".  The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. ", "original_text": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method. "}, "hash": "cb4c381de9c21b5a0fca682449400185dbf2a825154c6ac20f9f406aa9cf4b62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66d56be-48dc-4252-a842-c9d7ab4fcc2a", "node_type": "1", "metadata": {"window": "Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. ", "original_text": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. "}, "hash": "ddfcd30fff01cc8ca124daa06ff2ce4bee2e0ed7f0e99be194dc77694573f907", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. ", "mimetype": "text/plain", "start_char_idx": 5501, "end_char_idx": 5815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f66d56be-48dc-4252-a842-c9d7ab4fcc2a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. ", "original_text": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28276f56-469d-4007-8adc-368d1a2504da", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The bottom row, in white boxes with red borders, indicates the contributions of \"This paper\": a scoring function \"\u03c6^PAC\" and an aggregation function \"h_\u03b1\".  Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees. ", "original_text": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case. "}, "hash": "27497691cb3008ed26b5521c4fbd9dde15c9286eb2f89a108290933bbb2a7cac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f4968f6-2a9a-4c42-809b-ab00ffafc1bd", "node_type": "1", "metadata": {"window": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. ", "original_text": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n"}, "hash": "943b365c15172fbfcffb8df17caffc6ef3cca122da451f12af5f7a8b98a6cdd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. ", "mimetype": "text/plain", "start_char_idx": 5815, "end_char_idx": 6077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f4968f6-2a9a-4c42-809b-ab00ffafc1bd", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. ", "original_text": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f66d56be-48dc-4252-a842-c9d7ab4fcc2a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Arrows connect the boxes to show the flow of information and processes. *\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. ", "original_text": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3]. "}, "hash": "f0276653780fff3daaeefc3dace4132242e5b88dc98d87af9f97799bf072e075", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "053668fa-fd5f-4f93-a5ae-d971b10e108e", "node_type": "1", "metadata": {"window": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. ", "original_text": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. "}, "hash": "6b26cffc6f08017350d08c84d69b8170b76454d1f92c97f58ff3b6ef727106dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 6077, "end_char_idx": 6378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "053668fa-fd5f-4f93-a5ae-d971b10e108e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. ", "original_text": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f4968f6-2a9a-4c42-809b-ab00ffafc1bd", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\npoints are more evenly distributed than under the logarithmic scaling of \u03c6^IF.  In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. ", "original_text": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n"}, "hash": "baa603bfc34d7de11b4c1da8a4e78337c654f0175ee272a41e7ac422bdbf1e08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d2a6b8-7bd7-4b61-8e78-0c87486b9c2c", "node_type": "1", "metadata": {"window": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. ", "original_text": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. "}, "hash": "662ac7baaa284a4d8f1384bea1670e2f1a1950ef2fd98c6cef2284454dcad2c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. ", "mimetype": "text/plain", "start_char_idx": 6378, "end_char_idx": 6526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18d2a6b8-7bd7-4b61-8e78-0c87486b9c2c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. ", "original_text": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "053668fa-fd5f-4f93-a5ae-d971b10e108e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In terms of the above tuple notation, IF is then specified as\n\nIF = (n, A_{IF}, \u03c6^{IF}, h_{IF}, \u03c4),\n\nwhere we leave the specification of n and \u03c4 implicit.\n\n ### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. ", "original_text": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm. "}, "hash": "de79586055e830763edb975b7c831f1e668e74dd63ce091d80b5cc4df83918ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c2180f6-f944-4e3b-80ea-fecd11a29a85", "node_type": "1", "metadata": {"window": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data. ", "original_text": "Among the disadvantages is the random choice of the split value used in the construction of the trees. "}, "hash": "34109b30cc5a93d50cbb4ecaf248c43690edbe077ade42411a678e2b34fed6ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. ", "mimetype": "text/plain", "start_char_idx": 6526, "end_char_idx": 6633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c2180f6-f944-4e3b-80ea-fecd11a29a85", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data. ", "original_text": "Among the disadvantages is the random choice of the split value used in the construction of the trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d2a6b8-7bd7-4b61-8e78-0c87486b9c2c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### A. Contributions\n\nIn this paper, we contribute two logically independent variants of the IF method.  In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. ", "original_text": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance. "}, "hash": "0ab57205deeafe093ac354244d088529e276a321f4aa0f58459f725e67cb13be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b03e1f-f6ac-47f5-bb64-018d98b1c92c", "node_type": "1", "metadata": {"window": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores. ", "original_text": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. "}, "hash": "d3feb901c46163c15491d568188d69bc42885e3176a9fc346b496d5cf6fe8354", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among the disadvantages is the random choice of the split value used in the construction of the trees. ", "mimetype": "text/plain", "start_char_idx": 6633, "end_char_idx": 6736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10b03e1f-f6ac-47f5-bb64-018d98b1c92c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores. ", "original_text": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c2180f6-f944-4e3b-80ea-fecd11a29a85", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In particular, while we don\u2019t touch the fitting routing A_{IF} (as the core of the IF algorithm), we study a different per-estimator scoring function \u03c6^{PAC} that is inspired by computational learning theory and also introduce a parametrized family of aggregation functions h_\u03b1 that have h_{IF} as a special case.  We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data. ", "original_text": "Among the disadvantages is the random choice of the split value used in the construction of the trees. "}, "hash": "5138a2d6675fccef86695b6910a1cd0b81c9287d9ecf597e3e6e07fa54ef1718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a39a6cc6-f1c0-4878-9570-c89b7d60b926", "node_type": "1", "metadata": {"window": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n", "original_text": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. "}, "hash": "a69104d51e36efea62a88ca29112df47404f6e09b99a67684c469c4c3a60a51c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. ", "mimetype": "text/plain", "start_char_idx": 6736, "end_char_idx": 6861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a39a6cc6-f1c0-4878-9570-c89b7d60b926", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n", "original_text": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b03e1f-f6ac-47f5-bb64-018d98b1c92c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We motivate the use of these variants for anomaly detection by evaluating them both on generated toy datasets, to exemplify their strengths compared to the \u201cplain vanilla\u201d IF, as well as on 34 datasets taken from the recent and exhaustive ADBench benchmark [3].  We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores. ", "original_text": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies. "}, "hash": "594f0512dea7f957b233ca1a95939902e42070d64606cb1ade732d9374a2e204", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9eacc859-eb5d-4c53-88bc-b7580a63eef6", "node_type": "1", "metadata": {"window": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. ", "original_text": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. "}, "hash": "1371bc17cb813da2bd752db5f18fbc43e69d3416c8106b013a832afd5a4bd88e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. ", "mimetype": "text/plain", "start_char_idx": 6861, "end_char_idx": 6968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9eacc859-eb5d-4c53-88bc-b7580a63eef6", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. ", "original_text": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a39a6cc6-f1c0-4878-9570-c89b7d60b926", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We find that our the variant based on the family h_\u03b1 slightly outperforms the standard IF on average, while the variant based on the scoring function \u03c6_{PAC} performs worse on average but outperforms IF and all other unsupervised anomaly detection methods benchmarked in [3] on 6 out of 34 datasets.\n\n ### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n", "original_text": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper. "}, "hash": "1f5a2f9554a5c0c261560e9c96bd7d4d79e01aaa7b2962c2ef8938a8b8fce434", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ea4db27-163d-496f-a9f0-11393ad3335e", "node_type": "1", "metadata": {"window": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. ", "original_text": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. "}, "hash": "90d9031eaedaa9d56558737063d7b1bc4b5faac52f1a6083f49973c88257f224", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. ", "mimetype": "text/plain", "start_char_idx": 6968, "end_char_idx": 7366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ea4db27-163d-496f-a9f0-11393ad3335e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. ", "original_text": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9eacc859-eb5d-4c53-88bc-b7580a63eef6", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### B. Related Work\n\n1) *Generalised Isolation Forest:* The original Isolation Forest is considered a state-of-the-art anomaly detection algorithm.  However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. ", "original_text": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality. "}, "hash": "c07849881a77f5430de0a5b4fd28d973979d7f31cac55864e71b3b0ebe12013c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8578e016-a3e0-4a54-96c2-5b023839a5e3", "node_type": "1", "metadata": {"window": "Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. ", "original_text": "In SCiForest [9] a sliding window approach is introduced to process stream data. "}, "hash": "b9ac2fd403805392d9682f98f4655634a6578c02acbc49883d1356688d9f716b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. ", "mimetype": "text/plain", "start_char_idx": 7366, "end_char_idx": 7587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8578e016-a3e0-4a54-96c2-5b023839a5e3", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. ", "original_text": "In SCiForest [9] a sliding window approach is introduced to process stream data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ea4db27-163d-496f-a9f0-11393ad3335e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "However, in some cases the limits of this algorithm are reflected in a poor anomaly detection performance.  Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. ", "original_text": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types. "}, "hash": "7e9ff00ce85866ae7e00ec67b678edfa2738b8bfbaae6b451c472662d951b007", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "364d3679-86f4-4f11-89a8-9742c581cff2", "node_type": "1", "metadata": {"window": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. ", "original_text": "In [11] different aggregation function are used to combine the tree scores. "}, "hash": "525d49bcabbd9387ceeb94c5e8a779ede69d619f6f8d7d7812435412525b4c59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In SCiForest [9] a sliding window approach is introduced to process stream data. ", "mimetype": "text/plain", "start_char_idx": 7587, "end_char_idx": 7668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "364d3679-86f4-4f11-89a8-9742c581cff2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. ", "original_text": "In [11] different aggregation function are used to combine the tree scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8578e016-a3e0-4a54-96c2-5b023839a5e3", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Among the disadvantages is the random choice of the split value used in the construction of the trees.  The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. ", "original_text": "In SCiForest [9] a sliding window approach is introduced to process stream data. "}, "hash": "f291a52335c1e92ba2ff260867d92ab5816abe9312d9e5a1a0e9afbe1b8b19f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae47fc15-8f1c-4d24-8120-67b8d9020191", "node_type": "1", "metadata": {"window": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n", "original_text": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n"}, "hash": "40d8f1b13b47e4be84984e015d894458edada2c5117d47b83e43e1e2fa47fc77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [11] different aggregation function are used to combine the tree scores. ", "mimetype": "text/plain", "start_char_idx": 7668, "end_char_idx": 7744, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae47fc15-8f1c-4d24-8120-67b8d9020191", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n", "original_text": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "364d3679-86f4-4f11-89a8-9742c581cff2", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The performance of this algorithm also suffers when it comes to dealing with high-dimensional data or clusters of anomalies.  There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. ", "original_text": "In [11] different aggregation function are used to combine the tree scores. "}, "hash": "53638104b8f64be29d2f8a3887993f2e67b16ac6e55ef9f197a475b10357eda3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d314ae62-5470-4dfc-bf22-3194991a0989", "node_type": "1", "metadata": {"window": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. ", "original_text": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. "}, "hash": "98610ae6566fa0a4e5bfd4c4f1244ab049f94b0f72316d6b75767b4702b6e564", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n", "mimetype": "text/plain", "start_char_idx": 7744, "end_char_idx": 7891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d314ae62-5470-4dfc-bf22-3194991a0989", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. ", "original_text": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae47fc15-8f1c-4d24-8120-67b8d9020191", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "There exist several IF generalisations which tackle the weaknesses of the original Isolation Forest paper.  In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n", "original_text": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n"}, "hash": "89f488ce65521561a58704a3bfdf0bd5d323432521cc807d42997d49c1ee8938", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a42c996a-14ff-4c08-93d9-331068da0153", "node_type": "1", "metadata": {"window": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. ", "original_text": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. "}, "hash": "aeff3b0627bcc420bced1205b730accf058b0386f41ef9de6687ba6d64ba77f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. ", "mimetype": "text/plain", "start_char_idx": 7891, "end_char_idx": 8112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a42c996a-14ff-4c08-93d9-331068da0153", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. ", "original_text": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d314ae62-5470-4dfc-bf22-3194991a0989", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Deep Isolation Forest [19] a deep neural network architecture is used, in the HDoutliers algorithm [16], extreme value theory is used to calculate a more suitable threshold for the anomalies and in PIDForest [2] a feature selection mechanism is introduced to identify the most relevant features to improve the performance on high dimensional data, which is known as the curse of dimensionality.  In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. ", "original_text": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15]. "}, "hash": "8c13a40c742ebaa32fe3d2f7a5f36f4e21d8fbefd9c9cfa79f4016f45bd2aaad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60b34ec7-a8c4-44ae-9df9-ee94f37eaaff", "node_type": "1", "metadata": {"window": "In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections. ", "original_text": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. "}, "hash": "1579dfa89af86577c1f8befb7d080043571ff60aa23fc67787504872350e2603", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. ", "mimetype": "text/plain", "start_char_idx": 8112, "end_char_idx": 8340, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60b34ec7-a8c4-44ae-9df9-ee94f37eaaff", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections. ", "original_text": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a42c996a-14ff-4c08-93d9-331068da0153", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Extended Isolation Forest [4] and Generalized IF [6] feature subsetting and categorical feature handling mechanisms, and different distance measures were added to tackle high dimensional datasets and mixed data types.  In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. ", "original_text": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5]. "}, "hash": "8d834efa9d65c74d9a697e297f10b4be32fcead5b3cb37d753001f1f004ee189", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5633a4c-6a17-4cc7-be6f-0219fef73098", "node_type": "1", "metadata": {"window": "In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. ", "original_text": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. "}, "hash": "f770b3bc826ff39282de7243fd451bf9496c02cc2e2c649cdfd0cd27aa137091", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 8340, "end_char_idx": 8446, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5633a4c-6a17-4cc7-be6f-0219fef73098", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. ", "original_text": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b34ec7-a8c4-44ae-9df9-ee94f37eaaff", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In SCiForest [9] a sliding window approach is introduced to process stream data.  In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections. ", "original_text": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection. "}, "hash": "ffe7b1fab585a74cd2f8bcbbc2231c48acaf700016d74e6194f650f6f1dcce42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d2e3e12-7bc9-4c88-b7ee-dbb513bc587d", "node_type": "1", "metadata": {"window": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n", "original_text": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n"}, "hash": "948022a60a42e8fe1d7ce6bd863ddf691dd13d8f90b22608148b684b8277dfd4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. ", "mimetype": "text/plain", "start_char_idx": 8446, "end_char_idx": 8568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d2e3e12-7bc9-4c88-b7ee-dbb513bc587d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n", "original_text": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5633a4c-6a17-4cc7-be6f-0219fef73098", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In [11] different aggregation function are used to combine the tree scores.  In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. ", "original_text": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible. "}, "hash": "7d59a387d7264595cdceed8254068d29ce843724bf3427205e407669dd2d5ca5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7ecb80c-3137-42a3-8628-4f5e0342b07e", "node_type": "1", "metadata": {"window": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. ", "original_text": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. "}, "hash": "9a0545f538dc0387e117b31af945d0e71d00013e6dc56acb08f3b375151ef6fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n", "mimetype": "text/plain", "start_char_idx": 8568, "end_char_idx": 8662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7ecb80c-3137-42a3-8628-4f5e0342b07e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. ", "original_text": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d2e3e12-7bc9-4c88-b7ee-dbb513bc587d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In [17] a probabilistic mechanism is used that allows to choose better split values to detect anomalies hidden between clusters more effectively.\n\n 2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n", "original_text": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n"}, "hash": "b63d1570693c1536852c9457d84ac58f89c831e2e10f4e003066d3e13aee9ec6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "354f1dcb-5082-42c6-9a5f-54ae56fd3f66", "node_type": "1", "metadata": {"window": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. ", "original_text": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. "}, "hash": "b0f8b6a93bf280a842ad8f6554eb4120befea6fcd36c0419f0ff2790d2fb6aa8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. ", "mimetype": "text/plain", "start_char_idx": 8662, "end_char_idx": 8746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "354f1dcb-5082-42c6-9a5f-54ae56fd3f66", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. ", "original_text": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7ecb80c-3137-42a3-8628-4f5e0342b07e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2) *Computational learning for anomaly detection:* Many recent works around PAC (Probably Approximately Correct) and anomaly detection have tried to find guarantees for semi-supervised [7] and unsupervised learning [15].  This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. ", "original_text": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II. "}, "hash": "1651983a39c9690096b022e918e1aab0d853c468d09effba737b77c9636b2d4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62c62694-5112-4f6f-a73f-005bc7d658f1", "node_type": "1", "metadata": {"window": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n", "original_text": "As they are logically independent, we introduce them in separate subsections. "}, "hash": "8d777b3d080260312ba75432382dfea449c8c0ef1841ccf803fbc6458afcfea0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. ", "mimetype": "text/plain", "start_char_idx": 8746, "end_char_idx": 8941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62c62694-5112-4f6f-a73f-005bc7d658f1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n", "original_text": "As they are logically independent, we introduce them in separate subsections. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "354f1dcb-5082-42c6-9a5f-54ae56fd3f66", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This research direction is motivated by the need for guarantees in order to bring learning tasks into (safety critical) real world applications like autonomous driving [1], manufacturing [12] and internet of medical things [5].  This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. ", "original_text": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods. "}, "hash": "0ae521355e9b7bd7e8ec09bc5a19ade67c15c3b6ba052ff28edc4269c3b7ebd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b0c6670-5ce9-4af0-8420-d333fdf923f5", "node_type": "1", "metadata": {"window": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . ", "original_text": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. "}, "hash": "e046678abc8ae6a516e8e734a86621f52da50b1886086d3aae191caaf4419cf2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As they are logically independent, we introduce them in separate subsections. ", "mimetype": "text/plain", "start_char_idx": 8941, "end_char_idx": 9019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b0c6670-5ce9-4af0-8420-d333fdf923f5", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . ", "original_text": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62c62694-5112-4f6f-a73f-005bc7d658f1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This work [15] investigates the sample complexity, in the PAC context, of unsupervised anomaly detection.  We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n", "original_text": "As they are logically independent, we introduce them in separate subsections. "}, "hash": "4c2d5b71849a8ce8621a49668bdcb8b37e4451c912b78530a8f3f10a470ae4dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7d8b200-2924-46e8-bd44-e750a0d35ddc", "node_type": "1", "metadata": {"window": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  . ", "original_text": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n"}, "hash": "04c62a22e7624ee8e6c121c81b27a9e115c40ed79a154880c162f85f9137516a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. ", "mimetype": "text/plain", "start_char_idx": 9019, "end_char_idx": 9161, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d7d8b200-2924-46e8-bd44-e750a0d35ddc", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  . ", "original_text": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b0c6670-5ce9-4af0-8420-d333fdf923f5", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We re-implemented and generalized their PAC motivated algorithm, since their experimental results where not reproducible.  This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . ", "original_text": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets. "}, "hash": "8e589dc082b2740361720536288296733fee51911b3590cf04a6d696026a36a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d07c3412-d0c1-4a43-b6dd-ed0b886d988c", "node_type": "1", "metadata": {"window": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  . ", "original_text": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. "}, "hash": "db2cdf74d299aa7045abd621733e520e2da016b3f7d339983f7431329773caba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 9161, "end_char_idx": 9234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d07c3412-d0c1-4a43-b6dd-ed0b886d988c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  . ", "original_text": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7d8b200-2924-46e8-bd44-e750a0d35ddc", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This generalization is practically motivated and might not inherit their theoretical proofs.\n\n ### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  . ", "original_text": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n"}, "hash": "3a72369265843abf37ce72ce24673801d45f3ab231572737a4acc131f21ce89f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a297f98-0f23-4af5-b6df-c90aeedc9764", "node_type": "1", "metadata": {"window": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension. ", "original_text": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. "}, "hash": "67a66b0c42c5a31dc54e68e71900feddde3eddd5748d76bfa20f6912f70e87aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. ", "mimetype": "text/plain", "start_char_idx": 9234, "end_char_idx": 9512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a297f98-0f23-4af5-b6df-c90aeedc9764", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension. ", "original_text": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d07c3412-d0c1-4a43-b6dd-ed0b886d988c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### C. Code\n\nWe make all code required to reproduce the results available.\u00b3\n\n## II.  METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  . ", "original_text": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point. "}, "hash": "670c41820f69134978469b07cdce3288e90f9c907058408ce080ef6ae9c9e5dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f18c7c5f-d30a-4e99-b8e8-fd28583f2b86", "node_type": "1", "metadata": {"window": "As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. ", "original_text": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n"}, "hash": "f4651da0b3060c961c3e13f22589e2bc5106fbcf34dcc8081e871d18d714dae0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. ", "mimetype": "text/plain", "start_char_idx": 9512, "end_char_idx": 9590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f18c7c5f-d30a-4e99-b8e8-fd28583f2b86", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. ", "original_text": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a297f98-0f23-4af5-b6df-c90aeedc9764", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "METHOD\n\nIn this section, we introduce the two main contributions of this work, a novel per-estimator score function and a novel aggregation function for ensemble-based anomaly detection methods.  As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension. ", "original_text": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate. "}, "hash": "09a8d2f93f0c5a7da57d476646a5680749a7d65afb1d899182750db248508b65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ba2af71-2a67-44a7-a347-540a20410b43", "node_type": "1", "metadata": {"window": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. ", "original_text": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . "}, "hash": "9b5a380ec673baead8db9616eed7cca69b9c5b78d85f9f72cf67a78a8797f45e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n", "mimetype": "text/plain", "start_char_idx": 9590, "end_char_idx": 9796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ba2af71-2a67-44a7-a347-540a20410b43", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. ", "original_text": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f18c7c5f-d30a-4e99-b8e8-fd28583f2b86", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As they are logically independent, we introduce them in separate subsections.  In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. ", "original_text": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n"}, "hash": "62cad33773e7bfef1cefcd3dc769fa5ab379223b5d9836e7bc848fd1fb3c2717", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92175fd8-0283-409b-9b99-4cc5ff7f9903", "node_type": "1", "metadata": {"window": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. ", "original_text": ". "}, "hash": "9f8f1ed7a31590c9a3630429992d76148432c31912a520ed27a5a9d2d00931b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . ", "mimetype": "text/plain", "start_char_idx": 9796, "end_char_idx": 9969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92175fd8-0283-409b-9b99-4cc5ff7f9903", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ba2af71-2a67-44a7-a347-540a20410b43", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In each case, we first define them formally and then motivate their usefulness in the context of anomaly detection, using generated datasets.  In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. ", "original_text": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, . "}, "hash": "b3e4291a28492db9988674f5fa989f5dedb7ac21c23a69b4615d43982397a6cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "454c4c40-040e-4ed0-9c84-c19b8e1cfe61", "node_type": "1", "metadata": {"window": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n", "original_text": ". "}, "hash": "47039186968c445a4e08056163583209465f60ffa24d16f750e8300bb7062390", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 9967, "end_char_idx": 9969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "454c4c40-040e-4ed0-9c84-c19b8e1cfe61", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92175fd8-0283-409b-9b99-4cc5ff7f9903", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In section III, we\u2019ll apply these methods to actual benchmark datasets.\n\n ### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. ", "original_text": ". "}, "hash": "87c9649719f908155f491fb61dc18fb0da43578a366a44e1300c8e454773d510", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faf92c99-4a7e-4368-b0d4-48ec67a0e5ae", "node_type": "1", "metadata": {"window": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. ", "original_text": ", 0.5) that lies just slightly outside of this cube along a single dimension. "}, "hash": "2c39d22b01f3f1debd05e0e4ce3b1f46b911cbaa9829abc6b9e0ba9aacc3e020", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 9969, "end_char_idx": 9971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "faf92c99-4a7e-4368-b0d4-48ec67a0e5ae", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. ", "original_text": ", 0.5) that lies just slightly outside of this cube along a single dimension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "454c4c40-040e-4ed0-9c84-c19b8e1cfe61", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### A. Distribution-based aggregation functions\n\nAs discussed in the introduction, an important part of an ensemble-based anomaly detection method is the question how to aggregate the individual per-estimator scores \u03c6_i(x) into an aggregate score, in order to classify a point.  As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n", "original_text": ". "}, "hash": "5916e5142b85541e2236e8fc7a990668a705b95261c8ab41756bfe6b83040b27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "941627cd-69ae-4c37-a0e2-9d108b67c3f5", "node_type": "1", "metadata": {"window": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. ", "original_text": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. "}, "hash": "b76043124fd5855602163200ec6d84ef1cd15535f5ba13a6025f167fee88dcbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", 0.5) that lies just slightly outside of this cube along a single dimension. ", "mimetype": "text/plain", "start_char_idx": 9973, "end_char_idx": 10051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "941627cd-69ae-4c37-a0e2-9d108b67c3f5", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. ", "original_text": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faf92c99-4a7e-4368-b0d4-48ec67a0e5ae", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As we\u2019ve seen, in IF (a function of) the sample mean is used is to aggregate.  This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. ", "original_text": ", 0.5) that lies just slightly outside of this cube along a single dimension. "}, "hash": "02ff25ea6ab40d8549d49c9979b7b5360645781bd9108024bd646794a81488ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a29055dd-63eb-46a4-a309-d91f0cc08899", "node_type": "1", "metadata": {"window": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. ", "original_text": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. "}, "hash": "8c4d8098ac85d17bb29ecda375e275796caccdcdaa620725ad96f1a07895dc97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. ", "mimetype": "text/plain", "start_char_idx": 10051, "end_char_idx": 10325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a29055dd-63eb-46a4-a309-d91f0cc08899", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. ", "original_text": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "941627cd-69ae-4c37-a0e2-9d108b67c3f5", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This is a natural and intuitive choice, however, there can be situations, in which a good estimator will take into consideration features of the vector of estimator scores \u03c6(x) other than its sample mean.\n\n For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. ", "original_text": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points. "}, "hash": "a85c96009b1d1393c8511ba7a4cfaba70ea6c66ead31116f6a46274c77d40180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dc3a36b-031b-4606-965f-8e1417f1f248", "node_type": "1", "metadata": {"window": ".  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n", "original_text": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. "}, "hash": "6fe753dd29b39c4b493ba64b39278018e81cdc38993a6f98b4b1c1a2117738a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. ", "mimetype": "text/plain", "start_char_idx": 10325, "end_char_idx": 10481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1dc3a36b-031b-4606-965f-8e1417f1f248", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n", "original_text": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a29055dd-63eb-46a4-a309-d91f0cc08899", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For example, consider a dataset X \u2282 R^d that contains N \u2212 1 \u226b 1 inlier points drawn at random from the d-dimensional unit cube and a single outlier point x\u0302 = (1.05, 0.5, .  .  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. ", "original_text": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points. "}, "hash": "857f4b4f6dfffcc5c116d3065c312f0c2a8c11ffc781370551c57ae3891ced2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99c3b278-b8c6-44e7-8fcb-8719049c2584", "node_type": "1", "metadata": {"window": ".  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). ", "original_text": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n"}, "hash": "232cbcda6918299bc99f16187b20f525d5d7b119446f030fa82b256c385a9ecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. ", "mimetype": "text/plain", "start_char_idx": 10481, "end_char_idx": 10689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99c3b278-b8c6-44e7-8fcb-8719049c2584", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). ", "original_text": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dc3a36b-031b-4606-965f-8e1417f1f248", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  .  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n", "original_text": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth. "}, "hash": "19d1c59624bcb3e94be8d4214e309bb50044f40c7f2017d8e922d0439008c474", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46a79fdd-1dab-4cfa-ad59-32ba69bf5b31", "node_type": "1", "metadata": {"window": ", 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n", "original_text": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. "}, "hash": "abdace0ec369516fdbbbda00968fd2530ec9beed9e27aba637ab1c5d46ce0628", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n", "mimetype": "text/plain", "start_char_idx": 10689, "end_char_idx": 10918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46a79fdd-1dab-4cfa-ad59-32ba69bf5b31", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ", 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n", "original_text": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99c3b278-b8c6-44e7-8fcb-8719049c2584", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ".  , 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). ", "original_text": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n"}, "hash": "0ea82ae8ccfe69900990c186bdebef203ecd04198b5152cb4a3500340d64357c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "594a6eb4-9582-4083-8583-e55a920c4e35", "node_type": "1", "metadata": {"window": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. ", "original_text": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. "}, "hash": "61abb2a55b7080e1442c793814e328cbb3698bcb64d660d5bc074d728e9340ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. ", "mimetype": "text/plain", "start_char_idx": 10918, "end_char_idx": 11288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "594a6eb4-9582-4083-8583-e55a920c4e35", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. ", "original_text": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46a79fdd-1dab-4cfa-ad59-32ba69bf5b31", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ", 0.5) that lies just slightly outside of this cube along a single dimension.  In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n", "original_text": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d. "}, "hash": "6a1b548b189ed51ba105491b75c3eb18ddb515cbbb92db5add358bcde2fce57f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ed25eac-5287-4337-b245-13c4962c4dbd", "node_type": "1", "metadata": {"window": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. ", "original_text": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. "}, "hash": "ce2cb72375c79f02f4dd80fe4f622bcf0350c98766068b853cc9ac172f1fbecf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 11288, "end_char_idx": 11512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ed25eac-5287-4337-b245-13c4962c4dbd", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. ", "original_text": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "594a6eb4-9582-4083-8583-e55a920c4e35", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In this setup, it is possible to separate this point using only a single decision node in a random tree, however, sampling such a tree is statistically relatively unlikely and, as such, most estimators will contain x\u0302 in a leaf node at a depth similar to the inlier points.  Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. ", "original_text": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores. "}, "hash": "76edd053edae04971becdd666b62bd7c6d7d171ca127d790c7b1d1b75a05643b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a1c9286-d676-4706-a958-11a86b6e0c32", "node_type": "1", "metadata": {"window": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. ", "original_text": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n"}, "hash": "00e0a5ed9855ee2dfa5e0b4a03dd8bdbfa969a2a7af45820def3185c4ed8c5d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. ", "mimetype": "text/plain", "start_char_idx": 11512, "end_char_idx": 11714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a1c9286-d676-4706-a958-11a86b6e0c32", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. ", "original_text": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ed25eac-5287-4337-b245-13c4962c4dbd", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Yet, for large N, it is, on average, significantly more likely for a random tree to separate x\u0302 using very few decision nodes than it is for inlier points.  As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. ", "original_text": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}. "}, "hash": "584fedb0699068f91b4deef533e2c91519b5a19514ad095a484c84e432dbc394", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c14b92ab-cee6-43cb-b300-cd9edd7e5cf5", "node_type": "1", "metadata": {"window": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig. ", "original_text": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). "}, "hash": "c4dceda4909b371385fcfbbdd3159e86309233183fb903b45360258a850ba670", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n", "mimetype": "text/plain", "start_char_idx": 11714, "end_char_idx": 11803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c14b92ab-cee6-43cb-b300-cd9edd7e5cf5", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig. ", "original_text": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a1c9286-d676-4706-a958-11a86b6e0c32", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As such, we intuitively expect that for sufficiently large n, there would be significantly more estimators that isolate x\u0302 at low depth than there are estimators that isolate any inlier points at that depth.  For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. ", "original_text": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n"}, "hash": "b59cb93b95b7d46798e15e36484b60a7517558714933b51c6151545c700cd7ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39fe0f84-a21e-4437-9363-98bccbd63134", "node_type": "1", "metadata": {"window": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n", "original_text": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n"}, "hash": "715bc3f28507bb234027844069a21f0b451fb9ec587383030acba997d883a9df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). ", "mimetype": "text/plain", "start_char_idx": 11803, "end_char_idx": 12297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39fe0f84-a21e-4437-9363-98bccbd63134", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n", "original_text": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c14b92ab-cee6-43cb-b300-cd9edd7e5cf5", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For this reason, we would expect an aggregation function that places disproportionate weight onto high per-estimator scores to lead to a classifier that outperforms one that only considers the sample mean across all estimators.\n\n An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig. ", "original_text": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e). "}, "hash": "bcdb23d17cd34454f7b4395bd6a0c795ed571e54801831bf0e5e5ae8dedc4ee4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a48a9f92-7b32-43f7-adbe-fc80db67a710", "node_type": "1", "metadata": {"window": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. ", "original_text": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. "}, "hash": "acaf619e0c5addedadf1488df6bbb2ad6ceefc62398889412b8c5bf32050f0d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n", "mimetype": "text/plain", "start_char_idx": 12297, "end_char_idx": 12479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a48a9f92-7b32-43f7-adbe-fc80db67a710", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. ", "original_text": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39fe0f84-a21e-4437-9363-98bccbd63134", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "An extreme version of this idea would be to place all the weight onto the single estimator with the worst anomaly score, however, such an extreme weighing mechanism would surely\n\n---\n\u00b3https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest\n---\n\nnot lead to a robust classifier and more often than not \u201cthrow away the baby with the bath water\u201d.  Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n", "original_text": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n"}, "hash": "75880b947c4414d409bf6088cfec1b39ab06c380cd90487cf798acdde1395e94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acbf10f8-1097-4198-bb3f-0abb387fb31a", "node_type": "1", "metadata": {"window": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n", "original_text": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. "}, "hash": "fe582470a549ac34299405a7761604e7f8cc531733414398cc1d4539ee4e214b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. ", "mimetype": "text/plain", "start_char_idx": 12479, "end_char_idx": 12789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "acbf10f8-1097-4198-bb3f-0abb387fb31a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n", "original_text": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a48a9f92-7b32-43f7-adbe-fc80db67a710", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Instead, in this paper, we introduce a *family* of aggregation functions, parametrized by a single parameter \u03b1, that lets users \u201ctune\u201d the sensitivity of the aggregation step to estimators with below-average anomaly scores.  Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. ", "original_text": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number. "}, "hash": "cf4f5019d7356ac2a9ba545cfde86a59b80098ba3c16fcb3e3d731c3fb2dc7a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d76dcfb-b5f5-4532-9a00-e17b1d9d4dc4", "node_type": "1", "metadata": {"window": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). ", "original_text": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. "}, "hash": "496bf9ef6e129f700d316596ff67cc9698a86990f8a317525475e59da4bd6964", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. ", "mimetype": "text/plain", "start_char_idx": 12789, "end_char_idx": 12863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d76dcfb-b5f5-4532-9a00-e17b1d9d4dc4", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). ", "original_text": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acbf10f8-1097-4198-bb3f-0abb387fb31a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Specifically, define the functions f_\u03b1 : R^n_{\u22650} \u2192 R, for \u03b1 \u2208 [0, 1) \u222a (1,\u221e), via the mapping\n\nf_\u03b1(x) = n^{\\frac{1}{1\u2212\u03b1}} (\\sum_{i=1}^n x_i^{1\u2212\u03b1})^\\frac{1}{1\u2212\u03b1}\n\nand f_\u03b2 = lim_{\u03b1\u2192\u03b2} f_\u03b1 for \u03b2 \u2208 {1,\u221e}.  This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n", "original_text": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier. "}, "hash": "fc36243644abf468bb7679af8adf504cea670e4d5d14da93f5a8e80308721def", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46f2fc93-11ae-4a9c-aca2-8d8c5f87607f", "node_type": "1", "metadata": {"window": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). ", "original_text": "Indeed, in Fig. "}, "hash": "254f24acb4f1dc9d14d3ea5be07aed28406d062b8aa72efa57c2c2f675ac1224", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. ", "mimetype": "text/plain", "start_char_idx": 12863, "end_char_idx": 13264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46f2fc93-11ae-4a9c-aca2-8d8c5f87607f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). ", "original_text": "Indeed, in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d76dcfb-b5f5-4532-9a00-e17b1d9d4dc4", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This leads to the corresponding aggregation functions for \u03b1 \u2265 0,\n\nh_\u03b1(x) = 2^{\u2212f_\u03b1(x)}.\n\n In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). ", "original_text": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method. "}, "hash": "a0ff19ce2e9273bdddad8d082c9c46c650f910a1935a13cae085e7a1d1838826", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5298170b-02df-4560-94f5-6daf4fcd807e", "node_type": "1", "metadata": {"window": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). ", "original_text": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n"}, "hash": "acd049ccfed44612ab2b65ddd005b823967980697e892cec8b614ff5b68e58d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, in Fig. ", "mimetype": "text/plain", "start_char_idx": 13264, "end_char_idx": 13280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5298170b-02df-4560-94f5-6daf4fcd807e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). ", "original_text": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46f2fc93-11ae-4a9c-aca2-8d8c5f87607f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Appendix A, we use a connection of these functions to the *R\u00e9nyi divergences* from information-theory to show that they satisfy the following properties: satisfy the following properties:\n1) h_0 = h_{IF}\n2) h_\u221e(x) = max(x)\n3) \u03b1 > \u03b1\u2032 \u21d2 h_\u03b1(x) \u2265 f_\u03b1\u2032(x), \u2200x \u2208 R^n_{\u22650}, \u03b1 \u2265 0\n\nIn other words, the aggregation functions h_\u03b1 functions are monotonically increasing in \u03b1 and interpolate between the standard IF aggregation function on the one end (\u03b1 = 0) and the maximum on the other end (\u03b1 = \u221e).  In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). ", "original_text": "Indeed, in Fig. "}, "hash": "eb6de82a6a8959d1757b1009fa756fc2eb10b17bcf56b1aabe7d82284eb7e0b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed66df13-9579-42ae-b299-bbcac8d191e6", "node_type": "1", "metadata": {"window": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). ", "original_text": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. "}, "hash": "cc38859b29356b354e405d3aaadf39384b12762b86d764367725f384454a7eea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n", "mimetype": "text/plain", "start_char_idx": 13280, "end_char_idx": 13439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed66df13-9579-42ae-b299-bbcac8d191e6", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). ", "original_text": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5298170b-02df-4560-94f5-6daf4fcd807e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In this sense, the above aggregation functions introduce a strict generalisation of the original IF algorithm and we can write\n\nIF_\u03b1 = (n, A_{IF}, \u03c6^{IF}, h_\u03b1, \u03c4),\n\nwith IF = IF_0.\n\n The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). ", "original_text": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n"}, "hash": "a6dd31bd0240138cbbd66b9434b6920e76760e26caf725b239ad4ba41b22db8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f032aa8b-8398-4f40-b3ac-45eeddf9059d", "node_type": "1", "metadata": {"window": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e.", "original_text": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n"}, "hash": "d42099a710622e1eb1d947eb256f5715cfbae31ad0ad1b8fc112eaf54a97d484", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. ", "mimetype": "text/plain", "start_char_idx": 13439, "end_char_idx": 13873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f032aa8b-8398-4f40-b3ac-45eeddf9059d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e.", "original_text": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed66df13-9579-42ae-b299-bbcac8d191e6", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The monotonicity property of the aggregation functions proves to be very convenient for thinking about the effect of changing the value of \u03b1: All else being equal, increasing \u03b1 can only increase the number of points a classifier considers anomalous, while conversely reducing it can only decrease this number.  It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). ", "original_text": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1. "}, "hash": "bc4f2df48918fe333c4bf5914368aa2e8a08af1898e9d0cfa16992e17d21e8c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e337239-8fa4-4413-bf98-4043d494625d", "node_type": "1", "metadata": {"window": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n", "original_text": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). "}, "hash": "501ea64372072ae90bef850802a73eb4ac754f4ad4e65fd7d234a41e39be386c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n", "mimetype": "text/plain", "start_char_idx": 13873, "end_char_idx": 14044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e337239-8fa4-4413-bf98-4043d494625d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n", "original_text": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f032aa8b-8398-4f40-b3ac-45eeddf9059d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "It is for this reason that we call \u03b1 the \u201csensitivity\u201d of the classifier.  As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e.", "original_text": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n"}, "hash": "fc0346e27c0a3a57fa9c1b9fc88ad0e61a1ff174bb5760ab3ae8d84ced106ea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055a836b-0de7-4b18-91e2-4518f29e9d79", "node_type": "1", "metadata": {"window": "Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points. ", "original_text": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). "}, "hash": "a93a69720acb713473fbedf69b89d96d73af7514d9c66013e78ba6c91543ff17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). ", "mimetype": "text/plain", "start_char_idx": 14044, "end_char_idx": 14342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "055a836b-0de7-4b18-91e2-4518f29e9d79", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points. ", "original_text": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e337239-8fa4-4413-bf98-4043d494625d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As a corollary, for any n, A, \u03c6, \u03c4, and \u03b1 > \u03b1\u2032, the anomaly detector\n\nAD_\u03b1 = (n, A, \u03c6, h_\u03b1, \u03c4)\n\nwill have a false negative rate smaller or equal to that of the corresponding detector AD_\u03b1\u2032 (at the cost of a potentially larger false positive rate).\u2074\n\nWe believe that the above family of aggregation functions introduces a practically useful degree of freedom for designing an anomaly detection method.  Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n", "original_text": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left). "}, "hash": "808fce268abe286071c73787e98a2ba8bb291b3edaaa9d22a825af64a1f3f694", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74baebf8-4b03-4192-a934-97c04d16015d", "node_type": "1", "metadata": {"window": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes. ", "original_text": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). "}, "hash": "df7bf3b2ad3eeed738bfaf917d6751634097fa3a97639f5741c8262e70f9ec56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). ", "mimetype": "text/plain", "start_char_idx": 14342, "end_char_idx": 14569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74baebf8-4b03-4192-a934-97c04d16015d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes. ", "original_text": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "055a836b-0de7-4b18-91e2-4518f29e9d79", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Indeed, in Fig.  2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points. ", "original_text": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right). "}, "hash": "71af7ff5164337d6b7d54bb828ab1275c5030912c57e956a35f01b4d780a9839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a18f932-48b8-4dd8-a251-c76e35d933c0", "node_type": "1", "metadata": {"window": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n", "original_text": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). "}, "hash": "725db18f9c4c01bb3c076b242d40aa6e2b12a5f54236441cb4c29bdf3c50e6e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). ", "mimetype": "text/plain", "start_char_idx": 14569, "end_char_idx": 14701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a18f932-48b8-4dd8-a251-c76e35d933c0", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n", "original_text": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74baebf8-4b03-4192-a934-97c04d16015d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2 we describe and analyse experiments, based on the motivating toy example above, that show a significant increase in performance for IF_\u03b1 when increasing \u03b1.\n\n ---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes. ", "original_text": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left). "}, "hash": "024548f2a0262b85fa7d9f1a9cd6dabb9460d9a261165757430b5e04ff4be415", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9b2c4da-1944-41a1-9b4d-14408ae06715", "node_type": "1", "metadata": {"window": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". ", "original_text": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e."}, "hash": "40cb4176be0a02dd051db4ea1bc22ca6b2f8f8c657219e090d4d9b8e9b1e6285", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). ", "mimetype": "text/plain", "start_char_idx": 14701, "end_char_idx": 14941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9b2c4da-1944-41a1-9b4d-14408ae06715", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". ", "original_text": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a18f932-48b8-4dd8-a251-c76e35d933c0", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "---\n\u2074As changing the threshold also has the effect of changing the predictions in only one direction, it may seem as if the effect of changing \u03b1 can similarly be obtained by changing \u03c4. That is, the reader might be under the impression that, continuing the example in the main text, for any value \u03b1\u2032, there would in general exist a value \u03c4\u2032 such that the predictions of the classifier (n, A, \u03c6, h_\u03b1\u2032, \u03c4\u2032) coincide with those of AD_\u03b1.  This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n", "original_text": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right). "}, "hash": "d00efbe11f062019e2f4aa708726464f7b29afdc3342d816eb0b693f4ec70e1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b310243c-0242-4501-a19d-bd3910c59042", "node_type": "1", "metadata": {"window": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves. ", "original_text": "**\n\n*Figure 2 Description: The figure contains four plots.\n"}, "hash": "f6ead6c037b3aafde8fc215cc32e72bb72b5530b8940d98d3dfe11b89b0c63a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e.", "mimetype": "text/plain", "start_char_idx": 14941, "end_char_idx": 15092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b310243c-0242-4501-a19d-bd3910c59042", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves. ", "original_text": "**\n\n*Figure 2 Description: The figure contains four plots.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9b2c4da-1944-41a1-9b4d-14408ae06715", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This is, however, not the case, as changing \u03b1 can lead to a different relative ordering of data points wrt their aggregated score, while this is impossible by changing \u03c4.\n ---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". ", "original_text": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e."}, "hash": "be96f269bed3269a9bc84b7a01efe31bc1722fca091c7ad2645ae6b74735441c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07fda37a-af59-4228-9e03-37f7ce3dda43", "node_type": "1", "metadata": {"window": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. ", "original_text": "Top left: A scatter plot showing data points. "}, "hash": "ea9201322ee7f29520a42fdea035ff356dbfc802df3d6404c1edfa485225c696", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Figure 2 Description: The figure contains four plots.\n", "mimetype": "text/plain", "start_char_idx": 15092, "end_char_idx": 15151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07fda37a-af59-4228-9e03-37f7ce3dda43", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. ", "original_text": "Top left: A scatter plot showing data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b310243c-0242-4501-a19d-bd3910c59042", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "---\n***\n**Figure 2: Motivating the use of distribution-based aggregation functions: We sample a dataset consisting of N = 128 points with 127 inliers (blue) uniformly sampled from a d-dimensional unit cube and a single anomalous point (orange) slightly outside along a single dimension (Top left).  Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves. ", "original_text": "**\n\n*Figure 2 Description: The figure contains four plots.\n"}, "hash": "47ddb7436a302d362aa1210e2f7f5c6888f037bae7a36cc5f0d2dd6455935532", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1de8da3-a669-49de-b968-f50936f8c4ae", "node_type": "1", "metadata": {"window": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n", "original_text": "Most points are blue and clustered within a square region from 0 to 1 on both axes. "}, "hash": "283a9152fe97ad6781fc7184ce8af175661baaab1f965e4c7e1cdbe07abadb2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Top left: A scatter plot showing data points. ", "mimetype": "text/plain", "start_char_idx": 15151, "end_char_idx": 15197, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b1de8da3-a669-49de-b968-f50936f8c4ae", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n", "original_text": "Most points are blue and clustered within a square region from 0 to 1 on both axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07fda37a-af59-4228-9e03-37f7ce3dda43", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Fitting an isolation forest with n = 100 estimators to such a dataset leads to a distribution of (sorted) per-estimator scores in which the anomaly receives a \u201ctail\u201d of significantly worse scores than most inliers (Top right).  As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. ", "original_text": "Top left: A scatter plot showing data points. "}, "hash": "39137a4cf4c2283e34ddd8e5fd8484b701ed114908867edd0349bad3490b8125", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ef19786-39a8-4a72-85b7-62afe9b8e288", "node_type": "1", "metadata": {"window": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". ", "original_text": "A single orange point lies outside this region.\n"}, "hash": "9417d52a833ae3fea512a7065c385d8738bdadb74289335436989430d9dce4a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Most points are blue and clustered within a square region from 0 to 1 on both axes. ", "mimetype": "text/plain", "start_char_idx": 15197, "end_char_idx": 15281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ef19786-39a8-4a72-85b7-62afe9b8e288", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". ", "original_text": "A single orange point lies outside this region.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1de8da3-a669-49de-b968-f50936f8c4ae", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As a result, we can separate the outlier from the inliers by using an aggregation function with sufficiently large \u03b1 (Bottom left).  In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n", "original_text": "Most points are blue and clustered within a square region from 0 to 1 on both axes. "}, "hash": "6aa526d9c74c77492aabd25ea9069bfaec903845299c460e2b1017e55d08c7af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92e73bc2-14a5-417e-8618-89803a835acd", "node_type": "1", "metadata": {"window": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown. ", "original_text": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". "}, "hash": "806350532acc5c5838925c479d08af31c5f24922479129cd272faccd45604cac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single orange point lies outside this region.\n", "mimetype": "text/plain", "start_char_idx": 15281, "end_char_idx": 15329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92e73bc2-14a5-417e-8618-89803a835acd", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown. ", "original_text": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ef19786-39a8-4a72-85b7-62afe9b8e288", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In particular, zooming into this curve for \u03b1 \u2264 1, we see that using the sample mean (\u03b1 = 0) leads to an outlier anomaly score better than many inliers (blue shaded regions correspond to the 95th percentile of inlier scores) (Bottom right).  Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". ", "original_text": "A single orange point lies outside this region.\n"}, "hash": "495a04763243fb657c0b814d291fe0383f9b7110217c1077a3c9a2fdf2fd4264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d516e997-6498-4140-8214-7f009ce00588", "node_type": "1", "metadata": {"window": "**\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. ", "original_text": "It shows two curves. "}, "hash": "c930f8e3995e036c18b349d78a53b569f99186521425ab9375088d31d551cb6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". ", "mimetype": "text/plain", "start_char_idx": 15329, "end_char_idx": 15383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d516e997-6498-4140-8214-7f009ce00588", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. ", "original_text": "It shows two curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92e73bc2-14a5-417e-8618-89803a835acd", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Running the above experiment a hundred times for d = 10 leads to average AUCROC score of 0.78 for the IF algorithm, as opposed to 0.98 when using IF_\u221e. **\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown. ", "original_text": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\". "}, "hash": "798ca100bbfe2f0a5a9cf6afa02a2fdd3d47a8ea23c82fb9dcb1822ab3907dc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b1aa87d-9568-4461-ab29-62933961582f", "node_type": "1", "metadata": {"window": "Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat. ", "original_text": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. "}, "hash": "148a18617896b7ac092376160588019a37af341390895a8826d89a32238f0eb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It shows two curves. ", "mimetype": "text/plain", "start_char_idx": 15383, "end_char_idx": 15404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b1aa87d-9568-4461-ab29-62933961582f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat. ", "original_text": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d516e997-6498-4140-8214-7f009ce00588", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n*Figure 2 Description: The figure contains four plots.\n Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. ", "original_text": "It shows two curves. "}, "hash": "c5ce9bc2b8c7e58b7ec75ef9464ecd923ecacf1e9d19e844ab22360c8c61f1f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0c35352-3c03-40eb-9242-d5f868913668", "node_type": "1", "metadata": {"window": "Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n", "original_text": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n"}, "hash": "8be6fe658bbe9016edd7056ef4d6b679eb27727d15562aebc8d7d00af1057cbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. ", "mimetype": "text/plain", "start_char_idx": 15404, "end_char_idx": 15515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0c35352-3c03-40eb-9242-d5f868913668", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n", "original_text": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b1aa87d-9568-4461-ab29-62933961582f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Top left: A scatter plot showing data points.  Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat. ", "original_text": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5. "}, "hash": "364c91f964743c3e3268ac8544c25a896121904d6d703e4400e3d193a47fcbd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc4ff51c-e7bd-4602-947e-fd90a2809132", "node_type": "1", "metadata": {"window": "A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. ", "original_text": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". "}, "hash": "ff464de6ea6bff6ae97726dd1d8a3fef725070dc643829173183b849274baea1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n", "mimetype": "text/plain", "start_char_idx": 15515, "end_char_idx": 15680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc4ff51c-e7bd-4602-947e-fd90a2809132", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. ", "original_text": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0c35352-3c03-40eb-9242-d5f868913668", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Most points are blue and clustered within a square region from 0 to 1 on both axes.  A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n", "original_text": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n"}, "hash": "f862b77c6ea04f9165d4d4527c584e008e3db540c2ff42c2eefd15ff9967dea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4a726c2-d711-4335-9ff9-7d33c6c3ef7a", "node_type": "1", "metadata": {"window": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases.", "original_text": "Two curves are shown. "}, "hash": "b4695f6d58ccf27eca531960dd614f0e027eed568d3e7f5fee61d21593e675bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". ", "mimetype": "text/plain", "start_char_idx": 15680, "end_char_idx": 15731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4a726c2-d711-4335-9ff9-7d33c6c3ef7a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases.", "original_text": "Two curves are shown. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc4ff51c-e7bd-4602-947e-fd90a2809132", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A single orange point lies outside this region.\n Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. ", "original_text": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\". "}, "hash": "1f28b7d71ed62e97a5d182d5d18c0044aadcd7de0da7785acc10bc40d8b513a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f0ad45a-8763-4e9f-bae6-3de4c3c0d673", "node_type": "1", "metadata": {"window": "It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. ", "original_text": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. "}, "hash": "6d006a76d8a753cba1ba75f53e680985b70c73998794395a54ff421a068ace8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Two curves are shown. ", "mimetype": "text/plain", "start_char_idx": 15731, "end_char_idx": 15753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f0ad45a-8763-4e9f-bae6-3de4c3c0d673", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. ", "original_text": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4a726c2-d711-4335-9ff9-7d33c6c3ef7a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Top right: A plot of \"Value of \u03c6\" vs \"ith estimator\".  It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases.", "original_text": "Two curves are shown. "}, "hash": "85991d01bf110a25cba7beecad671a77321e5c8faf341549784a3a4a358caeb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e406ea5c-7f36-4c1c-9a0c-9b8419190143", "node_type": "1", "metadata": {"window": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. ", "original_text": "The blue curve (inlier) starts at the same point but remains relatively flat. "}, "hash": "d2eb15ae86b7a1fd8717c86021b96592ddd5749bd05bfe0d6a7624c6843cce33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. ", "mimetype": "text/plain", "start_char_idx": 15753, "end_char_idx": 15852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e406ea5c-7f36-4c1c-9a0c-9b8419190143", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. ", "original_text": "The blue curve (inlier) starts at the same point but remains relatively flat. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f0ad45a-8763-4e9f-bae6-3de4c3c0d673", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "It shows two curves.  A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. ", "original_text": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases. "}, "hash": "2f418015ffa3d5507a15afff01d0a9b8ab29c34b3ff1b53577b7783e9b370235", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a017238-fadc-49bf-b909-3b15126e25bd", "node_type": "1", "metadata": {"window": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. ", "original_text": "The two curves diverge as \u03b1 increases.\n"}, "hash": "530a9a7f1f9e24ec2c353fa070024ea1c3314fd0def682be0952f95b7ba350cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue curve (inlier) starts at the same point but remains relatively flat. ", "mimetype": "text/plain", "start_char_idx": 15852, "end_char_idx": 15930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a017238-fadc-49bf-b909-3b15126e25bd", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. ", "original_text": "The two curves diverge as \u03b1 increases.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e406ea5c-7f36-4c1c-9a0c-9b8419190143", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A blue shaded region represents the distribution for inlier points, showing values mostly between 0.3 and 0.5.  A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. ", "original_text": "The blue curve (inlier) starts at the same point but remains relatively flat. "}, "hash": "109210da66896085d5effa8b343e093931f5b19d7f5ef6066b97d8dd03140883", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9294a94-10ff-4663-8cbf-11ff663e1482", "node_type": "1", "metadata": {"window": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. ", "original_text": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. "}, "hash": "0dfb42c0aa16c6e8677551ec7a9c9528f3e74df02849ebccef8452edcb4e454d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two curves diverge as \u03b1 increases.\n", "mimetype": "text/plain", "start_char_idx": 15930, "end_char_idx": 15969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9294a94-10ff-4663-8cbf-11ff663e1482", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. ", "original_text": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a017238-fadc-49bf-b909-3b15126e25bd", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A single orange line represents the outlier, which has values that increase sharply for the highest-ranked estimators, forming a \"tail\" that reaches a value of 1.0.\n Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. ", "original_text": "The two curves diverge as \u03b1 increases.\n"}, "hash": "5399fc24ce47ed7763fe1f8895c4c164ad63bf97978eada789b8a2dbb7bba2f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "182c2bc9-0426-433a-8f58-c99d87a6152a", "node_type": "1", "metadata": {"window": "Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. ", "original_text": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases."}, "hash": "077f2389a112fb8b7903ca2203414359b26e78642639cad5634f6bcbd3b409c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. ", "mimetype": "text/plain", "start_char_idx": 15969, "end_char_idx": 16055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "182c2bc9-0426-433a-8f58-c99d87a6152a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. ", "original_text": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9294a94-10ff-4663-8cbf-11ff663e1482", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Bottom left: A plot of \"Anomaly score h_\u03b1\" vs \"\u03b1\".  Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. ", "original_text": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0. "}, "hash": "932a6361dc4ee3a12ea038062cba1e32176b14c830bd96e702b819b3da7e6612", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00bfdecd-56a5-4d13-952c-31cea6209209", "node_type": "1", "metadata": {"window": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. ", "original_text": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. "}, "hash": "93c3104b97696247d7e0f377a88a89656458c26f51824c050a3b3d48ac78490c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases.", "mimetype": "text/plain", "start_char_idx": 16055, "end_char_idx": 16235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "00bfdecd-56a5-4d13-952c-31cea6209209", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. ", "original_text": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "182c2bc9-0426-433a-8f58-c99d87a6152a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Two curves are shown.  The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. ", "original_text": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases."}, "hash": "6a15f145c2387c1d7ed7a1c9b88ae188354c6ee97af520e01f0aa7bc2c207962", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c8f5c28-2ae3-4539-b6b4-93f10923c752", "node_type": "1", "metadata": {"window": "The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n", "original_text": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. "}, "hash": "0462b25e38c323f5ae3ae58de271e36b2362af12d55882ac3096dccd5509c2ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. ", "mimetype": "text/plain", "start_char_idx": 16235, "end_char_idx": 16504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c8f5c28-2ae3-4539-b6b4-93f10923c752", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n", "original_text": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00bfdecd-56a5-4d13-952c-31cea6209209", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The orange curve (outlier) starts around 0.56 and increases steadily to about 0.78 as \u03b1 increases.  The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. ", "original_text": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators. "}, "hash": "078497b0e57943b87d39d2db28dd75a5bdb52d5bc46cd1632d848a54a1788629", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b0f09cc-acbf-4f04-86ae-753c2fc02d92", "node_type": "1", "metadata": {"window": "The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. ", "original_text": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. "}, "hash": "16f8472ae024862c69b931a54ce68656417503f50e657455fbbe34388c7f93e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. ", "mimetype": "text/plain", "start_char_idx": 16504, "end_char_idx": 16672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b0f09cc-acbf-4f04-86ae-753c2fc02d92", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. ", "original_text": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c8f5c28-2ae3-4539-b6b4-93f10923c752", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The blue curve (inlier) starts at the same point but remains relatively flat.  The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n", "original_text": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node. "}, "hash": "6d2da2d99909d53b0f59426c9c99eead0142ce471ec44403bedee37e8363671b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dea9901a-0406-4e10-9d76-b48f65fc2938", "node_type": "1", "metadata": {"window": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e. ", "original_text": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. "}, "hash": "11193596e0fad5dd98aca7ebeb47782e958247ce9e293dfb5c2b9b6b6c4d3a70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. ", "mimetype": "text/plain", "start_char_idx": 16672, "end_char_idx": 16799, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dea9901a-0406-4e10-9d76-b48f65fc2938", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e. ", "original_text": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b0f09cc-acbf-4f04-86ae-753c2fc02d92", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The two curves diverge as \u03b1 increases.\n Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. ", "original_text": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node. "}, "hash": "4a2a141ee570f8e58e733bf785f400c821acd7a966af5200614c866030f22681", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ad9136f-91fd-4425-a90a-66e079a56a21", "node_type": "1", "metadata": {"window": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform. ", "original_text": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. "}, "hash": "0dd2a6497d6570847a52cdc5167fa85c8eb1ea5f8a3c7ce0f3bdacf5e25874eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. ", "mimetype": "text/plain", "start_char_idx": 16799, "end_char_idx": 16923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ad9136f-91fd-4425-a90a-66e079a56a21", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform. ", "original_text": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dea9901a-0406-4e10-9d76-b48f65fc2938", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Bottom right: A zoomed-in version of the bottom left plot, for \u03b1 between 0.0 and 1.0.  It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e. ", "original_text": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node. "}, "hash": "6cb92cdc0c42c9f1f4f7a96db6c1841d55e59100abf4aa338f52529f1049cc5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a309d1fb-078b-4ac8-846e-4dbaae066d6e", "node_type": "1", "metadata": {"window": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion. ", "original_text": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. "}, "hash": "783ee01b627182d73f73ea89c5b94e2df4bace7d9584f83940596e3255d267f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. ", "mimetype": "text/plain", "start_char_idx": 16923, "end_char_idx": 17225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a309d1fb-078b-4ac8-846e-4dbaae066d6e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion. ", "original_text": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ad9136f-91fd-4425-a90a-66e079a56a21", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "It shows the orange curve for the outlier starting below the blue shaded region (representing the 95th percentile of inlier scores) at \u03b1=0, and then rising above it as \u03b1 increases. *\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform. ", "original_text": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework. "}, "hash": "0df13e3f330b9904adcd8b9e6243a319a7a006e61a0f42ef33ddbc3660bc4458", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4893d2fd-0985-4d06-a8b2-081b6ea167bb", "node_type": "1", "metadata": {"window": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n", "original_text": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n"}, "hash": "9c79bd61a6be0f6db21f22f5555ca7245a575db46896b3a8a1f832493c542e30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. ", "mimetype": "text/plain", "start_char_idx": 17225, "end_char_idx": 17393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4893d2fd-0985-4d06-a8b2-081b6ea167bb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n", "original_text": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a309d1fb-078b-4ac8-846e-4dbaae066d6e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\n### B. Hypervolume-based scoring function\n\nThe other important part of an ensemble-based anomaly detection method is the question how to choose the per-estimator scores \u03c6_i(x) for a given data point x.\n\nThe standard IF algorithm uses random trees as estimators.  As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion. ", "original_text": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees. "}, "hash": "b25ea580a3ad4f2a2d56c15343922d50c740971e854e3e720fc91029e73094d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dae72eea-18d1-4ce2-8f72-6e2971f0144e", "node_type": "1", "metadata": {"window": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n", "original_text": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. "}, "hash": "1e5afd466b23d892c8ec75cbb87099440864d924140c3868547a3ba5657081b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n", "mimetype": "text/plain", "start_char_idx": 17393, "end_char_idx": 17809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dae72eea-18d1-4ce2-8f72-6e2971f0144e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n", "original_text": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4893d2fd-0985-4d06-a8b2-081b6ea167bb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As such, the leaf nodes (L^(i))_j of the ith estimator E_i by construction partition the space R^d into a set of axis-aligned hyper-rectangles, one for each leaf node.  The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n", "original_text": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n"}, "hash": "9345b67433bb593ad70e7718985a5d657aa326883eabe222a9879145cb26b7c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "351e6367-1ea0-44a8-864c-7db91b67b93b", "node_type": "1", "metadata": {"window": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. ", "original_text": "As such, points with bad (i.e. "}, "hash": "1c9da3174c5d689f3c2581cbd61964440471caa803e03dc2d7d38d57c9c7d6a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. ", "mimetype": "text/plain", "start_char_idx": 17809, "end_char_idx": 18267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "351e6367-1ea0-44a8-864c-7db91b67b93b", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. ", "original_text": "As such, points with bad (i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dae72eea-18d1-4ce2-8f72-6e2971f0144e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The boundaries of these rectangles simply correspond to the threshold tests at the decision nodes that lead up to a leaf node.  With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n", "original_text": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random. "}, "hash": "f31cbb3bca17ee6c6f61a774efd5254c926bafda131429ca0a8433ef7db8408c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e61e55e-a3ff-4e04-866f-55dfbdd26da4", "node_type": "1", "metadata": {"window": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. ", "original_text": "low) anomaly score are those that have density significantly below a uniform. "}, "hash": "66f7fa866a3d3c05a35ee469c96c384603815c3d2799736bb2027630b5dd4722", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, points with bad (i.e. ", "mimetype": "text/plain", "start_char_idx": 18267, "end_char_idx": 18298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e61e55e-a3ff-4e04-866f-55dfbdd26da4", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. ", "original_text": "low) anomaly score are those that have density significantly below a uniform. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "351e6367-1ea0-44a8-864c-7db91b67b93b", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "With slight abuse of notation, in the following we will use L^(i)_j to denote both this rectangle as well as the leaf node.  Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. ", "original_text": "As such, points with bad (i.e. "}, "hash": "ebec4a97be354cf5bd9dc28a5d4fc24bcddf173515ed7772eb6b7f468181d49c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bbbb007-ed66-4939-a35f-899550870824", "node_type": "1", "metadata": {"window": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. ", "original_text": "See [15] for a detailed discussion. "}, "hash": "97f0fda6f46f6daa9d290d232cb3431e058d7bc2809d8c63cd60a400ab5c8240", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "low) anomaly score are those that have density significantly below a uniform. ", "mimetype": "text/plain", "start_char_idx": 18298, "end_char_idx": 18376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5bbbb007-ed66-4939-a35f-899550870824", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. ", "original_text": "See [15] for a detailed discussion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e61e55e-a3ff-4e04-866f-55dfbdd26da4", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Moreover, we will use L^(i)(x) to denote that hyperrectangle in the partition corresponding to E_i that contains the data point x.\n\nInstead of the classical IF scoring based on the depths of the leaf nodes, we propose a scoring function motivated by the Probably Approximately Correct (PAC) framework.  The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. ", "original_text": "low) anomaly score are those that have density significantly below a uniform. "}, "hash": "cad3904682d36b6cdb0fbc0e0d7b2595a776175484d5243998a3ae2902460459", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a031539f-8e34-475d-9660-c04803c6e225", "node_type": "1", "metadata": {"window": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. ", "original_text": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n"}, "hash": "c2e40d3e688ca7ba91d2bd935327d7a44b4ea22720f44f164698891f73cddf83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See [15] for a detailed discussion. ", "mimetype": "text/plain", "start_char_idx": 18376, "end_char_idx": 18412, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a031539f-8e34-475d-9660-c04803c6e225", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. ", "original_text": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bbbb007-ed66-4939-a35f-899550870824", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The PAC-RPAD framework is a method developed in [15] to show polynomial sample complexity for learning an unsupervised anomaly detection method with proven guarantees.  Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. ", "original_text": "See [15] for a detailed discussion. "}, "hash": "f170d6277e49fcddb30ac1ae803de810620474e62aeec9515cc3ebd0a4fffa86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76f4b6ee-5fa2-4fe3-8ec6-0b87408990aa", "node_type": "1", "metadata": {"window": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. ", "original_text": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n"}, "hash": "ace6dd89e69b17de7ead3c56118d26fe72c39577d5fb5b0d3acc414748c436bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n", "mimetype": "text/plain", "start_char_idx": 18412, "end_char_idx": 18569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76f4b6ee-5fa2-4fe3-8ec6-0b87408990aa", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. ", "original_text": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a031539f-8e34-475d-9660-c04803c6e225", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Inspired by this framework, we define the per-estimator scoring function\n\n\u03c6^{PAC}(E_i, x) = |L^(i)(x)| / |Y| * V(Y) / V(L^(i)(x)) , (1)\n\nwhere |L^(i)(x)| denotes the number of points of the subsampling set Y that are contained in L^(i)(x), V(L) denotes the volume of the hyperrectangle L and V(Y) denotes some bounding volume that contains all points in Y but whose total size is a hyperparameter of the algorithm.\n\n To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. ", "original_text": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n"}, "hash": "13d7c423908df00d4ce0e660e6b28f2ae55daa1b2eaed7d16dc182b7f158d183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f2ea86e-8b18-4796-9738-5658a7888917", "node_type": "1", "metadata": {"window": "As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. ", "original_text": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. "}, "hash": "dce62486c44a93aeb88c2c236f837a07affedc82615ae59d40ffb4a574d86ec9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n", "mimetype": "text/plain", "start_char_idx": 18569, "end_char_idx": 18736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f2ea86e-8b18-4796-9738-5658a7888917", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. ", "original_text": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76f4b6ee-5fa2-4fe3-8ec6-0b87408990aa", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "To understand the motivation behind this scoring function, note that the first of the two factors in the RHS of (1) constitutes a density estimate of the likelihood of sampling a given hyper-rectangle when randomly sampling a point in the subsampling set, while the second factor measures the relative volume of this rectangle, which coincides with the probability of sampling this rectangle when sampling a point in the bounding volume uniformly at random.  As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. ", "original_text": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n"}, "hash": "36725f7570ae9342dc1a5cf0a711bcb9510253cab14a7573f527d945fb462667", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb19c627-1285-4d60-a122-0f412db58230", "node_type": "1", "metadata": {"window": "low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig. ", "original_text": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. "}, "hash": "38d40f2b61cac0a96073f73ed5c3eb37482691bbc6a77e676eb8ba3e55b5b036", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. ", "mimetype": "text/plain", "start_char_idx": 18736, "end_char_idx": 18910, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb19c627-1285-4d60-a122-0f412db58230", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig. ", "original_text": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f2ea86e-8b18-4796-9738-5658a7888917", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As such, points with bad (i.e.  low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. ", "original_text": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring. "}, "hash": "e18719099aeca5a00a176b62b0dc94687f9d3494480c2aa6e2af4d6add980f5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e486f32-eb1e-4b16-94d3-95136d2db207", "node_type": "1", "metadata": {"window": "See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n", "original_text": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. "}, "hash": "a7acb24598910d16bc03bd3b97a1bd495284a1ede607be8ef875ea84f451376a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. ", "mimetype": "text/plain", "start_char_idx": 18910, "end_char_idx": 19249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4e486f32-eb1e-4b16-94d3-95136d2db207", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n", "original_text": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb19c627-1285-4d60-a122-0f412db58230", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "low) anomaly score are those that have density significantly below a uniform.  See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig. ", "original_text": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin. "}, "hash": "8482b4822ca80bc1b5d9e6543dcb1559cf2f67be9bec132183b991ef19d58d53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03fc1f34-8e75-4afe-ac1d-bd3b232dbb69", "node_type": "1", "metadata": {"window": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III. ", "original_text": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. "}, "hash": "6c1492662da91ac8778a701308d385fdb760f5f50d9eec879fffa95a81045a1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. ", "mimetype": "text/plain", "start_char_idx": 19249, "end_char_idx": 19421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "03fc1f34-8e75-4afe-ac1d-bd3b232dbb69", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III. ", "original_text": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e486f32-eb1e-4b16-94d3-95136d2db207", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "See [15] for a detailed discussion.  The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n", "original_text": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point. "}, "hash": "42ae78de07c51e3cfe1262a3ed979595419424bbabbb7a0935509725f67b964e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "798ce810-7a0d-4f3f-8d12-52f4cccdd174", "node_type": "1", "metadata": {"window": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. ", "original_text": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. "}, "hash": "1cdabf3f74b8d099e1d81de88f70292beb95e1a11d29e595076410ce5d1a48af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. ", "mimetype": "text/plain", "start_char_idx": 19421, "end_char_idx": 19531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "798ce810-7a0d-4f3f-8d12-52f4cccdd174", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. ", "original_text": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03fc1f34-8e75-4afe-ac1d-bd3b232dbb69", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The heuristic behind this choice of scoring function is that outliers x are expected to be located inside hyperrectangles which occur with small frequency.\n\n Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III. ", "original_text": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume. "}, "hash": "c7425f879ff549d8e2e49335fbb54ba8535eb1491310734e15f65b945d269d93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7392253c-6e1b-482e-a1a0-7e6bd7fc84b4", "node_type": "1", "metadata": {"window": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. ", "original_text": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. "}, "hash": "01c768ec815578d157a812c0a7e08b9ffd4bd586ed2e6466d8dbcdde424effe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. ", "mimetype": "text/plain", "start_char_idx": 19531, "end_char_idx": 19660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7392253c-6e1b-482e-a1a0-7e6bd7fc84b4", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. ", "original_text": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "798ce810-7a0d-4f3f-8d12-52f4cccdd174", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Using the above notation and the concepts from the previous section, we then introduce the family of anomaly detection methods\n\nPAC_\u03b1 = (n, A_{IF}, \u03c6^{PAC}, h_\u03b1, \u03c4).\n\n Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. ", "original_text": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases. "}, "hash": "3fe035cad3640913678c4b38144b8cd8b29352d860907511989b5893ebf59427", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0678574d-4408-45d6-8ac6-20fbe3b99906", "node_type": "1", "metadata": {"window": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. ", "original_text": "See Fig. "}, "hash": "373193cfad73bd93f6b3b22577565c6da36975d1218e0102753f9253c3731b92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. ", "mimetype": "text/plain", "start_char_idx": 19660, "end_char_idx": 19797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0678574d-4408-45d6-8ac6-20fbe3b99906", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. ", "original_text": "See Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7392253c-6e1b-482e-a1a0-7e6bd7fc84b4", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Just as in the case of the aggregation functions h_\u03b1, an obvious question is under which circumstances we expect hypervolume-based scoring to outperform depth-based scoring.  While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. ", "original_text": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2. "}, "hash": "7c5419e2dd3e169a19146fb45b5a16d1478c233fb090829d6d10e22638db3b0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96c19f02-8475-4201-ba09-35bc77b1ed5f", "node_type": "1", "metadata": {"window": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. ", "original_text": "3 for details.\n\n"}, "hash": "02c5f3789a55124b947ec1581288eec1380a2fe0f779a664c9792a9a0ccd5239", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See Fig. ", "mimetype": "text/plain", "start_char_idx": 19797, "end_char_idx": 19806, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "96c19f02-8475-4201-ba09-35bc77b1ed5f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. ", "original_text": "3 for details.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0678574d-4408-45d6-8ac6-20fbe3b99906", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "While we do not have a systematic answer to this question, in the following we want to present a simple example in which we found a significant difference in their performance: Consider a dataset X that consists of N \u2212 1 \u226b 1 inlier points randomly distributed on the d \u2212 1-dimensional unit sphere and a single outlier point at the origin.  In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. ", "original_text": "See Fig. "}, "hash": "a952ef0e6dd1c7854db288a9c8e468d26d3f1f349bd94aa356ed2d1b2a2b7abf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "107ada42-3eb5-4aa0-9902-61a6adb42bb2", "node_type": "1", "metadata": {"window": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n", "original_text": "## III. "}, "hash": "f0f058e61294053b5455fe0574d804c21a07106220f1e004cdaeca1416066c02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3 for details.\n\n", "mimetype": "text/plain", "start_char_idx": 19806, "end_char_idx": 19822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "107ada42-3eb5-4aa0-9902-61a6adb42bb2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n", "original_text": "## III. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96c19f02-8475-4201-ba09-35bc77b1ed5f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In this case, on average, the largest, in volume, axis-aligned hyper-rectangle that contains only the outlier is significantly larger than than that of every inlier point.  While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. ", "original_text": "3 for details.\n\n"}, "hash": "e1f7b4bbb13b7114b60cf1941d82db2b07056faf4970708877815c841a67541c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ec5be41-d6d3-44a9-8b8f-edc8970defcc", "node_type": "1", "metadata": {"window": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. ", "original_text": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. "}, "hash": "006dd2b787513278f759eb4226288e5210aad81958c37146084e3488fd2c0ada", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## III. ", "mimetype": "text/plain", "start_char_idx": 19822, "end_char_idx": 19830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ec5be41-d6d3-44a9-8b8f-edc8970defcc", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. ", "original_text": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "107ada42-3eb5-4aa0-9902-61a6adb42bb2", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "While a similar statement holds true for the average depth, it is not nearly as pronounced as for the volume.  This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n", "original_text": "## III. "}, "hash": "e7e83f25e49bc5dfd884fdeda6530df7df3cb0e843ccc07c039168f7d9cfffe9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a7b8ed4-b010-426a-9695-55220d4911b1", "node_type": "1", "metadata": {"window": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n", "original_text": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. "}, "hash": "3b66bda661389370008d934cf9c526524627122cb09c38347b8f2a2aa001d3e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. ", "mimetype": "text/plain", "start_char_idx": 19830, "end_char_idx": 20159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a7b8ed4-b010-426a-9695-55220d4911b1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n", "original_text": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ec5be41-d6d3-44a9-8b8f-edc8970defcc", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This separation between inliers and outlier increases in d, hence we expect PAC_\u03b1 to outperform IF_\u03b1 as the dimension increases.  This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. ", "original_text": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}. "}, "hash": "bde99f79512821e957824d6eea0a77c3f26259bca7ebb7b1f8880b2e48dbf84e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b8971ba-072c-429f-9aa0-753e2f3d27c8", "node_type": "1", "metadata": {"window": "See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. ", "original_text": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. "}, "hash": "0c483cb41c4b86abfa9b43e8e246c72eb1e0e954386af3dacf41384aea1e575e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. ", "mimetype": "text/plain", "start_char_idx": 20159, "end_char_idx": 20252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b8971ba-072c-429f-9aa0-753e2f3d27c8", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. ", "original_text": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a7b8ed4-b010-426a-9695-55220d4911b1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This expectation is confirmed by experiments we ran for the above setup and that show a breakdown of the AUCROC score of IF_0 for d > 2.  See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n", "original_text": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3]. "}, "hash": "4e0004413f7e08baaf259550c5a9b6f8e526596bc48450ace04d0e46846fd0df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67e56b4e-abc4-43bf-ade9-dbf1ce8f2638", "node_type": "1", "metadata": {"window": "3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n", "original_text": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. "}, "hash": "766f3090d6c1069b5b447670849f3202df839853b24d1d0347772f189ecdd299", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. ", "mimetype": "text/plain", "start_char_idx": 20252, "end_char_idx": 20378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67e56b4e-abc4-43bf-ade9-dbf1ce8f2638", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n", "original_text": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b8971ba-072c-429f-9aa0-753e2f3d27c8", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "See Fig.  3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. ", "original_text": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods. "}, "hash": "96c4998a3f820cca9d070405659046e24bd4f655dfc1d111048275cbf200b46d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c687c7db-f234-4cbb-8792-89b6ba128be4", "node_type": "1", "metadata": {"window": "## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. ", "original_text": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n"}, "hash": "9e18bc2a95e62a468f13bfcd71de98aeaffd9930220850773f147a4a7c4180a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. ", "mimetype": "text/plain", "start_char_idx": 20378, "end_char_idx": 20507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c687c7db-f234-4cbb-8792-89b6ba128be4", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. ", "original_text": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67e56b4e-abc4-43bf-ade9-dbf1ce8f2638", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "3 for details.\n\n ## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n", "original_text": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed. "}, "hash": "479cc15b49f0378250fe068c7f90475c9736dac9b1080935ffc340331f2471dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22e0e90f-8933-4f97-8347-ae7a00603f43", "node_type": "1", "metadata": {"window": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab. ", "original_text": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. "}, "hash": "050fc5ae7ced0c7e3eccb5633b258c28eb751727292f0e85d90197888b72c97a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n", "mimetype": "text/plain", "start_char_idx": 20507, "end_char_idx": 20604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22e0e90f-8933-4f97-8347-ae7a00603f43", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab. ", "original_text": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c687c7db-f234-4cbb-8792-89b6ba128be4", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## III.  EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. ", "original_text": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n"}, "hash": "1af3960cc0c1d7bfafe91782eacc26c25d34bae82d246e6c80fe6c9d377bd572", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fcb6d55-7e85-4c98-bf91-0ac18a60e69f", "node_type": "1", "metadata": {"window": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. ", "original_text": "4).\n"}, "hash": "6214dd04a3af040107be9b587dba7913406f6e530a05b242e7c9554c448daea4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. ", "mimetype": "text/plain", "start_char_idx": 20604, "end_char_idx": 20824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6fcb6d55-7e85-4c98-bf91-0ac18a60e69f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. ", "original_text": "4).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22e0e90f-8933-4f97-8347-ae7a00603f43", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "EXPERIMENTS\n\n### A. Setup\n\nWe used the recently published ADBench [3] suite of benchmark datasets to compare the performance of the anomaly detection methods IF_\u03b1 and PAC_\u03b1 introduced in this paper with that of the standard isolation forest and other unsupervised anomaly detection methods, for the values \u03b1 \u2208 {0, 0.5, 1, 2, \u221e}.  As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab. ", "original_text": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig. "}, "hash": "ed970a622e04b0a2d8c53b90e1527223926725b136c05825a87acb877d607fc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7728116a-494d-4ba3-9be8-98dbae8b9a65", "node_type": "1", "metadata": {"window": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets. ", "original_text": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. "}, "hash": "da84a2d5f94ab2c889cc87b3ce6f47656e633f7474d15b3dc5e29f9fb7cbd068", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4).\n", "mimetype": "text/plain", "start_char_idx": 20824, "end_char_idx": 20828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7728116a-494d-4ba3-9be8-98dbae8b9a65", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets. ", "original_text": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fcb6d55-7e85-4c98-bf91-0ac18a60e69f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "As common, we used the AUCROC metric as the score metric, using the values as stated in [3].  We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. ", "original_text": "4).\n"}, "hash": "35ca35359fca06be5331581855c3c62eae9ec04869459cfbfc91a7f0d4975c4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd2f1206-ecc7-46e3-b2df-8dd2835e2bab", "node_type": "1", "metadata": {"window": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n", "original_text": "5).\n"}, "hash": "710e20e88e2954bd6cb89e7d2fe8ccb93fde33db8982ca08bac0cca3bc7563fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. ", "mimetype": "text/plain", "start_char_idx": 20828, "end_char_idx": 21034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd2f1206-ecc7-46e3-b2df-8dd2835e2bab", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n", "original_text": "5).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7728116a-494d-4ba3-9be8-98dbae8b9a65", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We ran the experiments on 34 datasets, using the code provided in [3] and using a custom implementation of the above methods.  For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets. ", "original_text": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig. "}, "hash": "3ef4d7e839c00754e901a58cf40dd6a6e140e66af962abae11e23b667425b5cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6e7a152-e6a1-497c-a357-259ee5e40995", "node_type": "1", "metadata": {"window": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. ", "original_text": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. "}, "hash": "e1f4626a15f4f2ddd26a27ff27bd5eada50fbbd728f1a744a1528d2e4f6f4743", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5).\n", "mimetype": "text/plain", "start_char_idx": 21034, "end_char_idx": 21038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6e7a152-e6a1-497c-a357-259ee5e40995", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. ", "original_text": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd2f1206-ecc7-46e3-b2df-8dd2835e2bab", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For the isolation forest and its variants, we used 100 estimators and evaluated datasets for which an AUCROC value was computed.  This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n", "original_text": "5).\n"}, "hash": "7fd483b17fafa16f4d38c975bbfb906aff9bad9eb188c3c5ed3f0e6b7764fb14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ec41b48-c9b8-4c89-8b5d-945196121817", "node_type": "1", "metadata": {"window": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). ", "original_text": "To illustrate this, Tab. "}, "hash": "bfa22f740251c75e6d197a1c9b3dd9ee36769ca7a830913ae25b877a8cf9128b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. ", "mimetype": "text/plain", "start_char_idx": 21038, "end_char_idx": 21205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8ec41b48-c9b8-4c89-8b5d-945196121817", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). ", "original_text": "To illustrate this, Tab. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6e7a152-e6a1-497c-a357-259ee5e40995", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This means that we ignored datasets for which the AUCROC value was \u201cNone\u201d in the ADBench paper.\n\n ### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. ", "original_text": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods. "}, "hash": "9da8f8b6a2230592d50e524587c12bd5831db5a7cbf339b08c840d6af4c2dc6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1b890b5-f546-4262-98ac-5a9c8f5d5da2", "node_type": "1", "metadata": {"window": "4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively. ", "original_text": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. "}, "hash": "0caa10378f8f20b2837b6d931f00c341f53c68f064053b7d28bcde5b12964926", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To illustrate this, Tab. ", "mimetype": "text/plain", "start_char_idx": 21205, "end_char_idx": 21230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c1b890b5-f546-4262-98ac-5a9c8f5d5da2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively. ", "original_text": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ec41b48-c9b8-4c89-8b5d-945196121817", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "### B. Results\n\nWe obtained the following results:\n1) On the ADBench datasets, IF_\u03b1 slightly outperforms IF on average across datasets, for \u03b1 \u2208 {0.5, 1, 2}, while PAC_\u03b1 on average performs significantly poorer (see Fig.  4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). ", "original_text": "To illustrate this, Tab. "}, "hash": "5b471facdef4d3dbf1e69cabadc3ef36a856f2688e2b22ccdc5008ad63f99f48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39765e48-279d-4a8d-a503-6c82d99c53c9", "node_type": "1", "metadata": {"window": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. ", "original_text": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets. "}, "hash": "b432395168a98b9011c761db700e4cbfb3ffc3f797ce0d5b99b7348e1aba99ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. ", "mimetype": "text/plain", "start_char_idx": 21230, "end_char_idx": 21521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39765e48-279d-4a8d-a503-6c82d99c53c9", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. ", "original_text": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1b890b5-f546-4262-98ac-5a9c8f5d5da2", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "4).\n 2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively. ", "original_text": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications. "}, "hash": "6d53811e04b8b9c85cd76a5b4453a4f37b29cd44757ec75eafc34853e888b370", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af6fb5a3-a0cd-4727-a5d0-fce6a03fcb71", "node_type": "1", "metadata": {"window": "5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right).", "original_text": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n"}, "hash": "777c6b3d90f917a8b7d3eb01f7449be8f9ec74513ef84c5a8bcbfc3bc0651e6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets. ", "mimetype": "text/plain", "start_char_idx": 21521, "end_char_idx": 21591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af6fb5a3-a0cd-4727-a5d0-fce6a03fcb71", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right).", "original_text": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39765e48-279d-4a8d-a503-6c82d99c53c9", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2) Consistent with the results of [3], the variance across datasets in the performance of unsupervised anomaly detection methods is high and the algorithms defined herein are no exception to this (see Fig.  5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. ", "original_text": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets. "}, "hash": "0fd8843cc64e20f0742eb5edc08b17580d7cdac5d6618d764914e6bf0862b529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f15d337-12a6-4660-aa3a-804b5ffb96cb", "node_type": "1", "metadata": {"window": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n", "original_text": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. "}, "hash": "6a67011f2d89d8c4c212ba103023842e4e00438004f95bdd01c64edfc5e8ae20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n", "mimetype": "text/plain", "start_char_idx": 21591, "end_char_idx": 21782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f15d337-12a6-4660-aa3a-804b5ffb96cb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n", "original_text": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af6fb5a3-a0cd-4727-a5d0-fce6a03fcb71", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "5).\n 3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right).", "original_text": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n"}, "hash": "4f332e86e5c555a6a7ce62451037c0f9b98977034eb5586ba1c4826ab2985a68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "021d2a2e-f370-4646-b9fb-ae5dbb1e8172", "node_type": "1", "metadata": {"window": "To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus. ", "original_text": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). "}, "hash": "1917b4f33a7768249f8588764c16847c6a3d09a6c41e01c37202b8f961fb6d7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. ", "mimetype": "text/plain", "start_char_idx": 21782, "end_char_idx": 22047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "021d2a2e-f370-4646-b9fb-ae5dbb1e8172", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus. ", "original_text": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f15d337-12a6-4660-aa3a-804b5ffb96cb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "3) While poor on average, there are datasets in which PAC_\u03b1 anomaly detection algorithms significantly outperform their counterpart IF_\u03b1 and state-of-the-art methods.  To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n", "original_text": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin. "}, "hash": "2368fa465f81d2621011a84e04df3ef0059e0223d41064ed482a7adcd4dd7092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65d7b79e-7090-4a24-8713-109c9e76766b", "node_type": "1", "metadata": {"window": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". ", "original_text": "We fit these data to IF_0 and PAC_0 respectively. "}, "hash": "b9892d9137d4b3518338ba6afe1dfc47276e9dec32d7ba971f85fa537d3233e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). ", "mimetype": "text/plain", "start_char_idx": 22047, "end_char_idx": 22180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65d7b79e-7090-4a24-8713-109c9e76766b", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". ", "original_text": "We fit these data to IF_0 and PAC_0 respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "021d2a2e-f370-4646-b9fb-ae5dbb1e8172", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "To illustrate this, Tab.  I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus. ", "original_text": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left). "}, "hash": "92ea5a5ba165a3235e61916961b6f510b6e1b229e093bcfb12c5de2145958ae8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42782a91-638f-4f47-b67f-e50fb26532bb", "node_type": "1", "metadata": {"window": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D. ", "original_text": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. "}, "hash": "418d11f5ccdad4e57943bdd0f8fcb91fcfc24999eef9fe08eb4623abf199eb9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We fit these data to IF_0 and PAC_0 respectively. ", "mimetype": "text/plain", "start_char_idx": 22180, "end_char_idx": 22230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42782a91-638f-4f47-b67f-e50fb26532bb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D. ", "original_text": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65d7b79e-7090-4a24-8713-109c9e76766b", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "I in the appendix provides an overview of the performances of PAC_\u03b1 against IF_\u03b1\n\n---\n\u2075For reasons not entirely clear to us, we could not reproduce the results for the isolation forest that are stated in [3], even when using the authors\u2019 provided scripts and code without any modifications.  Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". ", "original_text": "We fit these data to IF_0 and PAC_0 respectively. "}, "hash": "31eb543d82bb32e90378b89ef58061eca48c2f9f5650294660bbd196f23d7c28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab28b62c-8c0e-4c49-aa0d-e36fc8a26a6d", "node_type": "1", "metadata": {"window": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n", "original_text": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right)."}, "hash": "36f5d04f74d7c321ec5cfdd276173c355fccc618590bfe0da6e20ac0de3c1a67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. ", "mimetype": "text/plain", "start_char_idx": 22230, "end_char_idx": 22369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab28b62c-8c0e-4c49-aa0d-e36fc8a26a6d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n", "original_text": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42782a91-638f-4f47-b67f-e50fb26532bb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Indeed, we found an average of 0.06 AUCROC points across 34 datasets.  While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D. ", "original_text": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3. "}, "hash": "1c1670f2e34e3714483e8656ec7222b374f44358bac3503a5d6b06095da5c57f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "208d8f99-c409-4a49-9fc9-d225cf3ff8f1", "node_type": "1", "metadata": {"window": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension). ", "original_text": "**\n\n*Figure 3 Description: The figure contains two plots.\n"}, "hash": "28e5a64677d395a3030730320257b38be5dbca39fa24de121a3ddb20bf00f555", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right).", "mimetype": "text/plain", "start_char_idx": 22369, "end_char_idx": 22576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "208d8f99-c409-4a49-9fc9-d225cf3ff8f1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension). ", "original_text": "**\n\n*Figure 3 Description: The figure contains two plots.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab28b62c-8c0e-4c49-aa0d-e36fc8a26a6d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "While this means that the comparison between the IF variants of this paper and the other algorithms should be interpreted with care, it does not affect the comparison between those variants.\n ---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n", "original_text": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right)."}, "hash": "5a42aa3b7b8203f008acaa1f39b5e653be7122c561e2b719226d92decd0e3853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bb92b2e-7708-4ceb-99f1-ac9b90ff40be", "node_type": "1", "metadata": {"window": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". ", "original_text": "Left: A scatter plot of data points forming a ring or annulus. "}, "hash": "3a195aabade82d360d051eb5bb8451e40812d5e795a035635c895bd46f1a18fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Figure 3 Description: The figure contains two plots.\n", "mimetype": "text/plain", "start_char_idx": 22576, "end_char_idx": 22634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9bb92b2e-7708-4ceb-99f1-ac9b90ff40be", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". ", "original_text": "Left: A scatter plot of data points forming a ring or annulus. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "208d8f99-c409-4a49-9fc9-d225cf3ff8f1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "---\n\n***\n**Figure 3: Motivating the use of a hypervolume-based scoring function: We sample a dataset consisting of N = 127 points uniformly from the surface of a d\u20131 dimensional sphere (with some additional Gaussian noise) and place a single anomaly at the origin.  The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension). ", "original_text": "**\n\n*Figure 3 Description: The figure contains two plots.\n"}, "hash": "61160ea27a905d6d10c07f3ec782ce550087b27f7e91cb91b5ee4ee3f7687392", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37bfe18e-8b3c-404c-a1d6-dbc421359470", "node_type": "1", "metadata": {"window": "We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. ", "original_text": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". "}, "hash": "f6c86da25ad23fafe3bf62e858ce55241d5cd44c40b46da42c21cbdb8a78b37b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Left: A scatter plot of data points forming a ring or annulus. ", "mimetype": "text/plain", "start_char_idx": 22634, "end_char_idx": 22697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37bfe18e-8b3c-404c-a1d6-dbc421359470", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. ", "original_text": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bb92b2e-7708-4ceb-99f1-ac9b90ff40be", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The anomaly is easy to separate in the full-dimensional space but \u201cscreened\u201d in any lower-dimensional projection of the data (Left).  We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". ", "original_text": "Left: A scatter plot of data points forming a ring or annulus. "}, "hash": "1d1b51d054015816f6f973b01a27b34bbfe9d3b26238ef58f676f67b0e0e7e88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c7106c1-085e-421e-891e-c3b03366b7ce", "node_type": "1", "metadata": {"window": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. ", "original_text": "The points form a circular shape, indicating a distribution on a sphere projected to 2D. "}, "hash": "328b9856d9355c4ca8321c1c6cb7789c7b2166d2af027ab4a86046179804b29e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". ", "mimetype": "text/plain", "start_char_idx": 22697, "end_char_idx": 22807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c7106c1-085e-421e-891e-c3b03366b7ce", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. ", "original_text": "The points form a circular shape, indicating a distribution on a sphere projected to 2D. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37bfe18e-8b3c-404c-a1d6-dbc421359470", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We fit these data to IF_0 and PAC_0 respectively.  Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. ", "original_text": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\". "}, "hash": "f3028ab3b3e27ad006bebd0013a236692d4db3f1c0ae2de585ff27f03eab0a26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c29194c3-a03c-4034-98cf-b092097a60d1", "node_type": "1", "metadata": {"window": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4.", "original_text": "A single point is visible at the origin (0,0), which is the anomaly.\n"}, "hash": "4717b408a90d586446e26cc378c5e6d73d30a4bb8a7c29d5e9523a8004a3c839", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The points form a circular shape, indicating a distribution on a sphere projected to 2D. ", "mimetype": "text/plain", "start_char_idx": 22807, "end_char_idx": 22896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c29194c3-a03c-4034-98cf-b092097a60d1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4.", "original_text": "A single point is visible at the origin (0,0), which is the anomaly.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c7106c1-085e-421e-891e-c3b03366b7ce", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Running this experiment a hundred times shows a strong and robust separation between the AUCROCs the two algorithms, especially for d = 3.  The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. ", "original_text": "The points form a circular shape, indicating a distribution on a sphere projected to 2D. "}, "hash": "48934bb8bc57be91d3f3862ade044318493f704788cae9149bc6bc9b952def07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33ce4614-65e7-4cf3-b9f9-2c8e108f0623", "node_type": "1", "metadata": {"window": "**\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. ", "original_text": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension). "}, "hash": "68c9e06327e9e062aa34bac5c0a348ccdc85511f34f02f464b59a0410fb8a54f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single point is visible at the origin (0,0), which is the anomaly.\n", "mimetype": "text/plain", "start_char_idx": 22896, "end_char_idx": 22965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33ce4614-65e7-4cf3-b9f9-2c8e108f0623", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. ", "original_text": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c29194c3-a03c-4034-98cf-b092097a60d1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The reason for the collapse of both algorithms for increasing d we assume to be an exponentially small likelihood of sampling random trees whose decision nodes imply full-dimensional hyperrectangles (Right). **\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4.", "original_text": "A single point is visible at the origin (0,0), which is the anomaly.\n"}, "hash": "2911b9bd05ce65dd6104d219ef9ea2f6c234e595911583d160e349b8ec64b2b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "525eb5f4-25b0-4927-9526-fc8769d56308", "node_type": "1", "metadata": {"window": "Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets.", "original_text": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". "}, "hash": "a9667bdc70a42bb6cdd6c27e57673f19d6c918f6a3ae09616c9a0573f3dc792e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension). ", "mimetype": "text/plain", "start_char_idx": 22965, "end_char_idx": 23016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "525eb5f4-25b0-4927-9526-fc8769d56308", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets.", "original_text": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33ce4614-65e7-4cf3-b9f9-2c8e108f0623", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n*Figure 3 Description: The figure contains two plots.\n Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. ", "original_text": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension). "}, "hash": "8bb7c9c9b7b076826f3427ed4ab2ff52962e5b786d17e1a2cb3a582afee0446f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e863d57-245e-4bd6-8816-a94d15991dfa", "node_type": "1", "metadata": {"window": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance.", "original_text": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. "}, "hash": "c285996b7cfb69734fce9693602ff9e372f520ccf4519edb53397362358fe8b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". ", "mimetype": "text/plain", "start_char_idx": 23016, "end_char_idx": 23089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e863d57-245e-4bd6-8816-a94d15991dfa", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance.", "original_text": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "525eb5f4-25b0-4927-9526-fc8769d56308", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Left: A scatter plot of data points forming a ring or annulus.  The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets.", "original_text": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\". "}, "hash": "33d63a8a5a79c386a5e6881b1f194b126ee2457cfdcb8016c99e0545de3304d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4650959d-305d-4138-bbd0-e739bdd22c59", "node_type": "1", "metadata": {"window": "The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n", "original_text": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. "}, "hash": "721727a5b2d2ada3c4d939b0af521bbd312cbd6f0853dc282b2af72fd0d55196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. ", "mimetype": "text/plain", "start_char_idx": 23089, "end_char_idx": 23179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4650959d-305d-4138-bbd0-e739bdd22c59", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n", "original_text": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e863d57-245e-4bd6-8816-a94d15991dfa", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The points are colored based on a legend: a blue point is labeled \"False\" and a pink point is labeled \"True\".  The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance.", "original_text": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases. "}, "hash": "b91b9cf5e894f515c4bbbb52bea46402ae2a6dfc70a3a0423d7a547a97b0eb4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7679146f-babb-4459-85e1-424f22491a16", "node_type": "1", "metadata": {"window": "A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. ", "original_text": "Both curves converge to around 0.5 for d=4."}, "hash": "8a5e9a2f23c5f1fc2ff6fd9397748c01b1e917b9447076dd3ede7d46c592f99f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. ", "mimetype": "text/plain", "start_char_idx": 23179, "end_char_idx": 23295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7679146f-babb-4459-85e1-424f22491a16", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. ", "original_text": "Both curves converge to around 0.5 for d=4."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4650959d-305d-4138-bbd0-e739bdd22c59", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The points form a circular shape, indicating a distribution on a sphere projected to 2D.  A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n", "original_text": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases. "}, "hash": "34553bb17d3d4cf8ee20cfbd8033e918fbbac26a9b116f3173dbb1cf359c4936", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91f52260-a520-44c7-82ef-043a0fb10243", "node_type": "1", "metadata": {"window": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. ", "original_text": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. "}, "hash": "63ad5eaaa2c0ed57da3dca9fd0184774e9663d0f3bd0bc23a17fab3e6039fc22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both curves converge to around 0.5 for d=4.", "mimetype": "text/plain", "start_char_idx": 23295, "end_char_idx": 23338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91f52260-a520-44c7-82ef-043a0fb10243", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. ", "original_text": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7679146f-babb-4459-85e1-424f22491a16", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A single point is visible at the origin (0,0), which is the anomaly.\n Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. ", "original_text": "Both curves converge to around 0.5 for d=4."}, "hash": "ea3bd2703f892a40a952133bc1a4fb1c1df3c100d3a9b41f463ca2ed9e4d4f6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ef10d0e-81b4-4c4f-8c34-b5fe15e27f33", "node_type": "1", "metadata": {"window": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. ", "original_text": "For completeness we also state to the average AUCROC across the considered datasets."}, "hash": "48a705e7a12c6a20158b0a1c21ebcc40ca9a798cc5b083d0d8ab4125b70e394d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. ", "mimetype": "text/plain", "start_char_idx": 23338, "end_char_idx": 23546, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ef10d0e-81b4-4c4f-8c34-b5fe15e27f33", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. ", "original_text": "For completeness we also state to the average AUCROC across the considered datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91f52260-a520-44c7-82ef-043a0fb10243", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Right: A line plot of \"AUCROC\" vs \"d\" (dimension).  Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. ", "original_text": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets. "}, "hash": "d239f0204f6c2d6be3b1cad8c4770ae368f73712d33000d1a09e0bfc37c7774d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "424e1dfb-c5ff-46be-99ce-9e29beda0aa6", "node_type": "1", "metadata": {"window": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets.", "original_text": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance."}, "hash": "b780b89034d78eab20efdf07853c4691a932e5440ec654b72817342be011446e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For completeness we also state to the average AUCROC across the considered datasets.", "mimetype": "text/plain", "start_char_idx": 23546, "end_char_idx": 23630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "424e1dfb-c5ff-46be-99ce-9e29beda0aa6", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets.", "original_text": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ef10d0e-81b4-4c4f-8c34-b5fe15e27f33", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Two lines are shown, one orange labeled \"IF\" and one blue labeled \"PAC\".  The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. ", "original_text": "For completeness we also state to the average AUCROC across the considered datasets."}, "hash": "290ad539a24ab7e24208322beace6a83dbadd25e1fcdbd56acd1da5b9adca169", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bb3a8e4-6c03-4dd4-bb82-e8424ef14a7c", "node_type": "1", "metadata": {"window": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. ", "original_text": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n"}, "hash": "1ac1f04c77f611b19046ff60857bcb277b65e3aecc58a5b18f93b840140d774f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance.", "mimetype": "text/plain", "start_char_idx": 23630, "end_char_idx": 24443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8bb3a8e4-6c03-4dd4-bb82-e8424ef14a7c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. ", "original_text": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "424e1dfb-c5ff-46be-99ce-9e29beda0aa6", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The PAC curve starts high (around 0.9) at d=2, peaks near 1.0 at d=3, and then decreases.  The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets.", "original_text": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance."}, "hash": "b7ad615508a0fd2d35130ce79dcdc851a85ed42c76ed0572718372b6e052b6da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5603cf3-7dc9-459f-ac5f-1144d713a662", "node_type": "1", "metadata": {"window": "Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. ", "original_text": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. "}, "hash": "c5317878d6251698f26aaa7cfb967b5de9f5022b1d9a4b76b651ce6780cdbdac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 24443, "end_char_idx": 24540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5603cf3-7dc9-459f-ac5f-1144d713a662", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. ", "original_text": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bb3a8e4-6c03-4dd4-bb82-e8424ef14a7c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The IF curve starts lower (around 0.75), also peaks near d=3 but at a lower value (around 0.8), and then decreases.  Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. ", "original_text": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n"}, "hash": "11d37574e4ea88f8a772332f326189a78c8f264224538b22224d6b8c4ce11776", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8382955b-ec4e-4418-9ebb-2b8c1a2823b3", "node_type": "1", "metadata": {"window": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n", "original_text": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. "}, "hash": "92a16ad389ae2fe80acb7b2605e0381a7de6d73da512197f4f08130c56079587", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. ", "mimetype": "text/plain", "start_char_idx": 24540, "end_char_idx": 24747, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8382955b-ec4e-4418-9ebb-2b8c1a2823b3", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n", "original_text": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5603cf3-7dc9-459f-ac5f-1144d713a662", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Both curves converge to around 0.5 for d=4. *\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. ", "original_text": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms. "}, "hash": "daa466db43f699a352b6cd4f93b6109f5818ff9bfb2d18a65b441364b55c0fa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2770e189-5c13-455f-be43-de19b59a9c7c", "node_type": "1", "metadata": {"window": "For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. ", "original_text": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. "}, "hash": "94f912fd6f981b9346678d815a48f74440f3fff05578d04e0c0cd64803705e5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. ", "mimetype": "text/plain", "start_char_idx": 24747, "end_char_idx": 24800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2770e189-5c13-455f-be43-de19b59a9c7c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. ", "original_text": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8382955b-ec4e-4418-9ebb-2b8c1a2823b3", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\n***\n**Figure 4: Ranking of the algorithms according to performance: Following [3], to generate the ranking, we ranked all algorithms on each dataset by AUCROC and then averaged the rank over datasets.  For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n", "original_text": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100. "}, "hash": "31e9df227d4e34a173eeedc9b86a1bd556a45b902b9d6ca2d53b25e7983f6ef1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "537498e6-e3b6-4603-8ef9-ffc3a3fa6db7", "node_type": "1", "metadata": {"window": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n", "original_text": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets."}, "hash": "ecd879dd7b955d2369fc116814c9f72fd4646289863fdd300bd4467ae2a8738f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. ", "mimetype": "text/plain", "start_char_idx": 24800, "end_char_idx": 24995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "537498e6-e3b6-4603-8ef9-ffc3a3fa6db7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n", "original_text": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2770e189-5c13-455f-be43-de19b59a9c7c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For completeness we also state to the average AUCROC across the considered datasets. **\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. ", "original_text": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper. "}, "hash": "e81763e765d43db4e886d5366c46930c922fc8fb82e690244a65cedf37ef03a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f1b35e6-4998-4e39-8108-23148439e89f", "node_type": "1", "metadata": {"window": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV. ", "original_text": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. "}, "hash": "d8d17de6698369cfc7a4551fe1d49e1b2602f93128713d41bc88177d6a49dbdb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets.", "mimetype": "text/plain", "start_char_idx": 24995, "end_char_idx": 25203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f1b35e6-4998-4e39-8108-23148439e89f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV. ", "original_text": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "537498e6-e3b6-4603-8ef9-ffc3a3fa6db7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "**\n\n| Average rank | Algorithm | Average AUCROC |\n| :--- | :--- | :--- |\n| 8.53 | IFAlpha1 | 75.68 |\n| 8.62 | IFAlpha0.5 | 75.62 |\n| 8.78 | IFAlpha2 | 75.71 |\n| 8.88 | IForestAlpha0 (IF) | 75.52 |\n| 10.16 | CBLOF | 72.83 |\n| 10.22 | PACForest1 | 74.03 |\n| 10.60 | PCA | 73.23 |\n| 10.75 | PACForest2 | 75.54 |\n| 11.51 | HBOS | 72.81 |\n| 11.85 | ECOD | 72.76 |\n| 12.04 | COPOD | 72.95 |\n| 12.13 | KNN | 70.27 |\n| 12.85 | PACForest0.5 | 70.02 |\n| 13.04 | IForestAlphainf | 72.63 |\n| 13.31 | PACForestinf | 72.63 |\n| 13.96 | SOD | 68.01 |\n| 14.28 | OCSVM | 69.15 |\n| 14.72 | LOF | 63.37 |\n| 14.78 | COF | 64.08 |\n| 15.06 | PACForest0 | 64.00 |\n| 16.09 | LODA | 66.39 |\n| 16.72 | GMM | 65.31 |\n| 18.34 | SVDD | 53.16 |\n\n*(Note: An arrow points from the top to the bottom of the list, indicating decreasing performance. )*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n", "original_text": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets."}, "hash": "e9f2c73c5f78620438d52efc45d158a770600a724bd83e3ab59e926ff05fcd75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0be070bd-2da0-4f8d-836b-88513f67665f", "node_type": "1", "metadata": {"window": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. ", "original_text": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. "}, "hash": "118e6eb648fd8433dd00116ee6e169b85c747ef754a2459d9e41c5f0b07ea5d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. ", "mimetype": "text/plain", "start_char_idx": 25203, "end_char_idx": 25419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0be070bd-2da0-4f8d-836b-88513f67665f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. ", "original_text": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f1b35e6-4998-4e39-8108-23148439e89f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": ")*\n***\n\nand the best of the other comparison algorithms, for a selected subset of the datasets.\n\n ***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV. ", "original_text": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception. "}, "hash": "720c9263a38b80f8b98f0a001e36f9ab5bb1fc77b95b95b9dcc152fecc38135d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dd389aa-95d1-4715-a80f-085346c7f668", "node_type": "1", "metadata": {"window": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. ", "original_text": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n"}, "hash": "4b5bf32b6dff1120f8990f6a2e836d5ef35028d1fe8fd2c3750a2e842d6a06ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. ", "mimetype": "text/plain", "start_char_idx": 25419, "end_char_idx": 25647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8dd389aa-95d1-4715-a80f-085346c7f668", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. ", "original_text": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0be070bd-2da0-4f8d-836b-88513f67665f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "***\n**Figure 5: Boxplot of AUCROC IF variants and SOTA algorithms**\n\n*Figure 5 Description: This is a boxplot chart showing the distribution of AUCROC scores (in %) for various anomaly detection algorithms.  The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. ", "original_text": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e. "}, "hash": "7a27405cb4176ecf76016a760bb9414a8c6903fc87e2994546707b7da6890c10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0332ff5-e24a-40ee-8f9b-b383f67c80fa", "node_type": "1", "metadata": {"window": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n", "original_text": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. "}, "hash": "56c307b88149586e4016ea40cc1c9be187267ce15fd1ef2ab55a41ff649f0ddb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n", "mimetype": "text/plain", "start_char_idx": 25647, "end_char_idx": 25795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0332ff5-e24a-40ee-8f9b-b383f67c80fa", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n", "original_text": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dd389aa-95d1-4715-a80f-085346c7f668", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The y-axis is \"AUCROC in (%)\" ranging from 0 to 100.  The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. ", "original_text": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n"}, "hash": "3d6e97c4da94bb1aa0dc6152d03b1160acba948a2aa6b72e8a2cb48de7134798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d663c787-ffbd-433f-a30c-71ddd4b07fd8", "node_type": "1", "metadata": {"window": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. ", "original_text": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n"}, "hash": "4f1cddb1a93d48c7f15e32ffd82b8d06add7ec78347f147ae04e41e5e81a71c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. ", "mimetype": "text/plain", "start_char_idx": 25795, "end_char_idx": 26026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d663c787-ffbd-433f-a30c-71ddd4b07fd8", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. ", "original_text": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0332ff5-e24a-40ee-8f9b-b383f67c80fa", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The x-axis lists the algorithms, including PCA, OCSVM, LOF, CBLOF, COF, HBOS, KNN, SOD, COPOD, ECOD, SVDD, GMM, LODA, and the different variants of IForest and PACForest introduced in the paper.  For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n", "original_text": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor. "}, "hash": "df3744d62953e36c9cfb0703328a37cbf0b634a2d4433f82ed72548b8974aaa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38a8a6f5-ca36-4376-a675-02dfbcc666ca", "node_type": "1", "metadata": {"window": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. ", "original_text": "## IV. "}, "hash": "c7b5030e2923d3c2eb68214f1cfa97312e2b12404ff584dd1ca7ee28496d0555", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n", "mimetype": "text/plain", "start_char_idx": 26026, "end_char_idx": 26215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38a8a6f5-ca36-4376-a675-02dfbcc666ca", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. ", "original_text": "## IV. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d663c787-ffbd-433f-a30c-71ddd4b07fd8", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For most algorithms, the median AUCROC is high (around 80-90%), but the distributions show wide variance with many outliers, especially on the lower end, indicating performance varies greatly across datasets. *\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. ", "original_text": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n"}, "hash": "62791d88af0fb43ffb23983079184477357a3295f9efe4227a9c82fee0994390", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "505a18b5-e38a-4120-81a5-13dc6ce261c9", "node_type": "1", "metadata": {"window": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n", "original_text": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. "}, "hash": "33f0f3c7d82d6cea1836566909da0c1a9b189c44759e7dc0ff34915a5f9cd6bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## IV. ", "mimetype": "text/plain", "start_char_idx": 26215, "end_char_idx": 26222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "505a18b5-e38a-4120-81a5-13dc6ce261c9", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n", "original_text": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38a8a6f5-ca36-4376-a675-02dfbcc666ca", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*\n***\n\n### C. Discussion\n\nThe findings confirm the verdict from [3] that there is not one anomaly detection method that suits all purposes, and the anomaly detection methods discussed in this paper are no exception.  At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. ", "original_text": "## IV. "}, "hash": "c7fdc2ed180809413020ebd39a3aee5d90371d03c0caefaacdcbdb144309df1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80caa3be-7178-4c52-9eaf-e9b1aff58905", "node_type": "1", "metadata": {"window": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr. ", "original_text": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. "}, "hash": "f977354640c20b1b02d88b12b9a4aad308d2c46c62bd5f770de276dcd381edcb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. ", "mimetype": "text/plain", "start_char_idx": 26222, "end_char_idx": 26458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80caa3be-7178-4c52-9eaf-e9b1aff58905", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr. ", "original_text": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "505a18b5-e38a-4120-81a5-13dc6ce261c9", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "At the same time, we see that the introduction of distribution-based scoring has an overall positive, albeit mild, impact on the performance of IF, when \u03b1 is relatively small, while performance significantly decreases as \u03b1 \u2192 \u221e.  This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n", "original_text": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm. "}, "hash": "6d499a55db63b15fc6fefaf27fd73debd22881f180068c56813c86174a2dff28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "214d5e59-6340-40d3-9785-308289e4befc", "node_type": "1", "metadata": {"window": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. ", "original_text": "We briefly discussed these results.\n\n"}, "hash": "a5ca8598fa5e5a193357451b359f9168f6ff5b950b3df4cb17db14fd6c70a25f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. ", "mimetype": "text/plain", "start_char_idx": 26458, "end_char_idx": 26702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "214d5e59-6340-40d3-9785-308289e4befc", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. ", "original_text": "We briefly discussed these results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80caa3be-7178-4c52-9eaf-e9b1aff58905", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "This confirms the intuition that some sensitivity for outlier votes across estimators helps with anomaly detection, while too much of it does not.\n\n Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr. ", "original_text": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1. "}, "hash": "abc94283dbc7ab73ee04697b17970999f93774480b4435dad603eedee400ab79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9b5255e-5172-4574-a42e-5e77d9492f1a", "node_type": "1", "metadata": {"window": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n", "original_text": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. "}, "hash": "814e1934610c2b53e0db1c436a6e28f2d05d0c65540ef1cc36c3e5c8b85e33da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We briefly discussed these results.\n\n", "mimetype": "text/plain", "start_char_idx": 26702, "end_char_idx": 26739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9b5255e-5172-4574-a42e-5e77d9492f1a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n", "original_text": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "214d5e59-6340-40d3-9785-308289e4befc", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Regarding our volume-based variant of IF, the success hinges very much on the dataset: On some datasets, we found a significant improvement over IF and other unsupervised algorithms, however, on most datasets performance was poor.  Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. ", "original_text": "We briefly discussed these results.\n\n"}, "hash": "99a24d931c40688871df16990da6251b63a89e3399557c51315be80906e16cf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e636550-aaab-4eaa-8fde-cce1dea2b765", "node_type": "1", "metadata": {"window": "## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. ", "original_text": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. "}, "hash": "b7e701fa535b695c0d817309c4a5f87cdd3ea3642a680dc59f906b61860a57ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 26739, "end_char_idx": 27005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4e636550-aaab-4eaa-8fde-cce1dea2b765", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. ", "original_text": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9b5255e-5172-4574-a42e-5e77d9492f1a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Unfortunately, at this point we lack a systematic understanding of the criteria that determine whether PAC_\u03b1 is a suitable method for a given dataset, in the absence of ground truth data.\n\n ## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n", "original_text": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection. "}, "hash": "4c9cc16cc1f525c7cb687183630417230be92c920296cb27d3aba3b54045ec20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8d8c834-c097-4d22-8316-25d9b0d3b141", "node_type": "1", "metadata": {"window": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles. ", "original_text": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n"}, "hash": "3a33be72553526d56a0dca0ee27297a3cfb0e27a4233f7c6704215657569ed49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. ", "mimetype": "text/plain", "start_char_idx": 27005, "end_char_idx": 27182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8d8c834-c097-4d22-8316-25d9b0d3b141", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles. ", "original_text": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e636550-aaab-4eaa-8fde-cce1dea2b765", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## IV.  CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. ", "original_text": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset. "}, "hash": "6f83af070645035824987093d70b2836c48f39cabbc653c8a0263bf87634a92f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a03402c6-4d08-4225-ad96-30131fdd05b0", "node_type": "1", "metadata": {"window": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. ", "original_text": "## ACKNOWLEDGMENT\n\nWe thank Dr. "}, "hash": "b705977b643f685c37be3f43dd19a9e94e97d1c0051acc5f372710acc7815630", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n", "mimetype": "text/plain", "start_char_idx": 27182, "end_char_idx": 27380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a03402c6-4d08-4225-ad96-30131fdd05b0", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. ", "original_text": "## ACKNOWLEDGMENT\n\nWe thank Dr. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8d8c834-c097-4d22-8316-25d9b0d3b141", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "CONCLUSION AND FUTURE WORK\n\nTo conclude, in this note we introduced, motivated and empirically tested two families of ensemble-based anomaly detection methods, IF_\u03b1 and PAC_\u03b1, both variants of the successful Isolation Forest algorithm.  We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles. ", "original_text": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n"}, "hash": "e23a3fada7da834d4b5f6d4bedd58362e54e4e0d35e4e76a4b63adf013173822", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b054dff6-4af7-4a64-8de2-99b93e5cf2a7", "node_type": "1", "metadata": {"window": "We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n", "original_text": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. "}, "hash": "fd959d1df198dd886f5e62e8212b6ee8439c12cb0711ecb80c66c2bb6ac1f246", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## ACKNOWLEDGMENT\n\nWe thank Dr. ", "mimetype": "text/plain", "start_char_idx": 27380, "end_char_idx": 27412, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b054dff6-4af7-4a64-8de2-99b93e5cf2a7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n", "original_text": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a03402c6-4d08-4225-ad96-30131fdd05b0", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We evaluated them on 34 datasets from the recent and exhaustive \u201cADBench\u201d benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for IF_\u03b1.  We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. ", "original_text": "## ACKNOWLEDGMENT\n\nWe thank Dr. "}, "hash": "fbf2f214672c5a806ed4d4e8b48106bfed204fc7c04b98e866abca9c10303623", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cf1180e-d9fd-43de-b2e7-3814981b61e6", "node_type": "1", "metadata": {"window": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. ", "original_text": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n"}, "hash": "3ba28a2a56718b92135a77a9b3d29f879aef370aff1649c3e884657ad7f9888e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. ", "mimetype": "text/plain", "start_char_idx": 27412, "end_char_idx": 27520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7cf1180e-d9fd-43de-b2e7-3814981b61e6", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. ", "original_text": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b054dff6-4af7-4a64-8de2-99b93e5cf2a7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We briefly discussed these results.\n\n For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n", "original_text": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof. "}, "hash": "94570b39f82fa5fb550b2bc25b80c37efadc86ce77614eed416bc376a6002099", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25596639-5205-4f7a-8584-391e2b41a3ed", "node_type": "1", "metadata": {"window": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification. ", "original_text": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. "}, "hash": "4896f382d3fc7fe5314441027231af1177460bbe7a4d25eefd19c8dd73eaa987", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n", "mimetype": "text/plain", "start_char_idx": 27520, "end_char_idx": 27615, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "25596639-5205-4f7a-8584-391e2b41a3ed", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification. ", "original_text": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cf1180e-d9fd-43de-b2e7-3814981b61e6", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "For future work, we believe that it could be beneficial to apply the distribution-based aggregation functions h_\u03b1 also to other ensemble-based anomaly detection methods, such as LODA [13], or even ensemble-based methods in machine learning beyond anomaly detection.  We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. ", "original_text": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n"}, "hash": "fa183f4ebd74987de4e304860ce1aba381c51c2cdab5c4bd01c89c2087e39034", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f8018dc-c1cc-469f-9bb1-20a6a9e21a7b", "node_type": "1", "metadata": {"window": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n", "original_text": "Task-relevant failure detection for trajectory predictors in autonomous vehicles. "}, "hash": "7ef242822a0202e80108724b8f9d33282363e830f5f85733b02f1af0a5f51195", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. ", "mimetype": "text/plain", "start_char_idx": 27615, "end_char_idx": 27707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f8018dc-c1cc-469f-9bb1-20a6a9e21a7b", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n", "original_text": "Task-relevant failure detection for trajectory predictors in autonomous vehicles. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25596639-5205-4f7a-8584-391e2b41a3ed", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "We also believe that it would be valuable to research the possibility of an \u201cauto-tuning\u201d mechanism for \u03b1, that is, some way of tuning \u03b1 based on an unlabeled training dataset.  Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification. ", "original_text": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone. "}, "hash": "9d61700993131ac068eac8cf54050242fbc806a155b3281098986b09c08ca4b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eea19528-4f7f-44aa-a4ae-6e7deae710e2", "node_type": "1", "metadata": {"window": "## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. ", "original_text": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. "}, "hash": "cda92f14cfa9e31e5e93649d3d06725c809fe8fce84443f4eca9cb7dc2efc997", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Task-relevant failure detection for trajectory predictors in autonomous vehicles. ", "mimetype": "text/plain", "start_char_idx": 27707, "end_char_idx": 27789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eea19528-4f7f-44aa-a4ae-6e7deae710e2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. ", "original_text": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f8018dc-c1cc-469f-9bb1-20a6a9e21a7b", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Finally, regarding PAC_\u03b1, we believe that it would be interesting to better understand the types of anomaly datasets on which hypervolume-based scoring is a better choice than depth-based scoring.\n\n ## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n", "original_text": "Task-relevant failure detection for trajectory predictors in autonomous vehicles. "}, "hash": "ee4603f382566331a89a01f014960dc181ecd9c11999d6a5eae5f1a60d10ce80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bda947c-6f94-41ce-94a5-91495fe88b3c", "node_type": "1", "metadata": {"window": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark. ", "original_text": "PMLR, 14\u201318 Dec 2023.\n"}, "hash": "36a59085882cc21788f75fefd90a75ba46aba9a9492cab6b501940abac8bae02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. ", "mimetype": "text/plain", "start_char_idx": 27789, "end_char_idx": 27974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5bda947c-6f94-41ce-94a5-91495fe88b3c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark. ", "original_text": "PMLR, 14\u201318 Dec 2023.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eea19528-4f7f-44aa-a4ae-6e7deae710e2", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## ACKNOWLEDGMENT\n\nWe thank Dr.  Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. ", "original_text": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969. "}, "hash": "ce5e25add2a9994b60092600070fecd0f7e1bdbfde8ce306c7cbde7b9efee561", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f072b20-f05e-4087-a734-6b82638105d9", "node_type": "1", "metadata": {"window": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n", "original_text": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. "}, "hash": "49843914350905954adaadc8b424765e126383b48059c29012e1984a1180993c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PMLR, 14\u201318 Dec 2023.\n", "mimetype": "text/plain", "start_char_idx": 27974, "end_char_idx": 27996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f072b20-f05e-4087-a734-6b82638105d9", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n", "original_text": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bda947c-6f94-41ce-94a5-91495fe88b3c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Mahdi Manesh, Regina Kirschner, and the Porsche PANAMERA project for valuable discussions, as well as Prof.  Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark. ", "original_text": "PMLR, 14\u201318 Dec 2023.\n"}, "hash": "d143b2e4b4cb7f27afa9243ba5b5484506ebeac1ac73602efeb9b90290d526b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d24d0d-c859-477c-9f5c-846902c0ef59", "node_type": "1", "metadata": {"window": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. ", "original_text": "Pidforest: Anomaly detection via partial identification. "}, "hash": "4808840fb693ef74f43e54ab9336c92db3aa47ba86ba99db99983da9205eb87c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. ", "mimetype": "text/plain", "start_char_idx": 27996, "end_char_idx": 28050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18d24d0d-c859-477c-9f5c-846902c0ef59", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. ", "original_text": "Pidforest: Anomaly detection via partial identification. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f072b20-f05e-4087-a734-6b82638105d9", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Jean-Pierre Seifert and Niklas Pirnay for discussions during the early phase of this project.\n\n ## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n", "original_text": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder. "}, "hash": "35e5f2a80a78357415a15a89999856683c7a18336ce500972ccab6cedd0ded47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb0ddb42-557d-4b61-b14c-cc9e71c0b8ae", "node_type": "1", "metadata": {"window": "Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest. ", "original_text": "2019.\n"}, "hash": "5433a5b4550b425ba9e9891b58b8b312d3bbbddfc64b26db03991f6a6dfea200", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pidforest: Anomaly detection via partial identification. ", "mimetype": "text/plain", "start_char_idx": 28050, "end_char_idx": 28107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb0ddb42-557d-4b61-b14c-cc9e71c0b8ae", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest. ", "original_text": "2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d24d0d-c859-477c-9f5c-846902c0ef59", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## REFERENCES\n\n[1] Alec Farid, Sushant Veer, Boris Ivanovic, Karen Leung, and Marco Pavone.  Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. ", "original_text": "Pidforest: Anomaly detection via partial identification. "}, "hash": "a79e6ddef5ef0f8f97ef4c3848ae0d7cccebbfe10d7835614b2024e0184086b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d701d3e-660c-499c-ab38-8d7ddc9d25ed", "node_type": "1", "metadata": {"window": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n", "original_text": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. "}, "hash": "9aaf8e5fd4eca7a3a0a02667c2ea48fd8f97d81d4cf1436ed5b2e43925b93f6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2019.\n", "mimetype": "text/plain", "start_char_idx": 28107, "end_char_idx": 28113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d701d3e-660c-499c-ab38-8d7ddc9d25ed", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n", "original_text": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb0ddb42-557d-4b61-b14c-cc9e71c0b8ae", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Task-relevant failure detection for trajectory predictors in autonomous vehicles.  In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest. ", "original_text": "2019.\n"}, "hash": "d4c83c6a273b607cafb9729d97f9f91834ecf2f269abc07c8a740edc9ff8fad2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9a908b1-a7e9-4ae8-b771-4225a88211d7", "node_type": "1", "metadata": {"window": "PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. ", "original_text": "Adbench: Anomaly detection benchmark. "}, "hash": "1418dee335e840c6e96e3ebe7ac53b4138ce996677a98dbf41e7c3cbe1f316b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. ", "mimetype": "text/plain", "start_char_idx": 28113, "end_char_idx": 28186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a9a908b1-a7e9-4ae8-b771-4225a88211d7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. ", "original_text": "Adbench: Anomaly detection benchmark. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d701d3e-660c-499c-ab38-8d7ddc9d25ed", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, *Proceedings of The 6th Conference on Robot Learning*, volume 205 of *Proceedings of Machine Learning Research*, pages 1959\u20131969.  PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n", "original_text": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. "}, "hash": "dee042e4ca08bd4326f4010ee58c6e89e7d52684ee6f6c9a86f2789785085123", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "878599a6-90dc-4556-a8b5-b8eccde9fa75", "node_type": "1", "metadata": {"window": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things. ", "original_text": "2022.\n"}, "hash": "adcd23ae07f2dc22a49a82e3b28a72ab390aa5d1cd29aabd7835bfd42eeffa3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Adbench: Anomaly detection benchmark. ", "mimetype": "text/plain", "start_char_idx": 28186, "end_char_idx": 28224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "878599a6-90dc-4556-a8b5-b8eccde9fa75", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things. ", "original_text": "2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9a908b1-a7e9-4ae8-b771-4225a88211d7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "PMLR, 14\u201318 Dec 2023.\n [2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. ", "original_text": "Adbench: Anomaly detection benchmark. "}, "hash": "567127c784073abdd54c9617edfb36b722419b19d4c2b18a03cc5ee673a89be1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c83384f7-7a66-4f9c-9696-ed2f1b686af1", "node_type": "1", "metadata": {"window": "Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n", "original_text": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. "}, "hash": "b368b8eaf990af770b8aa076c6bcbd6dfa27f8930338119fa335ae639b69ecbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2022.\n", "mimetype": "text/plain", "start_char_idx": 28224, "end_char_idx": 28230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c83384f7-7a66-4f9c-9696-ed2f1b686af1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n", "original_text": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "878599a6-90dc-4556-a8b5-b8eccde9fa75", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[2] Parikshit Gopalan, Vatsal Sharan, and Udi Wieder.  Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things. ", "original_text": "2022.\n"}, "hash": "fe4489eda0781e67e9f68cdd0b4817ad5c65758b99772ebd53e316e70c35a1c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c840fb63-b550-482c-95bf-923834d72aff", "node_type": "1", "metadata": {"window": "2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "original_text": "Extended isolation forest. "}, "hash": "52b8d82d7f4f53232086c647595ebaea680e51fe4337ede94b708ec3a8954811", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. ", "mimetype": "text/plain", "start_char_idx": 28230, "end_char_idx": 28294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c840fb63-b550-482c-95bf-923834d72aff", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "original_text": "Extended isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c83384f7-7a66-4f9c-9696-ed2f1b686af1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Pidforest: Anomaly detection via partial identification.  2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n", "original_text": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. "}, "hash": "3e7f08cfc60e7b7278d8ae806f810e225778256bb1aad39c13ba5c9e0b589587", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6a50082-fc5e-4e42-b02b-98858af5c21b", "node_type": "1", "metadata": {"window": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection. ", "original_text": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n"}, "hash": "1233467960ad14da50d26df0735341b42bc6a734dd24777f591bb1b727aabe64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extended isolation forest. ", "mimetype": "text/plain", "start_char_idx": 28294, "end_char_idx": 28321, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e6a50082-fc5e-4e42-b02b-98858af5c21b", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection. ", "original_text": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c840fb63-b550-482c-95bf-923834d72aff", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2019.\n [3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "original_text": "Extended isolation forest. "}, "hash": "bd349b9240d7e223f0aea7e72e14223423de14651c85a52b4bfe5affdb4dfc5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bdbabef-994e-4b35-be3c-d7692dbbcf2b", "node_type": "1", "metadata": {"window": "Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n", "original_text": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. "}, "hash": "4da43fb135eb171402c8f3a40ac1ef1dc427f8951df2b19294559bc8113a41ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n", "mimetype": "text/plain", "start_char_idx": 28321, "end_char_idx": 28403, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1bdbabef-994e-4b35-be3c-d7692dbbcf2b", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n", "original_text": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6a50082-fc5e-4e42-b02b-98858af5c21b", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[3] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao.  Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection. ", "original_text": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n"}, "hash": "3b45f95be2829e8ed628791b4da6368307fd4e9d2a8695976c4a2fa526371ea0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09a39184-6410-4993-936a-6b93733e3e4e", "node_type": "1", "metadata": {"window": "2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. ", "original_text": "Incremental anomaly detection with guarantee in the internet of medical things. "}, "hash": "8279f64d27da983055de89e03ff9e5fa73168c57e38aea342263c85aec283a21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. ", "mimetype": "text/plain", "start_char_idx": 28403, "end_char_idx": 28464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09a39184-6410-4993-936a-6b93733e3e4e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. ", "original_text": "Incremental anomaly detection with guarantee in the internet of medical things. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bdbabef-994e-4b35-be3c-d7692dbbcf2b", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Adbench: Anomaly detection benchmark.  2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n", "original_text": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee. "}, "hash": "031fe75077d8758ee90f9df6a4eaf16bc1f389a13b7115404ab994e8d4b73437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cfee1f1-340c-48be-b817-2078c04f7a5b", "node_type": "1", "metadata": {"window": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap. ", "original_text": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n"}, "hash": "ca64da80b797dbdec8db7f65bc607f1fde4647ee8ff065b60082d39c51b8942f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Incremental anomaly detection with guarantee in the internet of medical things. ", "mimetype": "text/plain", "start_char_idx": 28464, "end_char_idx": 28544, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5cfee1f1-340c-48be-b817-2078c04f7a5b", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap. ", "original_text": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09a39184-6410-4993-936a-6b93733e3e4e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2022.\n [4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. ", "original_text": "Incremental anomaly detection with guarantee in the internet of medical things. "}, "hash": "8371c4b4c63510ee44b2de2133a7d38f0b5e9afa39c1e08d68bce657b9158e04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de74901-4978-4d7a-82eb-e9a5591c85a7", "node_type": "1", "metadata": {"window": "Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. ", "original_text": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. "}, "hash": "cf6db38390e8bc89703cd8046c2be9768192338da4e2b10428ff87fc683f45bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n", "mimetype": "text/plain", "start_char_idx": 28544, "end_char_idx": 28644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6de74901-4978-4d7a-82eb-e9a5591c85a7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. ", "original_text": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cfee1f1-340c-48be-b817-2078c04f7a5b", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[4] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner.  Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap. ", "original_text": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n"}, "hash": "5ed20675c8c875009049a0cf4331c6c310ef904a5499be1d3c69fbf35fc7ca76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5edb5b41-a226-4526-ab08-c6934c123f13", "node_type": "1", "metadata": {"window": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n", "original_text": "Generalized isolation forest for anomaly detection. "}, "hash": "8df0d0b3e4fe304992d34936adcd6985b7d71548735d9ba1d4d479b4410522c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "mimetype": "text/plain", "start_char_idx": 28644, "end_char_idx": 28719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5edb5b41-a226-4526-ab08-c6934c123f13", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n", "original_text": "Generalized isolation forest for anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6de74901-4978-4d7a-82eb-e9a5591c85a7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Extended isolation forest.  *IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. ", "original_text": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. "}, "hash": "9a2c554eac513c90cb34dcbd48b6364d2d5e0eb541c5beec7a0b8e1a6fe97ff7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6c325e3-5e0b-4fb2-9b57-e41b1c6dbc34", "node_type": "1", "metadata": {"window": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n"}, "hash": "f2652c866e0a0d7c18235234df9ef24f3825735d74a3a46f7574fb7619ff8fd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generalized isolation forest for anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 28719, "end_char_idx": 28771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6c325e3-5e0b-4fb2-9b57-e41b1c6dbc34", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5edb5b41-a226-4526-ab08-c6934c123f13", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*IEEE Transactions on Knowledge and Data Engineering*, 33(4):1479\u20131489, apr 2021.\n [5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n", "original_text": "Generalized isolation forest for anomaly detection. "}, "hash": "331ec8c22bcfcd765eb446758cc896c410f51a598481f6b265ba2a0c5654e801", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd1b83dc-615d-4d8c-8f67-c057ae1e560d", "node_type": "1", "metadata": {"window": "Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. "}, "hash": "a78ff7d124f90981e968a768254f6a187856d4fb2af89403c920ed1336290786", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n", "mimetype": "text/plain", "start_char_idx": 28771, "end_char_idx": 28831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd1b83dc-615d-4d8c-8f67-c057ae1e560d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6c325e3-5e0b-4fb2-9b57-e41b1c6dbc34", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[5] Xiayan Ji, Hyonyoung Choi, Oleg Sokolsky, and Insup Lee.  Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n"}, "hash": "6e3a85b4e4d37392a6d98ccf451d9bf3b743c8f79406b38943ec8f754c049e94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a22fee9f-c037-460c-88c2-c11597625b97", "node_type": "1", "metadata": {"window": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n", "original_text": "PAC-wrap. "}, "hash": "3c364bddb2fb5cb034304e4f6bcfa71ddb81ae069eb38f0f1ed9d8a092f1f3fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. ", "mimetype": "text/plain", "start_char_idx": 28831, "end_char_idx": 28901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a22fee9f-c037-460c-88c2-c11597625b97", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n", "original_text": "PAC-wrap. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd1b83dc-615d-4d8c-8f67-c057ae1e560d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Incremental anomaly detection with guarantee in the internet of medical things.  *Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee. "}, "hash": "a975ef3953ab77813946687c22b8b7e80fdf9d9a41aa16a3d70a03c3320980ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07f6954d-69fa-40de-9912-bf6ba96beedb", "node_type": "1", "metadata": {"window": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. "}, "hash": "06b76d703ec3ccc08cd29c524374ffcdb28ae56870897d5a60070c0ec829da7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PAC-wrap. ", "mimetype": "text/plain", "start_char_idx": 28901, "end_char_idx": 28911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07f6954d-69fa-40de-9912-bf6ba96beedb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a22fee9f-c037-460c-88c2-c11597625b97", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation*, 2023.\n [6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n", "original_text": "PAC-wrap. "}, "hash": "c9118c14e83a9d1ab9046d67f6153d926b32428c3f626702b831972c84e1f444", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fd9a334-ca28-42e2-881f-486f4d8970d8", "node_type": "1", "metadata": {"window": "Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest. ", "original_text": "ACM, aug 2022.\n"}, "hash": "8b1cd7d94136758a0c5621406eee0a05b3b6f3f0fe55cf3b5abd2b25696abe25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. ", "mimetype": "text/plain", "start_char_idx": 28911, "end_char_idx": 29002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7fd9a334-ca28-42e2-881f-486f4d8970d8", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest. ", "original_text": "ACM, aug 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07f6954d-69fa-40de-9912-bf6ba96beedb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[6] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. "}, "hash": "d2e43136730c9bc50c132325f810df859544ab2e26c3e870537add52bdb3d2c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7debbc4d-58bf-48b0-b8ec-8f7398f989bb", "node_type": "1", "metadata": {"window": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n", "original_text": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "1b5bd531482797f5019051e5309894f0999d377c60bd720cfbaa8dcc18628368", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ACM, aug 2022.\n", "mimetype": "text/plain", "start_char_idx": 29002, "end_char_idx": 29017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7debbc4d-58bf-48b0-b8ec-8f7398f989bb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n", "original_text": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fd9a334-ca28-42e2-881f-486f4d8970d8", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest. ", "original_text": "ACM, aug 2022.\n"}, "hash": "211fcd8cca35338a26ca7d06f4b011c62b19dd2570d99face1956cb12ade5a26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cbe7ebb-d265-4d9a-96c5-0239103ceccf", "node_type": "1", "metadata": {"window": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "Isolation forest. "}, "hash": "651ab27c71e1fdd01b32fe511451b28bef402df85dbff1e7a1d96b89e68365b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "mimetype": "text/plain", "start_char_idx": 29017, "end_char_idx": 29068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cbe7ebb-d265-4d9a-96c5-0239103ceccf", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "Isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7debbc4d-58bf-48b0-b8ec-8f7398f989bb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Pattern Recognition Letters*, 149:109\u2013119, September 2021.\n [7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n", "original_text": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "5db68a87e4de6c2d2fbfab24efc2169bd708d3f4d2a40ad29e622d137047cf41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61bf935c-0680-4e13-8d8c-a958f2e78a35", "node_type": "1", "metadata": {"window": "PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection. ", "original_text": "pages 413\u2013422, 2008.\n"}, "hash": "90a3114f261ad149ab9815e9ae72f630ddbdc5ad4df7a57a7d422d56b2e0536c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation forest. ", "mimetype": "text/plain", "start_char_idx": 29068, "end_char_idx": 29086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61bf935c-0680-4e13-8d8c-a958f2e78a35", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection. ", "original_text": "pages 413\u2013422, 2008.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cbe7ebb-d265-4d9a-96c5-0239103ceccf", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[7] Shuo Li, Xiayan Ji, Edgar Dobriban, Oleg Sokolsky, and Insup Lee.  PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "Isolation forest. "}, "hash": "10e297f8d24a3451b8f3aedf214f0c1ad3cf820f8e3f90d643f3bb38a9ca3b12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32abed02-02b0-4139-8a92-6b06de2f4c3f", "node_type": "1", "metadata": {"window": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans. ", "original_text": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "ebfd7149f5d4781973fb86db4567bee8350cae71e7b41b7b2db808b0967bc1c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pages 413\u2013422, 2008.\n", "mimetype": "text/plain", "start_char_idx": 29086, "end_char_idx": 29107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32abed02-02b0-4139-8a92-6b06de2f4c3f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans. ", "original_text": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61bf935c-0680-4e13-8d8c-a958f2e78a35", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "PAC-wrap.  In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection. ", "original_text": "pages 413\u2013422, 2008.\n"}, "hash": "8d6199df190fbe141b3ac9c5602f8b7c386086683a07d0f89f3461439a40cf4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9dd16ce4-4768-4b87-a9ea-2828f952924f", "node_type": "1", "metadata": {"window": "ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl. ", "original_text": "On detecting clustered anomalies using sciforest. "}, "hash": "f965032aa4c01732f7baab07282424b6e0358f98a6254b87f67e3cc36e79f1c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "mimetype": "text/plain", "start_char_idx": 29107, "end_char_idx": 29158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9dd16ce4-4768-4b87-a9ea-2828f952924f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl. ", "original_text": "On detecting clustered anomalies using sciforest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32abed02-02b0-4139-8a92-6b06de2f4c3f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*.  ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans. ", "original_text": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "a1765eebca4e2e31a4c1ddddbf4af1350e9649c7d39766c0458274abf00ede28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88195c7a-57b2-4c8d-94f8-51fbdd968aa7", "node_type": "1", "metadata": {"window": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov. ", "original_text": "2010.\n"}, "hash": "ec8d20d01f5687f82340da5743443500dc9beba68ae03bd1765ea79d9d348bea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On detecting clustered anomalies using sciforest. ", "mimetype": "text/plain", "start_char_idx": 29158, "end_char_idx": 29208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88195c7a-57b2-4c8d-94f8-51fbdd968aa7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov. ", "original_text": "2010.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9dd16ce4-4768-4b87-a9ea-2828f952924f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "ACM, aug 2022.\n [8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl. ", "original_text": "On detecting clustered anomalies using sciforest. "}, "hash": "8f466b823a96848b3b94a4ded8b1f1cb87aacba5253912df0f446d6014b1988c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa44c6c8-3620-4393-a421-606fb665d937", "node_type": "1", "metadata": {"window": "Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n", "original_text": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "49818d454fd23e52246c2cbc7ff95c9e29ff8158bc999ba16569d2497389e876", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2010.\n", "mimetype": "text/plain", "start_char_idx": 29208, "end_char_idx": 29214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aa44c6c8-3620-4393-a421-606fb665d937", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n", "original_text": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88195c7a-57b2-4c8d-94f8-51fbdd968aa7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[8] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov. ", "original_text": "2010.\n"}, "hash": "39b107022bed6f4653f9eaec229777d8143abfbfa118a791fbe4cce95c4c098b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2bc8596-eb37-48cb-946e-62304345e9a9", "node_type": "1", "metadata": {"window": "pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego. ", "original_text": "Isolation-based anomaly detection. "}, "hash": "182955d7dd5e309a55991302e007d3536f00c13732b33d0d9bd31305d5221f7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "mimetype": "text/plain", "start_char_idx": 29214, "end_char_idx": 29266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2bc8596-eb37-48cb-946e-62304345e9a9", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego. ", "original_text": "Isolation-based anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa44c6c8-3620-4393-a421-606fb665d937", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Isolation forest.  pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n", "original_text": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "fbd1d9cf582bc13014523b2f71c042df43ceb8bba24a01304a300c1a10e098aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "573ea18d-7ee2-4f4b-9dc1-a0f53b528cc0", "node_type": "1", "metadata": {"window": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests. ", "original_text": "*ACM Trans. "}, "hash": "73a96ffb7323aa247bfb706393fa3f1e8498c15affdf470b73a44d2b028ce2b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation-based anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 29266, "end_char_idx": 29301, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "573ea18d-7ee2-4f4b-9dc1-a0f53b528cc0", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests. ", "original_text": "*ACM Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2bc8596-eb37-48cb-946e-62304345e9a9", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "pages 413\u2013422, 2008.\n [9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego. ", "original_text": "Isolation-based anomaly detection. "}, "hash": "a19368eeef163fb33afef614ef1519c39559b0061e48ba1613f510bbd1755bbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b22fb645-4a81-4f4f-a092-7e8ff139ab7c", "node_type": "1", "metadata": {"window": "On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n", "original_text": "Knowl. "}, "hash": "67aae2bcbe67e6c11e7f6c9ab5ddca7214c2fe90d994557cb4efbae9c7c334fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*ACM Trans. ", "mimetype": "text/plain", "start_char_idx": 29301, "end_char_idx": 29313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b22fb645-4a81-4f4f-a092-7e8ff139ab7c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "573ea18d-7ee2-4f4b-9dc1-a0f53b528cc0", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[9] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests. ", "original_text": "*ACM Trans. "}, "hash": "944c1ba00ada239ec79fd5323bc2cd16b77f2697cf870d5682c35b6d77356c61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "637c0e1a-f113-4f63-8abe-52c55eaf8dd3", "node_type": "1", "metadata": {"window": "2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. ", "original_text": "Discov. "}, "hash": "8bf1d2cc365fa5bcb15216f0de43aa4d7d737100bb6de00e048c8072ada2431f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 29313, "end_char_idx": 29320, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "637c0e1a-f113-4f63-8abe-52c55eaf8dd3", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. ", "original_text": "Discov. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b22fb645-4a81-4f4f-a092-7e8ff139ab7c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "On detecting clustered anomalies using sciforest.  2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n", "original_text": "Knowl. "}, "hash": "2ee70263aa2a16fe0568e632cff95088e09dae633d56315b760fe0968ebf99af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9601774d-41b2-482c-b78d-b5d17643258e", "node_type": "1", "metadata": {"window": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems. ", "original_text": "Data*, 6:3:1\u20133:39, 2012.\n"}, "hash": "84cfbae5bcc6c476d800af12d84807b996005f00c93f14636c055d666cfd6549", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discov. ", "mimetype": "text/plain", "start_char_idx": 29320, "end_char_idx": 29328, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9601774d-41b2-482c-b78d-b5d17643258e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems. ", "original_text": "Data*, 6:3:1\u20133:39, 2012.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "637c0e1a-f113-4f63-8abe-52c55eaf8dd3", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2010.\n [10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. ", "original_text": "Discov. "}, "hash": "617909d4e356ab71d6a47e1ecceb0db5eefdcf76c96f07bb45088100bdc8f3a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3352419f-5c73-4a3e-b1f4-ae99cf4f6122", "node_type": "1", "metadata": {"window": "Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n", "original_text": "[11] Antonella Mensi and Manuele Bicego. "}, "hash": "b4f5b13dc0dbbc3a0907074280d601b5a605a2ae0216e4248e3620ae4dc18c8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data*, 6:3:1\u20133:39, 2012.\n", "mimetype": "text/plain", "start_char_idx": 29328, "end_char_idx": 29353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3352419f-5c73-4a3e-b1f4-ae99cf4f6122", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n", "original_text": "[11] Antonella Mensi and Manuele Bicego. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9601774d-41b2-482c-b78d-b5d17643258e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[10] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems. ", "original_text": "Data*, 6:3:1\u20133:39, 2012.\n"}, "hash": "8746eb5cb0ddeab9adc70f413767a485b809e0c91bc100ba0d7ddfbcb03817bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52727b84-95a6-4f91-917c-1cf4fc373cd7", "node_type": "1", "metadata": {"window": "*ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd. ", "original_text": "Enhanced anomaly scores for isolation forests. "}, "hash": "86949006a4ad27fbe249ae4f316369cf1869cbc4880e78ede9e4cb054c81ae18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] Antonella Mensi and Manuele Bicego. ", "mimetype": "text/plain", "start_char_idx": 29353, "end_char_idx": 29394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52727b84-95a6-4f91-917c-1cf4fc373cd7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd. ", "original_text": "Enhanced anomaly scores for isolation forests. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3352419f-5c73-4a3e-b1f4-ae99cf4f6122", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Isolation-based anomaly detection.  *ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n", "original_text": "[11] Antonella Mensi and Manuele Bicego. "}, "hash": "5bc1729ba2cedd356f860d498cf561011fdee811b5c85fa4356f18e39eda4fbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e79da223-7f28-4b48-8a30-60cdf5e25416", "node_type": "1", "metadata": {"window": "Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies. ", "original_text": "*Pattern Recognition*, 120:108115, December 2021.\n"}, "hash": "24a5af47f447e9229b0f769beb96ae88ed9852ce54d52f1acb1d446570e33a6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Enhanced anomaly scores for isolation forests. ", "mimetype": "text/plain", "start_char_idx": 29394, "end_char_idx": 29441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e79da223-7f28-4b48-8a30-60cdf5e25416", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies. ", "original_text": "*Pattern Recognition*, 120:108115, December 2021.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52727b84-95a6-4f91-917c-1cf4fc373cd7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*ACM Trans.  Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd. ", "original_text": "Enhanced anomaly scores for isolation forests. "}, "hash": "f582c5f7190d2619628607e350ff7b23a15ad55626c1e99f1c8fb95a1f8394fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d06e5669-4a6e-40cf-a58a-eb493f8fa52a", "node_type": "1", "metadata": {"window": "Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n", "original_text": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. "}, "hash": "1010cae44f8136ed5e04a54ac8ad347245000d174d10d8b538d6823e61054ee3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Pattern Recognition*, 120:108115, December 2021.\n", "mimetype": "text/plain", "start_char_idx": 29441, "end_char_idx": 29491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d06e5669-4a6e-40cf-a58a-eb493f8fa52a", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n", "original_text": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e79da223-7f28-4b48-8a30-60cdf5e25416", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Knowl.  Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies. ", "original_text": "*Pattern Recognition*, 120:108115, December 2021.\n"}, "hash": "4604fe742b9ba5aee79f8af033985cd5da1497c97b35537ab8bbf5dfd7b6764a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9450cd0f-a204-4400-9172-6e9e37ca9cf7", "node_type": "1", "metadata": {"window": "Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi. ", "original_text": "A learning anomaly detection algorithm for hybrid manufacturing systems. "}, "hash": "1cd38c7691dd783af5b16ec35fd8fa4d943a9d3c8e0bfec261f87a5e791ce717", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. ", "mimetype": "text/plain", "start_char_idx": 29491, "end_char_idx": 29593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9450cd0f-a204-4400-9172-6e9e37ca9cf7", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi. ", "original_text": "A learning anomaly detection algorithm for hybrid manufacturing systems. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d06e5669-4a6e-40cf-a58a-eb493f8fa52a", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Discov.  Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n", "original_text": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning. "}, "hash": "6aab839965ac34acd542c9808893282ce0a279b740cd6b6a11fb760e7c127f66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bccbaf4-509c-4f72-82d0-a2f61ad57e4c", "node_type": "1", "metadata": {"window": "[11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information. ", "original_text": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n"}, "hash": "e797822075ebceebf8d8c2bff9ec1bc082b1b9fe0d8f24d49f0070b8db46b811", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A learning anomaly detection algorithm for hybrid manufacturing systems. ", "mimetype": "text/plain", "start_char_idx": 29593, "end_char_idx": 29666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5bccbaf4-509c-4f72-82d0-a2f61ad57e4c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information. ", "original_text": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9450cd0f-a204-4400-9172-6e9e37ca9cf7", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Data*, 6:3:1\u20133:39, 2012.\n [11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi. ", "original_text": "A learning anomaly detection algorithm for hybrid manufacturing systems. "}, "hash": "5f0677fc20ab7d5b28b4f70b3d4402d02a3208521161f1ad309b9fa0ad0f464b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30fb5e45-2a60-48bc-b16e-e9611ce586eb", "node_type": "1", "metadata": {"window": "Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. ", "original_text": "[13] Tom\u00e1\u0161 Pevn\u00fd. "}, "hash": "393eb99319004e5c363fb8f1eab2912338a2f8f63f92af2dde1a7cb224baf7e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n", "mimetype": "text/plain", "start_char_idx": 29666, "end_char_idx": 29766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30fb5e45-2a60-48bc-b16e-e9611ce586eb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. ", "original_text": "[13] Tom\u00e1\u0161 Pevn\u00fd. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bccbaf4-509c-4f72-82d0-a2f61ad57e4c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[11] Antonella Mensi and Manuele Bicego.  Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information. ", "original_text": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n"}, "hash": "d41ace06dcbbf764037f76840f6e9d3a7ae742525c02ec1934841f1eec217f55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "774abe28-cd25-45fc-8c5e-1e2d279ddce0", "node_type": "1", "metadata": {"window": "*Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n", "original_text": "Loda: Lightweight on-line detector of anomalies. "}, "hash": "8b8cf863bea5e0a234c605813929b278d7bbb4c8e21187f2cbf8cda297ba3611", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[13] Tom\u00e1\u0161 Pevn\u00fd. ", "mimetype": "text/plain", "start_char_idx": 29766, "end_char_idx": 29784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "774abe28-cd25-45fc-8c5e-1e2d279ddce0", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n", "original_text": "Loda: Lightweight on-line detector of anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30fb5e45-2a60-48bc-b16e-e9611ce586eb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Enhanced anomaly scores for isolation forests.  *Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. ", "original_text": "[13] Tom\u00e1\u0161 Pevn\u00fd. "}, "hash": "fca43197dbbab9a81c384c2f38a4393ec87eee74c6432d440fe08a98defbc56e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68c2e86d-4279-4c95-be1b-1e2da0108722", "node_type": "1", "metadata": {"window": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. ", "original_text": "*Machine Learning*, 102(2):275\u2013304, 2016.\n"}, "hash": "864bda0ea5565d70b0258db3d7ad2cbb34f6a6ea24239ece93135b58d6a8758e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Loda: Lightweight on-line detector of anomalies. ", "mimetype": "text/plain", "start_char_idx": 29784, "end_char_idx": 29833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "68c2e86d-4279-4c95-be1b-1e2da0108722", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. ", "original_text": "*Machine Learning*, 102(2):275\u2013304, 2016.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "774abe28-cd25-45fc-8c5e-1e2d279ddce0", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Pattern Recognition*, 120:108115, December 2021.\n [12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n", "original_text": "Loda: Lightweight on-line detector of anomalies. "}, "hash": "67bdd61610e337328c733caf9612bc5844ed59bb326e74262864634e45e263aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdf37ca6-117a-4ba1-9edd-de4c94d03265", "node_type": "1", "metadata": {"window": "A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection. ", "original_text": "[14] Alfr\u00e9d R\u00e9nyi. "}, "hash": "a60a831837366f773e660ba3803bf2b31695b56ed1722de192d41241e9800b47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Machine Learning*, 102(2):275\u2013304, 2016.\n", "mimetype": "text/plain", "start_char_idx": 29833, "end_char_idx": 29875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdf37ca6-117a-4ba1-9edd-de4c94d03265", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection. ", "original_text": "[14] Alfr\u00e9d R\u00e9nyi. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68c2e86d-4279-4c95-be1b-1e2da0108722", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[12] Oliver Niggemann, Asmir Vodencarevic, Alexander Maier, Stefan Wind-mann, and Hans Kleine B\u00fcning.  A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. ", "original_text": "*Machine Learning*, 102(2):275\u2013304, 2016.\n"}, "hash": "835101396c1d4bf827b8b7016ea9f96beff5a4a098fc9ef8e3d5696bb49d88ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d28c704-4042-4fdf-aa46-6b80ce22bb58", "node_type": "1", "metadata": {"window": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n", "original_text": "On measures of entropy and information. "}, "hash": "d20583eebe7ebffd1fdefd1134c85b0736fc1d41e54f2d87979635e2604a9290", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] Alfr\u00e9d R\u00e9nyi. ", "mimetype": "text/plain", "start_char_idx": 29875, "end_char_idx": 29894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d28c704-4042-4fdf-aa46-6b80ce22bb58", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n", "original_text": "On measures of entropy and information. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdf37ca6-117a-4ba1-9edd-de4c94d03265", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A learning anomaly detection algorithm for hybrid manufacturing systems.  In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection. ", "original_text": "[14] Alfr\u00e9d R\u00e9nyi. "}, "hash": "f7d67b49aa2101aa1101040503512d9f2df83cd4604b04e40e3931264fb6ce64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52321a51-a018-4f5b-8abe-9ebe5f20fd1d", "node_type": "1", "metadata": {"window": "[13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. ", "original_text": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. "}, "hash": "9d0c60cde4676a4473c34b81ab9e4f6286f837a227d89fb9188afc22c1634c2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On measures of entropy and information. ", "mimetype": "text/plain", "start_char_idx": 29894, "end_char_idx": 29934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52321a51-a018-4f5b-8abe-9ebe5f20fd1d", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. ", "original_text": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d28c704-4042-4fdf-aa46-6b80ce22bb58", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In *The 24th International Workshop on Principles of Diagnosis (DX-2013)*, Jerusalem, Israel, 2013.\n [13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n", "original_text": "On measures of entropy and information. "}, "hash": "5a6449463cafe6285da2d615e346e638fd2b50cc36ac4c22df90056189a17ac2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "001a53b2-6a88-40a1-967b-dd55e4ad9ddf", "node_type": "1", "metadata": {"window": "Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data. ", "original_text": "University of California Press, 1961.\n"}, "hash": "8762629a45bb16fa3f1ad928fd5a640ce18555f27938c62606207f196e6a36bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. ", "mimetype": "text/plain", "start_char_idx": 29934, "end_char_idx": 30106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "001a53b2-6a88-40a1-967b-dd55e4ad9ddf", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data. ", "original_text": "University of California Press, 1961.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52321a51-a018-4f5b-8abe-9ebe5f20fd1d", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[13] Tom\u00e1\u0161 Pevn\u00fd.  Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. ", "original_text": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562. "}, "hash": "0f47085bba482566bbf46c7c5ea18c1b10dce83e95852f9d9984c80aeccf53a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13e6d1bb-4146-4682-ae5e-8530166ee2d4", "node_type": "1", "metadata": {"window": "*Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n", "original_text": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. "}, "hash": "af8fc26bcd1eddd6125e6ca5807741cae178a0f78b728f92caec7d0021730108", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "University of California Press, 1961.\n", "mimetype": "text/plain", "start_char_idx": 30106, "end_char_idx": 30144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13e6d1bb-4146-4682-ae5e-8530166ee2d4", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n", "original_text": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "001a53b2-6a88-40a1-967b-dd55e4ad9ddf", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Loda: Lightweight on-line detector of anomalies.  *Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data. ", "original_text": "University of California Press, 1961.\n"}, "hash": "a8211e7a3e5544ef59fe889c4a4e6fac1525dc62a63ccac7774aca8022a04fd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "137f0467-41d8-4c0b-8b5f-9aff7f882b76", "node_type": "1", "metadata": {"window": "[14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. ", "original_text": "Finite sample complexity of rare pattern anomaly detection. "}, "hash": "fff613a003162833ab5d023ae40817732d5628af23e7185c37d149ab46c168f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. ", "mimetype": "text/plain", "start_char_idx": 30144, "end_char_idx": 30213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "137f0467-41d8-4c0b-8b5f-9aff7f882b76", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. ", "original_text": "Finite sample complexity of rare pattern anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13e6d1bb-4146-4682-ae5e-8530166ee2d4", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Machine Learning*, 102(2):275\u2013304, 2016.\n [14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n", "original_text": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das. "}, "hash": "76aec10356b436a20ee6a6d46f0897bf4f751fcb9c546066ae9d899e5d73dbc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2223eee6-ae83-49bc-9c6c-d4112f89703f", "node_type": "1", "metadata": {"window": "On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest. ", "original_text": "2016.\n"}, "hash": "f4580c222096637bb3e34a87c2948c3eaee65be48916f2460374d30682458bf6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finite sample complexity of rare pattern anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 30213, "end_char_idx": 30273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2223eee6-ae83-49bc-9c6c-d4112f89703f", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest. ", "original_text": "2016.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "137f0467-41d8-4c0b-8b5f-9aff7f882b76", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[14] Alfr\u00e9d R\u00e9nyi.  On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. ", "original_text": "Finite sample complexity of rare pattern anomaly detection. "}, "hash": "b8b37ed1dba16f6b0533b8f51eb91a2e50fcf0ae2654d2e6eaf58b01a6ee6935", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b5ca416-1fb7-4839-b180-b4d31d65e612", "node_type": "1", "metadata": {"window": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n", "original_text": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. "}, "hash": "1bbc690ab75533ac8769e7a1491f3a31694109ee621a3126e5f41f136df32acc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2016.\n", "mimetype": "text/plain", "start_char_idx": 30273, "end_char_idx": 30279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b5ca416-1fb7-4839-b180-b4d31d65e612", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n", "original_text": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2223eee6-ae83-49bc-9c6c-d4112f89703f", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "On measures of entropy and information.  In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest. ", "original_text": "2016.\n"}, "hash": "4c413ff3ab5a99edde5edad90f63246c73d902fa94efd535baa0db87413d13d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c88d30f-4afc-4b28-a758-9a218f879195", "node_type": "1", "metadata": {"window": "University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos. ", "original_text": "Anomaly detection in high dimensional data. "}, "hash": "85bdb0f608726cd41833a4a1fe85e74133f5037e6ae345b971542aee47997d94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. ", "mimetype": "text/plain", "start_char_idx": 30279, "end_char_idx": 30348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c88d30f-4afc-4b28-a758-9a218f879195", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos. ", "original_text": "Anomaly detection in high dimensional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b5ca416-1fb7-4839-b180-b4d31d65e612", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "In *Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics*, volume 4, pages 547\u2013562.  University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n", "original_text": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles. "}, "hash": "43fd157a9a301a71f29831ba3f700466844458544f70e313536451f9ca4ca132", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5855ef21-71ea-4265-bc67-f9b526f2a5da", "node_type": "1", "metadata": {"window": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence. ", "original_text": "2019.\n"}, "hash": "96cedba767fb74d921024539f694aefe2290444e24471b2e3bcf5231616b8142", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly detection in high dimensional data. ", "mimetype": "text/plain", "start_char_idx": 30348, "end_char_idx": 30392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5855ef21-71ea-4265-bc67-f9b526f2a5da", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence. ", "original_text": "2019.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c88d30f-4afc-4b28-a758-9a218f879195", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "University of California Press, 1961.\n [15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos. ", "original_text": "Anomaly detection in high dimensional data. "}, "hash": "2f3b402f844c85b4d9cdf3d8835a9c53d8b8812ef42ea1ac18490f715c18b401", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7e26518-4965-4218-9980-934231086ae9", "node_type": "1", "metadata": {"window": "Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n", "original_text": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. "}, "hash": "88ccc6583fe8e33aefb1570221c6f5c46385751e657123d88b17c8288a8f4891", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2019.\n", "mimetype": "text/plain", "start_char_idx": 30392, "end_char_idx": 30398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7e26518-4965-4218-9980-934231086ae9", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n", "original_text": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5855ef21-71ea-4265-bc67-f9b526f2a5da", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[15] Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, and S. Das.  Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence. ", "original_text": "2019.\n"}, "hash": "d0dc76d72e786a848eec867516069870e56442cafa81f534442755a2a494c755", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ab7ca39-e52b-4ace-af53-30e4150cb2a4", "node_type": "1", "metadata": {"window": "2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. ", "original_text": "A probabilistic generalization of isolation forest. "}, "hash": "bae0126886f5aa4544db887220c9082d8341e8dbf24ac0ee52d46e56b049ce08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. ", "mimetype": "text/plain", "start_char_idx": 30398, "end_char_idx": 30443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ab7ca39-e52b-4ace-af53-30e4150cb2a4", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. ", "original_text": "A probabilistic generalization of isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7e26518-4965-4218-9980-934231086ae9", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Finite sample complexity of rare pattern anomaly detection.  2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n", "original_text": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek. "}, "hash": "1ee536df184a21940ced988961c2ebb16ee3e5ee5b31bb06c1721007ac629ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fd10fc6-cd3f-4762-afae-2b6ce2c9e013", "node_type": "1", "metadata": {"window": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection. ", "original_text": "*Information Sciences*, 584:433\u2013449, January 2022.\n"}, "hash": "da15f4c4b6c431e3f4fa046892390365a860fc7661403920db0e6d3e4cf8b788", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A probabilistic generalization of isolation forest. ", "mimetype": "text/plain", "start_char_idx": 30443, "end_char_idx": 30495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fd10fc6-cd3f-4762-afae-2b6ce2c9e013", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection. ", "original_text": "*Information Sciences*, 584:433\u2013449, January 2022.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ab7ca39-e52b-4ace-af53-30e4150cb2a4", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2016.\n [16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. ", "original_text": "A probabilistic generalization of isolation forest. "}, "hash": "4f490c8b8f46efca46d4922f6ec67e9ce22659d77705a58f7bc842fd6a689610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e389595-8d8b-4b04-b27b-20298fda2001", "node_type": "1", "metadata": {"window": "Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n", "original_text": "[18] Tim van Erven and Peter Harremos. "}, "hash": "6483571e0bd9715f94110013a55727d67b9228975a105f5b13f223f0b2a0e364", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Information Sciences*, 584:433\u2013449, January 2022.\n", "mimetype": "text/plain", "start_char_idx": 30495, "end_char_idx": 30546, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e389595-8d8b-4b04-b27b-20298fda2001", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n", "original_text": "[18] Tim van Erven and Peter Harremos. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fd10fc6-cd3f-4762-afae-2b6ce2c9e013", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[16] Priyanga Dilini Talagala, Rob J. Hyndman, and Kate Smith-Miles.  Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection. ", "original_text": "*Information Sciences*, 584:433\u2013449, January 2022.\n"}, "hash": "990b4695acdd043c0a0172bdc27e31007536d6e464b1e4a104b9e67a543a6f76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f97044e-60ac-464f-906f-726060301b78", "node_type": "1", "metadata": {"window": "2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. ", "original_text": "R\u00e9nyi divergence and kullback-leibler divergence. "}, "hash": "3ff0f9c758863b2e2793e37d512c13fc5f516e5e8d8fc74c94f14c9730370490", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] Tim van Erven and Peter Harremos. ", "mimetype": "text/plain", "start_char_idx": 30546, "end_char_idx": 30585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f97044e-60ac-464f-906f-726060301b78", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. ", "original_text": "R\u00e9nyi divergence and kullback-leibler divergence. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e389595-8d8b-4b04-b27b-20298fda2001", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Anomaly detection in high dimensional data.  2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n", "original_text": "[18] Tim van Erven and Peter Harremos. "}, "hash": "95f4d4142b3b31d8c21b2dcc266a52ce8e6752d1830659eded53302c66fead1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59b4206b-040a-4a0a-a2f4-0fc9c24a9bb1", "node_type": "1", "metadata": {"window": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. ", "original_text": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n"}, "hash": "b9ce94fab28e8c0f440a8136ab05582c1eb4cbf567d8247d789b507298f9ff26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "R\u00e9nyi divergence and kullback-leibler divergence. ", "mimetype": "text/plain", "start_char_idx": 30585, "end_char_idx": 30635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59b4206b-040a-4a0a-a2f4-0fc9c24a9bb1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. ", "original_text": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f97044e-60ac-464f-906f-726060301b78", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "2019.\n [17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. ", "original_text": "R\u00e9nyi divergence and kullback-leibler divergence. "}, "hash": "c20b8787cd5c459d582fe2d9bbe87e0f18c5786d4aa79acdd7ff8bd0db03f2cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da0f61cd-c449-45f8-aa68-1fd9ae746cda", "node_type": "1", "metadata": {"window": "A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. ", "original_text": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. "}, "hash": "318ae18afd5d95f20119b6352df8530ec5294e8a59c6e7d1f896b8bdbc090386", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n", "mimetype": "text/plain", "start_char_idx": 30635, "end_char_idx": 30701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da0f61cd-c449-45f8-aa68-1fd9ae746cda", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. ", "original_text": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59b4206b-040a-4a0a-a2f4-0fc9c24a9bb1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[17] Mikhail Tokovarov and Pawe\u0142 Karczmarek.  A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. ", "original_text": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n"}, "hash": "0cf100c11f86322f83d072951598c98e8cf2354c8714fa9d9cf12225ddfb8a29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f38170a-b137-4fe4-9eb4-1fc6d9fa5dc8", "node_type": "1", "metadata": {"window": "*Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. ", "original_text": "Deep isolation forest for anomaly detection. "}, "hash": "7da158674e1f4e996e27fb958abf550a2e7bd6a9f84a4c3a596059938ccc5deb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. ", "mimetype": "text/plain", "start_char_idx": 30701, "end_char_idx": 30763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f38170a-b137-4fe4-9eb4-1fc6d9fa5dc8", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. ", "original_text": "Deep isolation forest for anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da0f61cd-c449-45f8-aa68-1fd9ae746cda", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "A probabilistic generalization of isolation forest.  *Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. ", "original_text": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. "}, "hash": "ff219a9b26b35df4d4e88dfeab06c9e86bfc83028b0b839a048b79d463752ab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "102e8c84-47cd-4843-aaf1-8fae3af56096", "node_type": "1", "metadata": {"window": "[18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n", "original_text": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n"}, "hash": "b1ce60c8eb2a0caacedce7b103cd4acfc6c66cc0843fd144fd8980f510bd97bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Deep isolation forest for anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 30763, "end_char_idx": 30808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "102e8c84-47cd-4843-aaf1-8fae3af56096", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n", "original_text": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f38170a-b137-4fe4-9eb4-1fc6d9fa5dc8", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*Information Sciences*, 584:433\u2013449, January 2022.\n [18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. ", "original_text": "Deep isolation forest for anomaly detection. "}, "hash": "b70e61af828604c243ab680d003d7a21cc7ac905663f3865138e68d10916e787", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f430ed70-e165-4c80-9f9d-1056b4790b5e", "node_type": "1", "metadata": {"window": "R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. ", "original_text": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. "}, "hash": "166c07311614185c540edbb7e75256201ee22901c39bda3eac0cf8798d29c43a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n", "mimetype": "text/plain", "start_char_idx": 30808, "end_char_idx": 30882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f430ed70-e165-4c80-9f9d-1056b4790b5e", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. ", "original_text": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "102e8c84-47cd-4843-aaf1-8fae3af56096", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[18] Tim van Erven and Peter Harremos.  R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n", "original_text": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n"}, "hash": "ff6e0b0687101ce546f41cc70974399b327235edfd9a625bef13cb22950f3b11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f57dca2a-4307-43e7-85df-b593f73d42e9", "node_type": "1", "metadata": {"window": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold. ", "original_text": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. "}, "hash": "0fc82516e0c170b31ad05518759bb1b37febe08b00bcdd2c873062175ff8cfc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. ", "mimetype": "text/plain", "start_char_idx": 30882, "end_char_idx": 31229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f57dca2a-4307-43e7-85df-b593f73d42e9", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold. ", "original_text": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f430ed70-e165-4c80-9f9d-1056b4790b5e", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "R\u00e9nyi divergence and kullback-leibler divergence.  *IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. ", "original_text": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}. "}, "hash": "e058a297faf2c1f4b94e0df0abcca659c4ea5d5485272f7e51c7bdd5edcb4e9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3f8cf94-2b9d-4ec0-a4cd-ff83f06005a2", "node_type": "1", "metadata": {"window": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value.", "original_text": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. "}, "hash": "a2d1f7980fc8155b1d585fbe6cbb5314d3936c1112fdc3f8d837544d13c1a90a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. ", "mimetype": "text/plain", "start_char_idx": 31229, "end_char_idx": 31424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3f8cf94-2b9d-4ec0-a4cd-ff83f06005a2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value.", "original_text": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f57dca2a-4307-43e7-85df-b593f73d42e9", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*IEEE Transactions on Information Theory*, 60(7):3797\u20133820, 2014.\n [19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold. ", "original_text": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18]. "}, "hash": "2b673490f0866bb287ba50d5cd65347b32fe34a339142e93a05ba929279afb56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13bbe57c-b006-445f-98ac-6fe36f8e5825", "node_type": "1", "metadata": {"window": "Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. "}, "hash": "168a6931326bfda494401abecc9391c5f27daa9aef42cf090520084474b703b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. ", "mimetype": "text/plain", "start_char_idx": 31424, "end_char_idx": 31616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13bbe57c-b006-445f-98ac-6fe36f8e5825", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3f8cf94-2b9d-4ec0-a4cd-ff83f06005a2", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "[19] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang.  Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value.", "original_text": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones. "}, "hash": "622e42c63555b9c4ab074b1156e671f8f0b98761a03d320ddfe24240550b4213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "665c25bc-9697-4b21-ba49-6af4b8e60be1", "node_type": "1", "metadata": {"window": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "Theorems 3, 4 and 6 in [18].\n\n"}, "hash": "13f8fc0a3795bca584b923f2eea2762b060cb2606c1a039fc419934c7cd9ddbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. ", "mimetype": "text/plain", "start_char_idx": 31616, "end_char_idx": 31729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "665c25bc-9697-4b21-ba49-6af4b8e60be1", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "Theorems 3, 4 and 6 in [18].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13bbe57c-b006-445f-98ac-6fe36f8e5825", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Deep isolation forest for anomaly detection.  *IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g. "}, "hash": "30d89ce235de1b84abacb702955eddda29e3684b46591dd8f6b7d03f11ade230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2d170da-5042-492c-92cf-283c04447d1c", "node_type": "1", "metadata": {"window": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. "}, "hash": "fe017213fc52c61f568a2a9d1c72de96a9e4781d7770297e3f3d693d1df3cd89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Theorems 3, 4 and 6 in [18].\n\n", "mimetype": "text/plain", "start_char_idx": 31729, "end_char_idx": 31759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2d170da-5042-492c-92cf-283c04447d1c", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "665c25bc-9697-4b21-ba49-6af4b8e60be1", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "*IEEE Transactions on Knowledge and Data Engineering*, pages 1\u201314, 2023.\n\n ## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "Theorems 3, 4 and 6 in [18].\n\n"}, "hash": "4a8a35c216453d0e59305fe49fce9d95f252b360f31e37d022b9cb5b8096d5ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cda87f02-9b22-4c27-b578-054b4cafd0c3", "node_type": "1", "metadata": {"window": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The highest AUCROC is highlighted in bold. "}, "hash": "e5f6aa4a1efda83f089b44d5a2811ab092f822ad21bce480037c0ba5bca77533", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. ", "mimetype": "text/plain", "start_char_idx": 31759, "end_char_idx": 31865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cda87f02-9b22-4c27-b578-054b4cafd0c3", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The highest AUCROC is highlighted in bold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2d170da-5042-492c-92cf-283c04447d1c", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "## APPENDIX\n\n### A. Properties of f_\u03b1\n\nTo clarify the connection of the functions f_\u03b1 to information theory and show the claimed properties, let us first define the \u03b1-R\u00e9nyi divergence for \u03b1 \u2208 (0, 1) \u222a (1,\u221e), and p, q \u2208 R^d_{\u22650} as\n\nR_\u03b1(p||q) = \\frac{1}{\u03b1-1} ln \\sum_{i=1}^d p_i^\u03b1 q_i^{1-\u03b1}\n\nwith R_\u03b2(p||q) = lim_{\u03b1\u2192\u03b2} R_\u03b1(p||q) for \u03b2 \u2208 {0, 1, \u221e}.  The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets. "}, "hash": "9a517f7889f021ed6d41c927317c3ff7066306595c201f9437d4bb11a15fca84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a52fd724-5a1d-4963-8764-904268c7bbfb", "node_type": "1", "metadata": {"window": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value."}, "hash": "7c693b22e094751ae559110c59f2fcb0c1b858b8a8e8ccc30e3925bc6b4ab95d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The highest AUCROC is highlighted in bold. ", "mimetype": "text/plain", "start_char_idx": 31865, "end_char_idx": 31908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a52fd724-5a1d-4963-8764-904268c7bbfb", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cda87f02-9b22-4c27-b578-054b4cafd0c3", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The R\u00e9nyi divergences, introduced in [14], generalize the Kullback-Leibler divergence, or relative entropy (which corresponds to \u03b1 = 1) and have a variety of use cases in information [14], [18].  Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The highest AUCROC is highlighted in bold. "}, "hash": "7df8650f23baa030fff0469f3a50ddbb31b4385171274cfbc28e57854b5a4a79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19176a5e-6bd6-4255-90a9-9a56955b65c2", "node_type": "1", "metadata": {"window": "The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "**\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |"}, "hash": "0cf81f05d78549de97dd5e9dc988e8bd7b1aeb9cf4527603780974747b59709e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value.", "mimetype": "text/plain", "start_char_idx": 31908, "end_char_idx": 32001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19176a5e-6bd6-4255-90a9-9a56955b65c2", "embedding": null, "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "**\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "eee7ca07-ed24-48db-87d5-3d59a41de174", "node_type": "4", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf"}, "hash": "11c611cd90787e54d9308906c4d78a8916928deed81bc8eb0591f025f0be6f55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a52fd724-5a1d-4963-8764-904268c7bbfb", "node_type": "1", "metadata": {"title": "Distribution and volume based scoring for Isolation Forests", "authors": "Dhouib et al.", "year": 2023, "file_path": "ad-papers-pdf/scoring_isolation_forest.pdf", "window": "Here, we link them to the aggregation functions introduced in the main text via the simple identity\n\nf_\u03b1(x) = exp(-R_\u03b1(\\frac{x}{||x||_1} || \\frac{1}{n})) ,\n\nwhere **1** is the vector of ones.  The claimed properties of f_\u03b1 are then a direct consequence of the properties of the Renyi divergences, see e.g.  Theorems 3, 4 and 6 in [18].\n\n ***\n\n**TABLE I: Detailed results for the AUCROC for SOTA algorithms and IF variants on selected datasets.  The highest AUCROC is highlighted in bold.  The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value. **\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "original_text": "The best value of \u03b1 and best SOTA algorithm are given in parentheses behind the AUCROC value."}, "hash": "533a0d1705c94c4bde1104f0158555a24155020850acfd72719d1d41cac7982d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Dataset | best IF_\u03b1 | best PAC_\u03b1 | best SOTA |\n| :--- | :--- | :--- | :--- |\n| annthyroid | 82.12 (0) | **91.47** (0.5) | 82.12 (IForest) |\n| cover | **96.58** (\u221e) | 92.57 (\u221e) | 93.73 (PCA) |\n| landsat | 48.79 (1) | **66.24** (0) | 63.61 (SVDD) |\n| thyroid | 97.84 (0.5) | **98.85** (0) | 97.83 (IForest) |\n| Waveform | 69.49 (1) | **86.88** (0) | 75.03 (COPOD) |\n| wine | 76.4 (0.5) | **92.59** (0.5) | 91.36 (HBOS) |\n| WPBC | 52.66 (0) | **63.63** (0.5) | 52.66 (IForest) |", "mimetype": "text/plain", "start_char_idx": 32001, "end_char_idx": 32482, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]