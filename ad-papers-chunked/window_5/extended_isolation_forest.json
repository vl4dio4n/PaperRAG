[{"id_": "f00a9907-c74b-4ff9-820e-b57086a24043", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation. ", "original_text": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a25b47ca-a6aa-4b55-a85e-222324a5c606", "node_type": "1", "metadata": {"window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. ", "original_text": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. "}, "hash": "5523be6b554bb244fd6e519f8c6f9e1dea6d10efa020e92f801b8502440652bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a25b47ca-a6aa-4b55-a85e-222324a5c606", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. ", "original_text": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f00a9907-c74b-4ff9-820e-b57086a24043", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation. ", "original_text": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest. "}, "hash": "797b42f03eb0ebb563eecad8b52a00786b8c5ec97796ce62fb005710ecd83342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12ce9c3e-3fc1-4c66-b026-d4920ae0ecb8", "node_type": "1", "metadata": {"window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. ", "original_text": "We motivate the problem using heat maps for anomaly scores. "}, "hash": "91db5d0585c88435120f5bf30ae54ed77c05182f3bbd48fb40fe74e169130410", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. ", "mimetype": "text/plain", "start_char_idx": 190, "end_char_idx": 316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "12ce9c3e-3fc1-4c66-b026-d4920ae0ecb8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. ", "original_text": "We motivate the problem using heat maps for anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a25b47ca-a6aa-4b55-a85e-222324a5c606", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. ", "original_text": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. "}, "hash": "d45d0cf74e293e74c96945c613471256538a60585601593078fa118c466f17ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edb036e0-8c66-4a9c-9c3f-765204833f2a", "node_type": "1", "metadata": {"window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps. ", "original_text": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. "}, "hash": "a8510a2ede4bddf9fd3bde3704e1b26aac16f2994939ff9a5f282f96f8fba382", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We motivate the problem using heat maps for anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 316, "end_char_idx": 376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "edb036e0-8c66-4a9c-9c3f-765204833f2a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps. ", "original_text": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12ce9c3e-3fc1-4c66-b026-d4920ae0ecb8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. ", "original_text": "We motivate the problem using heat maps for anomaly scores. "}, "hash": "b0eae0cd472749d239a00359f5d94bf7bb475c0da58574a6c7441cb074d0d9c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f04c0550-13f3-4a45-9234-3db292c7cd2d", "node_type": "1", "metadata": {"window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. ", "original_text": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually. "}, "hash": "fd7f8bdc50b31697ce613757699d95edcff5c31023ee97f6c282f795e56bd5f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. ", "mimetype": "text/plain", "start_char_idx": 376, "end_char_idx": 479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f04c0550-13f3-4a45-9234-3db292c7cd2d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. ", "original_text": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edb036e0-8c66-4a9c-9c3f-765204833f2a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps. ", "original_text": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. "}, "hash": "2a3426154f49898b025ba287a18b85f63f0eac78dd20faac949b9e0121f0f0f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4763feb-a5cf-46d2-b673-4fe4eec4365e", "node_type": "1", "metadata": {"window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. ", "original_text": "We then propose two different approaches for improving the situation. "}, "hash": "9e2b3cc880e0c65850f39bf44728aaad9c5f54730fe85b253c19c548148a4e21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually. ", "mimetype": "text/plain", "start_char_idx": 479, "end_char_idx": 572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4763feb-a5cf-46d2-b673-4fe4eec4365e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. ", "original_text": "We then propose two different approaches for improving the situation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f04c0550-13f3-4a45-9234-3db292c7cd2d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. ", "original_text": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually. "}, "hash": "df1fa2dfe7a434823a0b2250822f019f3479f272f672984e6ffabc7507f621e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a342dfa6-7cc9-452d-97dc-00f7f2e774c7", "node_type": "1", "metadata": {"window": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n", "original_text": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. "}, "hash": "eac818056e5f2740f2f0f5df4d643b469637ff7d42c26d4166d4819f9ce6fd60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We then propose two different approaches for improving the situation. ", "mimetype": "text/plain", "start_char_idx": 572, "end_char_idx": 642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a342dfa6-7cc9-452d-97dc-00f7f2e774c7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n", "original_text": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4763feb-a5cf-46d2-b673-4fe4eec4365e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "# Extended Isolation Forest\nSahand Hariri, Matias Carrasco Kind, and Robert J. Brunner\n\n**Abstract**\u2014We present an extension to the model-free anomaly detection algorithm, Isolation Forest.  This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. ", "original_text": "We then propose two different approaches for improving the situation. "}, "hash": "9136a1aa950b6a53d58b45be80fd84a5ab350bba9f89a9c3c902f2651070a9b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66768e0-1a50-4c49-a865-05e89db2f1c9", "node_type": "1", "metadata": {"window": "We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. ", "original_text": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. "}, "hash": "0f92232fc9ac73be104c54048bb46e9e9113e6c87a2adfae9abbde18e3db144c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. ", "mimetype": "text/plain", "start_char_idx": 642, "end_char_idx": 761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f66768e0-1a50-4c49-a865-05e89db2f1c9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. ", "original_text": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a342dfa6-7cc9-452d-97dc-00f7f2e774c7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points.  We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n", "original_text": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. "}, "hash": "bb870b928966573b861f009507364f4c35c9823b2bceb360792c4140308ffd8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e468c5a7-2ae1-4f66-8d0a-708615e594ff", "node_type": "1", "metadata": {"window": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. ", "original_text": "This approach results in remedying the artifact seen in the anomaly score heat maps. "}, "hash": "16dcfea11b70d226ec067b3e233bf933e5bc7251f40a489c9720269988ff85af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. ", "mimetype": "text/plain", "start_char_idx": 761, "end_char_idx": 872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e468c5a7-2ae1-4f66-8d0a-708615e594ff", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. ", "original_text": "This approach results in remedying the artifact seen in the anomaly score heat maps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f66768e0-1a50-4c49-a865-05e89db2f1c9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We motivate the problem using heat maps for anomaly scores.  These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. ", "original_text": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. "}, "hash": "c4a2ee8a4a0f67aa112369e8a351c4646bba4bda0af7cdc8f7ef3dd7aee8b923", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "deace2ed-cdf9-4934-bba4-37023813b361", "node_type": "1", "metadata": {"window": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6]. ", "original_text": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. "}, "hash": "02d1ab7b3f05699d8e6ae4005b46a2e0ba42b0e2b14805097961af23c103ae7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This approach results in remedying the artifact seen in the anomaly score heat maps. ", "mimetype": "text/plain", "start_char_idx": 872, "end_char_idx": 957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "deace2ed-cdf9-4934-bba4-37023813b361", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6]. ", "original_text": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e468c5a7-2ae1-4f66-8d0a-708615e594ff", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These maps suffer from artifacts generated by the criteria for branching operation of the binary tree.  We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. ", "original_text": "This approach results in remedying the artifact seen in the anomaly score heat maps. "}, "hash": "e42f0fe9712a751d8c09817d1b234da8fb60dcfe9d38a8a70d4af989806ceabd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "743f348d-0ddc-47d5-ada4-6145d96e5be2", "node_type": "1", "metadata": {"window": "We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. ", "original_text": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. "}, "hash": "285aa5b861d660821e705bcbc9955dd80f7f96f5f03e2f7e2baf990c7a50c23b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. ", "mimetype": "text/plain", "start_char_idx": 957, "end_char_idx": 1128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "743f348d-0ddc-47d5-ada4-6145d96e5be2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. ", "original_text": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deace2ed-cdf9-4934-bba4-37023813b361", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We explain this problem in detail and demonstrate the mechanism by which it occurs visually.  We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6]. ", "original_text": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. "}, "hash": "d4422298fd005931f7cf2f0b0baca7b4d377fc5aaedf35382cde0ed1c19adf89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc0ee33-6e78-432b-8f84-c505a6672839", "node_type": "1", "metadata": {"window": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. ", "original_text": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n"}, "hash": "23b2e52ff7c1075b8c81efefdb84476d547ce01c299681eb41c7ca69f5700032", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. ", "mimetype": "text/plain", "start_char_idx": 1128, "end_char_idx": 1224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbc0ee33-6e78-432b-8f84-c505a6672839", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. ", "original_text": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "743f348d-0ddc-47d5-ada4-6145d96e5be2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We then propose two different approaches for improving the situation.  First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. ", "original_text": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. "}, "hash": "ac2e3b1cda3fe25fa715d9ced8ba714f4dbaa99a24969d057966b17d3c27364f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9828b90a-883b-4261-8a2f-20b569280e2d", "node_type": "1", "metadata": {"window": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. ", "original_text": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. "}, "hash": "b1e4a22dba850551035bf4ee6ab7cdbacf863ec2824fe8ab85d07a8c02101b7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n", "mimetype": "text/plain", "start_char_idx": 1224, "end_char_idx": 1357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9828b90a-883b-4261-8a2f-20b569280e2d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. ", "original_text": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc0ee33-6e78-432b-8f84-c505a6672839", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias.  Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. ", "original_text": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n"}, "hash": "02ce0e4b8dca9436318bf343f821b8058aa5c091bb6e9011d1f9ab715108cb94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f078cbb-72fd-4032-bafe-5f5236ae3be6", "node_type": "1", "metadata": {"window": "This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. ", "original_text": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. "}, "hash": "f433ad3976d14642ee18843fa7eeb7398630baf84fa64e3d1a6c8eae72d27101", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. ", "mimetype": "text/plain", "start_char_idx": 1357, "end_char_idx": 1676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f078cbb-72fd-4032-bafe-5f5236ae3be6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. ", "original_text": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9828b90a-883b-4261-8a2f-20b569280e2d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes.  This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. ", "original_text": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more. "}, "hash": "5986a42ad2e554a4073c545e8ecf8e3ed8fe04d1a21468931de88dd3767c1f63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c2ae36b-34ae-4414-8626-e334aff86f6c", "node_type": "1", "metadata": {"window": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. ", "original_text": "For a survey in research in anomaly detection see [3], [6]. "}, "hash": "87eefe17c30b4212c83f0d0b658d6c7a7d323a77fc581732e3bfc987fde19945", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. ", "mimetype": "text/plain", "start_char_idx": 1676, "end_char_idx": 1954, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c2ae36b-34ae-4414-8626-e334aff86f6c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. ", "original_text": "For a survey in research in anomaly detection see [3], [6]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f078cbb-72fd-4032-bafe-5f5236ae3be6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This approach results in remedying the artifact seen in the anomaly score heat maps.  We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. ", "original_text": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14]. "}, "hash": "3b889c5edc98f91c87129917807803abb162797202bd6fa3a84f3dee8a78b227", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfb6af1e-6fe4-405c-9cf3-aca604eda972", "node_type": "1", "metadata": {"window": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n", "original_text": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. "}, "hash": "5c8109970b087434a386f48840fc4a8d57d4491c2c11ad0d34b971bc4544835f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For a survey in research in anomaly detection see [3], [6]. ", "mimetype": "text/plain", "start_char_idx": 1954, "end_char_idx": 2014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfb6af1e-6fe4-405c-9cf3-aca604eda972", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n", "original_text": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c2ae36b-34ae-4414-8626-e334aff86f6c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets.  We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. ", "original_text": "For a survey in research in anomaly detection see [3], [6]. "}, "hash": "b514a22125ec9ad2874d9701fe0502a5a0fb4f6ae73939bae9d5563ca99ba7b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a6b42a4-273d-4233-bc32-bf82210e5054", "node_type": "1", "metadata": {"window": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. "}, "hash": "40487f892a5b4110a86e2b04d0845e9cd91f7676408112bf1052e2601946cbed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. ", "mimetype": "text/plain", "start_char_idx": 2014, "end_char_idx": 2124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a6b42a4-273d-4233-bc32-bf82210e5054", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfb6af1e-6fe4-405c-9cf3-aca604eda972", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets.  We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n", "original_text": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities. "}, "hash": "b93c337bccf70198f5f9b20f0f47ccb26e321a3dc8d22fb3cdb564530cc19437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a744292d-91fe-48e0-a7fa-b8f36955a7e9", "node_type": "1", "metadata": {"window": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n", "original_text": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. "}, "hash": "cd8a4d32327801cbf406c94632713b04117fb9d933f2e008a34279b8012658a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. ", "mimetype": "text/plain", "start_char_idx": 2124, "end_char_idx": 2327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a744292d-91fe-48e0-a7fa-b8f36955a7e9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n", "original_text": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a6b42a4-273d-4233-bc32-bf82210e5054", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.\n\n **Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16]. "}, "hash": "6d68fcb063e1839eb66d74ee1715fef285e2c83ab15f0d556276bc0f2df9362a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "234a67ec-c632-4dc7-81b1-e61e119ba9da", "node_type": "1", "metadata": {"window": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. "}, "hash": "4f141093ffccf373739a34ce59fa6c70c1bddbd6b2cbe99c5316676eda8f3c73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. ", "mimetype": "text/plain", "start_char_idx": 2327, "end_char_idx": 2463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "234a67ec-c632-4dc7-81b1-e61e119ba9da", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a744292d-91fe-48e0-a7fa-b8f36955a7e9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Index Terms**\u2014Anomaly detection, isolation forest\n\n## 1 INTRODUCTION\nIn the age of big data and high volume information, anomaly detection finds many areas of application, including network security, financial data, medical data analysis, and discovery of celestial events from astronomical surveys, among many more.  The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n", "original_text": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples. "}, "hash": "13cd18c450731b771781a60aae7042a8286b53dbbbaf6552cf5ca847a623a64e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eda3d28c-c9cc-47c9-9d70-a8a32c22f032", "node_type": "1", "metadata": {"window": "For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n", "original_text": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. "}, "hash": "71c9570ebc956d874ee4e5e20e72dc17534010c014250cbc321c03743581346f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. ", "mimetype": "text/plain", "start_char_idx": 2463, "end_char_idx": 2537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eda3d28c-c9cc-47c9-9d70-a8a32c22f032", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n", "original_text": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "234a67ec-c632-4dc7-81b1-e61e119ba9da", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The need for reliable and efficient algorithms is plentiful, and there are many techniques that have been developed over the years to address this need including multivariate data [15] and more recently, streaming data with need for updates on data with missing variables [14].  For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d. "}, "hash": "005a68105212cf6722f9c02a4a3f9bf756218f408cf95d9c381d3e79266d1af2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b266feb5-657a-4058-af4e-d73f17dca5d8", "node_type": "1", "metadata": {"window": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n"}, "hash": "7c579e90127dbdc0171464a155bd3aa66eac534316321c5b3525575b2ca5c06f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. ", "mimetype": "text/plain", "start_char_idx": 2537, "end_char_idx": 2712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b266feb5-657a-4058-af4e-d73f17dca5d8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eda3d28c-c9cc-47c9-9d70-a8a32c22f032", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For a survey in research in anomaly detection see [3], [6].  Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n", "original_text": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13]. "}, "hash": "37db432aa49e0863f7146fcdc2ba732e5427050988dc9fdfcc4653f130760ea5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99724556-4d26-43e6-9d38-124c655a98ea", "node_type": "1", "metadata": {"window": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n", "original_text": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "hash": "2ffefe356a14d87eb07e6eb1a762f4b7af18f710e3a8709b03011ac366193306", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n", "mimetype": "text/plain", "start_char_idx": 2712, "end_char_idx": 2873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99724556-4d26-43e6-9d38-124c655a98ea", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n", "original_text": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b266feb5-657a-4058-af4e-d73f17dca5d8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Among the different anomaly detection algorithms, Isolation Forest [9], [11] is one with unique capabilities.  It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "original_text": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n"}, "hash": "f38c1e41dd38fcbb1dea7359dfdd2b7daf1f36710e27ba6824fce375a1ccccc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43ac95a0-b8ad-4490-939c-bd3f90919d87", "node_type": "1", "metadata": {"window": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov. ", "original_text": "E-mail: sahandha@gmail.com.\n"}, "hash": "074932f899de980d1d158be3b74012e10cea13a696af94b25e77ed3f8c62fc6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "mimetype": "text/plain", "start_char_idx": 2873, "end_char_idx": 3018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "43ac95a0-b8ad-4490-939c-bd3f90919d87", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov. ", "original_text": "E-mail: sahandha@gmail.com.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99724556-4d26-43e6-9d38-124c655a98ea", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is a model free algorithm that is computationally efficient, can easily be adapted for use with parallel computing paradigms [7], and has been proven to be very effective in detecting anomalies [16].  The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n", "original_text": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "hash": "39ecb811f1242e8f05b4f259881dc74eb75e3cfbfed53d4e70e7fcffa954445c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7100e62e-fb1e-45bf-899b-fee6c8027ab8", "node_type": "1", "metadata": {"window": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct. ", "original_text": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "hash": "762c1673a8551e71810163eb8d1257ec0f189ff16236bd6038a994644c854e89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "E-mail: sahandha@gmail.com.\n", "mimetype": "text/plain", "start_char_idx": 3018, "end_char_idx": 3046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7100e62e-fb1e-45bf-899b-fee6c8027ab8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct. ", "original_text": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43ac95a0-b8ad-4490-939c-bd3f90919d87", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The main advantage of the algorithm is that it does not rely on building a profile for data in an effort to find nonconforming samples.  Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov. ", "original_text": "E-mail: sahandha@gmail.com.\n"}, "hash": "b032472b64307246b9e25daa342e85ff21bb030e90bb9ad50de86f43ba40f1ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7af794cf-4bc0-491e-a2f7-f650d300fd17", "node_type": "1", "metadata": {"window": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019. ", "original_text": "E-mail: mcarras2@illinois.edu.\n"}, "hash": "39a7bd2cd2ed1d950b5facce250c17c70646c2148f3011a34bbd30e4dc2e650d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "mimetype": "text/plain", "start_char_idx": 3046, "end_char_idx": 3197, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7af794cf-4bc0-491e-a2f7-f650d300fd17", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019. ", "original_text": "E-mail: mcarras2@illinois.edu.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7100e62e-fb1e-45bf-899b-fee6c8027ab8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Rather, it utilizes the fact that anomalous data are \u201cfew and different\u201d.  Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct. ", "original_text": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "hash": "73ac27a6392c3deba4b5191327c6feaedaa851f254c40e56f4478805c2711d19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fdd57a5-b606-4e2b-a9ba-27b157db1570", "node_type": "1", "metadata": {"window": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct. ", "original_text": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "hash": "814049febe6f5e07829d251873dd7d58786548bde3316dac00428e693ea93a3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "E-mail: mcarras2@illinois.edu.\n", "mimetype": "text/plain", "start_char_idx": 3197, "end_char_idx": 3228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3fdd57a5-b606-4e2b-a9ba-27b157db1570", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct. ", "original_text": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7af794cf-4bc0-491e-a2f7-f650d300fd17", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Most anomaly detection algorithms find anomalies by understanding the distribution of their properties and isolating them from the rest of normal data samples [4], [5], [13].  In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019. ", "original_text": "E-mail: mcarras2@illinois.edu.\n"}, "hash": "552880d494b95f06ceeb3cf7eb5dbe17f285ac91230b343b1e8a3f17c74fbfcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5509d74-4002-4d88-b040-fa22effd934b", "node_type": "1", "metadata": {"window": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar. ", "original_text": "E-mail: bigdog@illinois.edu.\n\n"}, "hash": "2f66f7ef78e6641ea157f4d22eb6c550eca5c49816c1dea2a8bdc7dec001229e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. ", "mimetype": "text/plain", "start_char_idx": 3228, "end_char_idx": 3356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5509d74-4002-4d88-b040-fa22effd934b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar. ", "original_text": "E-mail: bigdog@illinois.edu.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fdd57a5-b606-4e2b-a9ba-27b157db1570", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In an Isolation Forest, data is sub-sampled, and processed in a tree structure based on random cuts in the values of randomly selected features in the dataset.\n\n *   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct. ", "original_text": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA. "}, "hash": "b49fbdb377b98db67985f44582f2ee870ca7db02baef8bcb290c337e378fa8af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "594ed27d-3e65-4817-ac99-9d2f6fa4d4cf", "node_type": "1", "metadata": {"window": "E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021. ", "original_text": "*Manuscript received 3 Nov. "}, "hash": "f258ae87fb2cc93d70e2d89e8e1d3b5071d532ee470f869f60027c3985b32ab3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "E-mail: bigdog@illinois.edu.\n\n", "mimetype": "text/plain", "start_char_idx": 3356, "end_char_idx": 3386, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "594ed27d-3e65-4817-ac99-9d2f6fa4d4cf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021. ", "original_text": "*Manuscript received 3 Nov. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5509d74-4002-4d88-b040-fa22effd934b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*   S. Hariri is with the Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar. ", "original_text": "E-mail: bigdog@illinois.edu.\n\n"}, "hash": "5e3f813a907cc97306c64894bd23973c56d72d095d8a33712a249b5ec14489fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42f9a448-dda8-43c6-a6e4-37e4c91547ca", "node_type": "1", "metadata": {"window": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.) ", "original_text": "2018; revised 15 June 2019; accepted 2 Oct. "}, "hash": "bdacad2b6523f820cda18bebad8d1e05fb2a6a30c2d60ecc9d59f969af452a65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Manuscript received 3 Nov. ", "mimetype": "text/plain", "start_char_idx": 3386, "end_char_idx": 3414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42f9a448-dda8-43c6-a6e4-37e4c91547ca", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.) ", "original_text": "2018; revised 15 June 2019; accepted 2 Oct. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "594ed27d-3e65-4817-ac99-9d2f6fa4d4cf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "E-mail: sahandha@gmail.com.\n *   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021. ", "original_text": "*Manuscript received 3 Nov. "}, "hash": "786fc820e60211d7f16e8d0f068ffea45026005639421d6b2e7960d2bb6d2b85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d9c960c-e4ec-4ef1-82a8-cda4d93193f6", "node_type": "1", "metadata": {"window": "E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh. ", "original_text": "2019. "}, "hash": "5b6e9d6fe897c2703e400eeadd7b7dc1011aacc8e81b3cfa384874ca178d2b7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2018; revised 15 June 2019; accepted 2 Oct. ", "mimetype": "text/plain", "start_char_idx": 3414, "end_char_idx": 3458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d9c960c-e4ec-4ef1-82a8-cda4d93193f6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh. ", "original_text": "2019. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42f9a448-dda8-43c6-a6e4-37e4c91547ca", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*   M. Carrasco Kind is with the National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.) ", "original_text": "2018; revised 15 June 2019; accepted 2 Oct. "}, "hash": "b510b16077ba64cf11a2e8891b90a387e8b236c4475ff2feb8f8bd2d8ddd31c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46900cbc-8b22-4510-9129-75c3447d53f8", "node_type": "1", "metadata": {"window": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no. ", "original_text": "Date of publication 31 Oct. "}, "hash": "3cf7d3cc972ba8206753ef10ea033378103a0ac55fee7e2898f10bf6e1eb9e7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2019. ", "mimetype": "text/plain", "start_char_idx": 3458, "end_char_idx": 3464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46900cbc-8b22-4510-9129-75c3447d53f8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no. ", "original_text": "Date of publication 31 Oct. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d9c960c-e4ec-4ef1-82a8-cda4d93193f6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "E-mail: mcarras2@illinois.edu.\n *   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh. ", "original_text": "2019. "}, "hash": "ce6627da6e8e5fec4063291c60901cc8c97af5dd43106d5b42220f7907151c49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc4ba095-6635-425b-8fc8-b5af915ee930", "node_type": "1", "metadata": {"window": "E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. ", "original_text": "2019; date of current version 5 Mar. "}, "hash": "97b881ef756333bf8e6a9a96b431f43322c35fdc752051499dd39a27224f4f65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Date of publication 31 Oct. ", "mimetype": "text/plain", "start_char_idx": 3464, "end_char_idx": 3492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc4ba095-6635-425b-8fc8-b5af915ee930", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. ", "original_text": "2019; date of current version 5 Mar. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46900cbc-8b22-4510-9129-75c3447d53f8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*   R. J. Brunner is with the Gies College of Business, University of of Illinois at Urbana-Champaign, Champaign, IL 61820 USA.  E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no. ", "original_text": "Date of publication 31 Oct. "}, "hash": "ed035e0343b009543ab57036c083e816f4d004b706f2c48a80c96adfc1c2d87d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a53a83cb-562f-470e-bfcb-2fffa3978900", "node_type": "1", "metadata": {"window": "*Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. ", "original_text": "2021. "}, "hash": "f6478e578f1da90061561d390aa66955ee1c9a478d03c4e25863b98addcbfecd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2019; date of current version 5 Mar. ", "mimetype": "text/plain", "start_char_idx": 3492, "end_char_idx": 3529, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a53a83cb-562f-470e-bfcb-2fffa3978900", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. ", "original_text": "2021. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc4ba095-6635-425b-8fc8-b5af915ee930", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "E-mail: bigdog@illinois.edu.\n\n *Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. ", "original_text": "2019; date of current version 5 Mar. "}, "hash": "28e5095d4a7965611990991b853497e2498a93adfb78b6abbec75ebda1ba89f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85dd6913-92d9-4174-b3a0-5f46d6889957", "node_type": "1", "metadata": {"window": "2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. ", "original_text": "(Corresponding author: Sahand Hariri.) "}, "hash": "3e4516e691a0750a42b87bd5435ea44c751156b9e5b637dcd65915341dd4c33e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2021. ", "mimetype": "text/plain", "start_char_idx": 3529, "end_char_idx": 3535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85dd6913-92d9-4174-b3a0-5f46d6889957", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. ", "original_text": "(Corresponding author: Sahand Hariri.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a53a83cb-562f-470e-bfcb-2fffa3978900", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*Manuscript received 3 Nov.  2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. ", "original_text": "2021. "}, "hash": "772db9ee733eac73811dbb9d22c36bbd3e084d9ea1fde2651b887643699fefad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74a125c7-388b-453f-b241-66812d4cfe33", "node_type": "1", "metadata": {"window": "2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n", "original_text": "Recommended for acceptance by E. Keogh. "}, "hash": "15bc6b141ad2b7c97fb533fe7d4d99183dba5b0a1d691a0f7b3774d6e72a34ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(Corresponding author: Sahand Hariri.) ", "mimetype": "text/plain", "start_char_idx": 3535, "end_char_idx": 3574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74a125c7-388b-453f-b241-66812d4cfe33", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n", "original_text": "Recommended for acceptance by E. Keogh. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85dd6913-92d9-4174-b3a0-5f46d6889957", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2018; revised 15 June 2019; accepted 2 Oct.  2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. ", "original_text": "(Corresponding author: Sahand Hariri.) "}, "hash": "dbfce541bfa67991988f91391121c58094b302d2ee31d1bb7368a9e173455d38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57d31e30-f858-4235-b24d-2256869917f0", "node_type": "1", "metadata": {"window": "Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. ", "original_text": "Digital Object Identifier no. "}, "hash": "64cfb696345eb7bd91b8a1b311adda6e6bd944a91d531d3cc2f15320efcee09e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recommended for acceptance by E. Keogh. ", "mimetype": "text/plain", "start_char_idx": 3574, "end_char_idx": 3614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57d31e30-f858-4235-b24d-2256869917f0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. ", "original_text": "Digital Object Identifier no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74a125c7-388b-453f-b241-66812d4cfe33", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2019.  Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n", "original_text": "Recommended for acceptance by E. Keogh. "}, "hash": "45b1a6ede254d5f02f8d5835f16eae1a99f2f06184a114d6faa740cab0de0b42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15885cc8-f287-486a-91ab-a60e0cfdd21f", "node_type": "1", "metadata": {"window": "2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. ", "original_text": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. "}, "hash": "f8f87fbd9107329aa79bf50e9140f42b3c9d67399e399d60353fc1d3b49427b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Digital Object Identifier no. ", "mimetype": "text/plain", "start_char_idx": 3614, "end_char_idx": 3644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "15885cc8-f287-486a-91ab-a60e0cfdd21f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. ", "original_text": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57d31e30-f858-4235-b24d-2256869917f0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Date of publication 31 Oct.  2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. ", "original_text": "Digital Object Identifier no. "}, "hash": "b9236fd058b0e2f608eaf3429218953531d079730e19dedc7721c5b6498bee3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ab512e7-8e75-42da-83c3-27ffb793badd", "node_type": "1", "metadata": {"window": "2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. ", "original_text": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. "}, "hash": "07a9c7470db7ae81cd345fc8d196256a5fd930f5909a7148f0a4975c28a6ebbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. ", "mimetype": "text/plain", "start_char_idx": 3644, "end_char_idx": 3745, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ab512e7-8e75-42da-83c3-27ffb793badd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. ", "original_text": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15885cc8-f287-486a-91ab-a60e0cfdd21f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2019; date of current version 5 Mar.  2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. ", "original_text": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License. "}, "hash": "1805c0057792b0b9f163f1357ef02ffd5ece2ed60d9ea6160a2ebcceac158a87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c5c741e-17d4-4604-af89-23c3e1765b70", "node_type": "1", "metadata": {"window": "(Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n", "original_text": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. "}, "hash": "891f428528733c47db214ba8689a44f9f507a31b88fed97c8e9cd3107a8a9b17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. ", "mimetype": "text/plain", "start_char_idx": 3745, "end_char_idx": 3957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c5c741e-17d4-4604-af89-23c3e1765b70", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n", "original_text": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ab512e7-8e75-42da-83c3-27ffb793badd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2021.  (Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. ", "original_text": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly. "}, "hash": "e3b7aa27131754e28f14a59eff9bbb7dd7ad4e3cbca7a84824f3803f1fb541df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae5b4dd3-5f90-4d49-b0ab-facf4a3c5ed2", "node_type": "1", "metadata": {"window": "Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. ", "original_text": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n"}, "hash": "42d3c8e24db828c6d348e22fdbc26feec0c38a9313a44b4b4ee10b258e0b240e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. ", "mimetype": "text/plain", "start_char_idx": 3957, "end_char_idx": 4088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae5b4dd3-5f90-4d49-b0ab-facf4a3c5ed2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. ", "original_text": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c5c741e-17d4-4604-af89-23c3e1765b70", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(Corresponding author: Sahand Hariri.)  Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n", "original_text": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point. "}, "hash": "0535a31b3228de5104bd89425fb7f13801cf7bc95b34bb1f71b0d2f0b9aa3494", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9da4e376-079d-40a3-aa14-463aea4c7f83", "node_type": "1", "metadata": {"window": "Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. ", "original_text": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. "}, "hash": "ab6f2f10046ccb1e1f63ea4f864a4abc0d7edaa5a6548a682bf3d7900aa71978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n", "mimetype": "text/plain", "start_char_idx": 4088, "end_char_idx": 4242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9da4e376-079d-40a3-aa14-463aea4c7f83", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. ", "original_text": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae5b4dd3-5f90-4d49-b0ab-facf4a3c5ed2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Recommended for acceptance by E. Keogh.  Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. ", "original_text": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n"}, "hash": "cbdc54d4356736ad6d601d2a017d18eba5b589a1296ebac184d2fea8cfd8abfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdedbb6f-2e9d-46e8-8ea0-aef8d08eae5b", "node_type": "1", "metadata": {"window": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. ", "original_text": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. "}, "hash": "e497bb7868e4ab05e3b855f0351c49c3bbaaa23e6c84a5b7227f85f9e76a7dfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. ", "mimetype": "text/plain", "start_char_idx": 4242, "end_char_idx": 4404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdedbb6f-2e9d-46e8-8ea0-aef8d08eae5b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. ", "original_text": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9da4e376-079d-40a3-aa14-463aea4c7f83", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Digital Object Identifier no.  10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. ", "original_text": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place. "}, "hash": "aee370ab93c143a529f29c9d5c493a0a373aaefba794280d67131a4485ff1ef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80aaa0af-7eed-4eca-a9c7-185ea9b453e6", "node_type": "1", "metadata": {"window": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented. ", "original_text": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. "}, "hash": "32f3a4c3307cb7999c7c77065a7fb65f75d3fa775d7c831980103acfdc697b43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. ", "mimetype": "text/plain", "start_char_idx": 4404, "end_char_idx": 4537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80aaa0af-7eed-4eca-a9c7-185ea9b453e6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented. ", "original_text": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdedbb6f-2e9d-46e8-8ea0-aef8d08eae5b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10.1109/TKDE.2019.2947676*\n\n*This work is licensed under a Creative Commons Attribution 4.0 License.  For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. ", "original_text": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point. "}, "hash": "dd2a3fc1cd5abd1c3300526197d01b92cca3a7a8d000d6388ebcdf08d609809f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b417771d-0cc5-4bb6-813a-800264816df6", "node_type": "1", "metadata": {"window": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. ", "original_text": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n"}, "hash": "25d79ac2665bd43201f0d71fbcd7847abfb6943021d0de7e02d34b2336c5cb0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. ", "mimetype": "text/plain", "start_char_idx": 4537, "end_char_idx": 4667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b417771d-0cc5-4bb6-813a-800264816df6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. ", "original_text": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80aaa0af-7eed-4eca-a9c7-185ea9b453e6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For more information, see https://creativecommons.org/licenses/by/4.0/*\n\nThose samples that travel deeper into the tree branches are less likely to be anomalous, while shorter branches are indicative of anomaly.  As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented. ", "original_text": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming. "}, "hash": "ea576961207e2bc5f965a3d23223aa9a6269bfb98dd18469937fcc3e7c4ceb3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70fd4278-50bd-4025-85b8-9a0d3d14e675", "node_type": "1", "metadata": {"window": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. ", "original_text": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. "}, "hash": "aa94b4b649d451660cbf847d18a7130fa13bba7ffdb277a13eaf0107ee495683", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n", "mimetype": "text/plain", "start_char_idx": 4667, "end_char_idx": 4807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70fd4278-50bd-4025-85b8-9a0d3d14e675", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. ", "original_text": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b417771d-0cc5-4bb6-813a-800264816df6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As such, the aggregated lengths of the tree branches provide for a measure of anomaly or an \u201canomaly score\u201d for every given point.  However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. ", "original_text": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n"}, "hash": "03de7104971dfd183338937b559bafb7a587f04fe25717dcd1cf0023a266da4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "778987ef-eb57-4f9f-93ad-ae56907fcde5", "node_type": "1", "metadata": {"window": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. ", "original_text": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. "}, "hash": "11edb9fe62f7b46dea4d7a754b6c549fc6ce43a5443f04d1ea7716747bef64c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. ", "mimetype": "text/plain", "start_char_idx": 4807, "end_char_idx": 4984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "778987ef-eb57-4f9f-93ad-ae56907fcde5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. ", "original_text": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70fd4278-50bd-4025-85b8-9a0d3d14e675", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, before using this anomaly score for processing of data, there are issues with the standard Isolation Forest algorithm that need to be addressed.\n It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. ", "original_text": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees. "}, "hash": "43fec47d67c9ab8892f0742411a71d22d682c22af22c20bfadfe757a53055fc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c3b7dfc-941c-45b2-908b-43bcb4256bc8", "node_type": "1", "metadata": {"window": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. ", "original_text": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. "}, "hash": "2e1dc67d8c56264af8b4f2197b9ee12b14235758764f301fdc9f9ab422db11eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. ", "mimetype": "text/plain", "start_char_idx": 4984, "end_char_idx": 5122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c3b7dfc-941c-45b2-908b-43bcb4256bc8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. ", "original_text": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "778987ef-eb57-4f9f-93ad-ae56907fcde5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It turns out that while the algorithm is computationally efficient, it suffers from a bias that arises because of the way the branching of the trees takes place.  In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. ", "original_text": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10]. "}, "hash": "eb87c7c4bb04e5de2d2acd9970c67cb5b010a9691fe52b1f69feb9dcaa7d3439", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94ceddd5-a04e-41f1-a074-75c48cbfe870", "node_type": "1", "metadata": {"window": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n", "original_text": "No further discussion on developing this method, or why it might benefit the algorithm is presented. "}, "hash": "275acec93efbfbcbd69f5371766b00f90991cb770368470ca0bc8bf9a1e7f4b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. ", "mimetype": "text/plain", "start_char_idx": 5122, "end_char_idx": 5225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94ceddd5-a04e-41f1-a074-75c48cbfe870", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n", "original_text": "No further discussion on developing this method, or why it might benefit the algorithm is presented. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c3b7dfc-941c-45b2-908b-43bcb4256bc8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this paper we study this bias and discuss where it comes from and what effects it has on the anomaly score of a given data point.  We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. ", "original_text": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies. "}, "hash": "35a5cafc781a8c9acc57689a12fa0b4defe84ba61cdc08c715b4b7f09dee5dc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5138b471-06d3-490c-acc0-f564d7a1ef9b", "node_type": "1", "metadata": {"window": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. ", "original_text": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. "}, "hash": "6045da5794ca595edf7abf57459dcb300c9251cf086247db81f783abac5ed5d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "No further discussion on developing this method, or why it might benefit the algorithm is presented. ", "mimetype": "text/plain", "start_char_idx": 5225, "end_char_idx": 5326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5138b471-06d3-490c-acc0-f564d7a1ef9b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. ", "original_text": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94ceddd5-a04e-41f1-a074-75c48cbfe870", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We further introduce an extension to the Isolation Forest, named Extended Isolation Forest (EIF), that remedies this shortcoming.  While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n", "original_text": "No further discussion on developing this method, or why it might benefit the algorithm is presented. "}, "hash": "c2cee7edbd2da5bad1391a261084373694ab3a21abb6271d56d2dca01d7030ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aec43b26-6d5c-42a9-a801-a0797979cb71", "node_type": "1", "metadata": {"window": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. ", "original_text": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. "}, "hash": "56fbed95ef7d9d15c1b88097dcbb39efa29bb13d1dcecb97109efbfdee602542", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. ", "mimetype": "text/plain", "start_char_idx": 5326, "end_char_idx": 5516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aec43b26-6d5c-42a9-a801-a0797979cb71", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. ", "original_text": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5138b471-06d3-490c-acc0-f564d7a1ef9b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "While the basic idea is similar to the Isolation Forest, the details of implementation are modified to accommodate a more general approach.\n As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. ", "original_text": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted. "}, "hash": "3267036d2b66135a04c6de517c78c2508ef9e0c3fede80dcdf420b29be42c86d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8eef6173-aff6-47a1-a02b-44d65b6a8a7c", "node_type": "1", "metadata": {"window": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. ", "original_text": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. "}, "hash": "506d775652572f3fad5d34cfc09123764fc22c1cb110899841e4d27e87490b3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 5516, "end_char_idx": 5660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8eef6173-aff6-47a1-a02b-44d65b6a8a7c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. ", "original_text": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aec43b26-6d5c-42a9-a801-a0797979cb71", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we will see later, the proposed extension relies on the use of hyperplanes with random slopes (non-axis-parallel) for splitting the data in creating the binary search trees.  The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. ", "original_text": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest. "}, "hash": "a764e528f7ffe617a0f46e1a5ff61620ce25005a2843d1e979104bad3c574af0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ab3af49-5603-4edb-8a6b-8baa6996bb48", "node_type": "1", "metadata": {"window": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. ", "original_text": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. "}, "hash": "4c82a25e78650bc5d0a860f773d40d05d5c4017624542b44654bcee87661363f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. ", "mimetype": "text/plain", "start_char_idx": 5660, "end_char_idx": 5859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ab3af49-5603-4edb-8a6b-8baa6996bb48", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. ", "original_text": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8eef6173-aff6-47a1-a02b-44d65b6a8a7c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The idea of using hyperplanes with random slopes in the context of Isolation Forest with random choice of the slope is mentioned in [10].  However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. ", "original_text": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points. "}, "hash": "158bce20831784053b62dfd3ba6cff5d86e360444dc6d32471234a1a70d89558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2499990c-5ca4-4640-ab1f-316a77f9e293", "node_type": "1", "metadata": {"window": "No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n", "original_text": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n"}, "hash": "1fb153f811a6c93aace5351853d46184cacb7f041ddd21fb063261a36bd302ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. ", "mimetype": "text/plain", "start_char_idx": 5859, "end_char_idx": 6166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2499990c-5ca4-4640-ab1f-316a77f9e293", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n", "original_text": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ab3af49-5603-4edb-8a6b-8baa6996bb48", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, the main focus there is to find optimal slicing planes in oder to detect clustered anomalies.  No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. ", "original_text": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing. "}, "hash": "e32b1d24b7a0d1a880b8837d21572c8bb8aa86fccfc48d31f6fe2dd41909d154", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c5b9a5c-6031-4227-9f62-8c76bbd41c45", "node_type": "1", "metadata": {"window": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections. ", "original_text": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. "}, "hash": "e43af0adc6955489885479a042ea7da721903fd7cc0da16205c7a553d1c2e92a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n", "mimetype": "text/plain", "start_char_idx": 6166, "end_char_idx": 6384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c5b9a5c-6031-4227-9f62-8c76bbd41c45", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections. ", "original_text": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2499990c-5ca4-4640-ab1f-316a77f9e293", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "No further discussion on developing this method, or why it might benefit the algorithm is presented.  The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n", "original_text": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n"}, "hash": "726a7fb47192228723a8aed0a632912a7438a793e3af0762f543769b6bd38215", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72db9ffd-f747-4953-b320-7ea0034e21c3", "node_type": "1", "metadata": {"window": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study. ", "original_text": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. "}, "hash": "8249e05fc4d67a14eb71a8b5e526dfbfd5281ecfd430c96e4fa1bff5cf35349e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. ", "mimetype": "text/plain", "start_char_idx": 6384, "end_char_idx": 6574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72db9ffd-f747-4953-b320-7ea0034e21c3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study. ", "original_text": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c5b9a5c-6031-4227-9f62-8c76bbd41c45", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The methods for choosing random slopes for hyperplanes, the effects of these choices on the algorithm, and the consequences of this, especially in higher dimensional data, are also omitted.  Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections. ", "original_text": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature. "}, "hash": "c38847bb2ec0f7033555f9ccd12db09b55d3439a1c51068c9c8d7b2a80644ee7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ada7d8d2-b597-42af-abbd-191f70939f43", "node_type": "1", "metadata": {"window": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. ", "original_text": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. "}, "hash": "8e4c0b8ca2646bb5f68d24ff0dc2cd3cf845a456bad5bbd4ae310e94ebd02756", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. ", "mimetype": "text/plain", "start_char_idx": 6574, "end_char_idx": 6749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ada7d8d2-b597-42af-abbd-191f70939f43", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. ", "original_text": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72db9ffd-f747-4953-b320-7ea0034e21c3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Using oblique hyperplanes in tree based methods has also been studied in random forests [12] but again, not in the context of Isolation Forest.  An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study. ", "original_text": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps. "}, "hash": "613707fe6f5c0a1f38011856fdb37c1d813b1e92b50fc8bbeb5d4e70d91f0dc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b759a24-63e4-4380-9723-dcc3323ff860", "node_type": "1", "metadata": {"window": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. ", "original_text": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. "}, "hash": "4991008dc83cda3bc1f7ae404f1eef91ad8461053459771bc4618ec62185a82a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. ", "mimetype": "text/plain", "start_char_idx": 6749, "end_char_idx": 6851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b759a24-63e4-4380-9723-dcc3323ff860", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. ", "original_text": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ada7d8d2-b597-42af-abbd-191f70939f43", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "An approach to anomaly detection which utilizes the concept of isolation is presented in [1], [2] where a nearest neighbor algorithm instead of a tree based method is used for isolating data points.  While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. ", "original_text": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear. "}, "hash": "f36d1e71afeaabf165e67714ffefcac4f2bdd7c893162f2e7fc470585099ef77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1c4b85d-dc68-4e35-9604-9dd755722322", "node_type": "1", "metadata": {"window": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods. ", "original_text": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n"}, "hash": "ddb98eaa0f4b8488d822a13218b3d388044044838e2b74de705151204557828f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. ", "mimetype": "text/plain", "start_char_idx": 6851, "end_char_idx": 7027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1c4b85d-dc68-4e35-9604-9dd755722322", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods. ", "original_text": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b759a24-63e4-4380-9723-dcc3323ff860", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "While this algorithm performs better than the Isolation Forest by various metrics, and while the problem of the isolation forest discussed here is remedied because of the fundamentally different nature of achieving isolation, a discussion of what the causes of the problem with Isolation Forest is missing.  Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. ", "original_text": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not. "}, "hash": "8d3b9a09b8230ea309fa50ab54ef68df7d9534584ebeea8d880a0277ed3b6a2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b29ad9d2-efe8-496f-a285-6d6cc5b651ce", "node_type": "1", "metadata": {"window": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms. ", "original_text": "This paper is divided into five sections. "}, "hash": "e5bde8dbbb24455bd9e12868cc5ce475095921c70e878f92c310545e6720e19f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n", "mimetype": "text/plain", "start_char_idx": 7027, "end_char_idx": 7135, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b29ad9d2-efe8-496f-a285-6d6cc5b651ce", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms. ", "original_text": "This paper is divided into five sections. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1c4b85d-dc68-4e35-9604-9dd755722322", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Here we show one can achieve better performance by adjusting the algorithm and utilizing the tree based nature of the algorithm without having to resort to fundamentally different algorithms such as nearest neighbor.\n\n As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods. ", "original_text": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n"}, "hash": "601504e0d6d4b9e4eb7854a9591482c8463690dd3c9d5b331028f045731f38b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94a01303-3dde-4568-b82d-19bb429a95e4", "node_type": "1", "metadata": {"window": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5. ", "original_text": "Section 2 provides some examples that motivate this study. "}, "hash": "36e4116d6777277fd9a881820365c4b57f4f12a524148fc9abb1507c31330d21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This paper is divided into five sections. ", "mimetype": "text/plain", "start_char_idx": 7135, "end_char_idx": 7177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94a01303-3dde-4568-b82d-19bb429a95e4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5. ", "original_text": "Section 2 provides some examples that motivate this study. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b29ad9d2-efe8-496f-a285-6d6cc5b651ce", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As such, and perhaps most importantly, to the best of our knowledge, there is no discussion of the shortcomings of standard Isolation Forest as an anomaly detection algorithm in literature.  Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms. ", "original_text": "This paper is divided into five sections. "}, "hash": "282fbc5911ff251c6a3ba639e2ea2ee3ce300f4ec713898ef9c6c4ed34361a25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95f2dcb9-8cc0-43a6-8653-8e884064e1d6", "node_type": "1", "metadata": {"window": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n", "original_text": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. "}, "hash": "68a2c9ba1554256b8f5f6606188a87f8a86b95512b4601c38f5f769e1c1e64da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 2 provides some examples that motivate this study. ", "mimetype": "text/plain", "start_char_idx": 7177, "end_char_idx": 7236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95f2dcb9-8cc0-43a6-8653-8e884064e1d6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n", "original_text": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94a01303-3dde-4568-b82d-19bb429a95e4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Here we present an insightful review and show the need for an improvement on the algorithm based on heat maps for anomaly scores, henceforth simply referred to as score maps.  We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5. ", "original_text": "Section 2 provides some examples that motivate this study. "}, "hash": "285a8762b6200b1e64807d7b022687ba48b25db0b0dff8e46f44183277e33d9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d741f48c-2e78-467e-bd00-4feced768e3f", "node_type": "1", "metadata": {"window": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. ", "original_text": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. "}, "hash": "486f997644f4a923015a6ab635f86932d296b01bab099d16d9e7e7620fa26e1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. ", "mimetype": "text/plain", "start_char_idx": 7236, "end_char_idx": 7351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d741f48c-2e78-467e-bd00-4feced768e3f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. ", "original_text": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95f2dcb9-8cc0-43a6-8653-8e884064e1d6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We discuss the sources of the problem in great detail, and what consequences these shortcomings bear.  The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n", "original_text": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets. "}, "hash": "4bd40600420124c00200c84132ebdafe03ac69d0735639372d585c12b5642f50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f924a188-052e-4f2f-a621-93a457add755", "node_type": "1", "metadata": {"window": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n", "original_text": "Additionally, we compare rates of convergence between the two methods. "}, "hash": "6e18a0c7badc77ca928e7b78607afde48518692842f2bc6803dafbc298fdc4a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. ", "mimetype": "text/plain", "start_char_idx": 7351, "end_char_idx": 7506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f924a188-052e-4f2f-a621-93a457add755", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n", "original_text": "Additionally, we compare rates of convergence between the two methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d741f48c-2e78-467e-bd00-4feced768e3f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The importance of improving the algorithm comes to shine when we need to identify anomalies in regions where it is not immediately obvious whether a point is anomalous or not.  Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. ", "original_text": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots. "}, "hash": "2b26611adb6b554609b2337b5bca40b8a646ad343dff93d35b487ec5d2f163f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfd490ac-e448-4f4f-aae8-5b1a703b39ef", "node_type": "1", "metadata": {"window": "This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. ", "original_text": "We also present a comparison of AUROC and AUPRC for the two algorithms. "}, "hash": "266810419916dcd15bf6a0a04109a6dd4148c4a8c88bd8f4c7fc90a611edc06a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, we compare rates of convergence between the two methods. ", "mimetype": "text/plain", "start_char_idx": 7506, "end_char_idx": 7577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfd490ac-e448-4f4f-aae8-5b1a703b39ef", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. ", "original_text": "We also present a comparison of AUROC and AUPRC for the two algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f924a188-052e-4f2f-a621-93a457add755", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Robust score maps are needed in assigning probabilities for given data points in terms of being anomalies.\n\n This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n", "original_text": "Additionally, we compare rates of convergence between the two methods. "}, "hash": "a5b77945ef9148ec132d12b924decd8068355bbb2025aa8771967dba727b8b34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5696875-b916-4fed-b110-e17cbae9642b", "node_type": "1", "metadata": {"window": "Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig. ", "original_text": "Lastly, we close with some concluding remarks in Section 5. "}, "hash": "4c76ae1b3c4e695a8a282e3815ebe8e5d8328a1285e8de9bde4580e09770e635", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also present a comparison of AUROC and AUPRC for the two algorithms. ", "mimetype": "text/plain", "start_char_idx": 7577, "end_char_idx": 7649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5696875-b916-4fed-b110-e17cbae9642b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig. ", "original_text": "Lastly, we close with some concluding remarks in Section 5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfd490ac-e448-4f4f-aae8-5b1a703b39ef", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This paper is divided into five sections.  Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. ", "original_text": "We also present a comparison of AUROC and AUPRC for the two algorithms. "}, "hash": "16e731df3a8bb474b3734c817225b25feb44f8d70a3f2dc3fc65389d9e3a831c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "feb9edac-8930-481b-b058-f79f0c372043", "node_type": "1", "metadata": {"window": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a. ", "original_text": "Discussion is provided throughout the various sections in the paper.\n\n"}, "hash": "0d2f4e1dd17dd9129a4f7ff3ad3b86c260a5484f69f08626a2767ab61cb47b5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lastly, we close with some concluding remarks in Section 5. ", "mimetype": "text/plain", "start_char_idx": 7649, "end_char_idx": 7709, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "feb9edac-8930-481b-b058-f79f0c372043", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a. ", "original_text": "Discussion is provided throughout the various sections in the paper.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5696875-b916-4fed-b110-e17cbae9642b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Section 2 provides some examples that motivate this study.  In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig. ", "original_text": "Lastly, we close with some concluding remarks in Section 5. "}, "hash": "c7f031ce220cdf4d5f957c6f28f2ed9e771fcf0f237e93312c8db0fbecce7c65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc8ad0b-ec7c-494d-8247-033ea4ab9f96", "node_type": "1", "metadata": {"window": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. ", "original_text": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. "}, "hash": "75a9b510d29e3cc2ce10fc7953c2e00aa37b5829df57724e37e100277debac77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discussion is provided throughout the various sections in the paper.\n\n", "mimetype": "text/plain", "start_char_idx": 7709, "end_char_idx": 7779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbc8ad0b-ec7c-494d-8247-033ea4ab9f96", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. ", "original_text": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "feb9edac-8930-481b-b058-f79f0c372043", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Section 3 we lay out the extension to the algorithm and present its application to higher dimensional datasets.  In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a. ", "original_text": "Discussion is provided throughout the various sections in the paper.\n\n"}, "hash": "8c70ac601a37656231a3489a3a40d12275cf3fe8343be646fa9a72ecfb23754c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7de6b11-33ce-4685-96cb-ff71de075093", "node_type": "1", "metadata": {"window": "Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). ", "original_text": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n"}, "hash": "b28daac301f85e1a0e6c3614844a09647e2fcb72633edbacd2882c9a846db1f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. ", "mimetype": "text/plain", "start_char_idx": 7779, "end_char_idx": 8039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7de6b11-33ce-4685-96cb-ff71de075093", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). ", "original_text": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc8ad0b-ec7c-494d-8247-033ea4ab9f96", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Section 4 we compare the results obtained by the Extended Isolation Forest to those of standard Isolation Forest, using score maps, and variance plots.  Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. ", "original_text": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points. "}, "hash": "ba185f0f752d3a3de99784c44a5d0bc0c5562de003117948a7ae4b7c55a6d298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82bf43cd-fbe8-4332-af61-85c912382baa", "node_type": "1", "metadata": {"window": "We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase. ", "original_text": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. "}, "hash": "21231acf8a4086794b4ebd171938e4e2f4de744dc521ea2716cace437c78a00e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n", "mimetype": "text/plain", "start_char_idx": 8039, "end_char_idx": 8173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82bf43cd-fbe8-4332-af61-85c912382baa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase. ", "original_text": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7de6b11-33ce-4685-96cb-ff71de075093", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Additionally, we compare rates of convergence between the two methods.  We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). ", "original_text": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n"}, "hash": "6f210ab4673d73c278f3263d1b3a1bcd097232725fbe6f0440b3cdbc9ca4c1b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccb45413-3a7b-4a07-93cc-c847c0d77033", "node_type": "1", "metadata": {"window": "Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n", "original_text": "Consider the dataset shown in Fig. "}, "hash": "2c84e3c32974de3fb6c354bf3f50821bfabc91293ee18859dd024623506c9c54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. ", "mimetype": "text/plain", "start_char_idx": 8173, "end_char_idx": 8340, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ccb45413-3a7b-4a07-93cc-c847c0d77033", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n", "original_text": "Consider the dataset shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82bf43cd-fbe8-4332-af61-85c912382baa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also present a comparison of AUROC and AUPRC for the two algorithms.  Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase. ", "original_text": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly. "}, "hash": "1c3c2670224b0b4c3124adc566548a9a2574e62021d0ab0c0f9d1e623c365823", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a347229-2931-4d29-96ad-085e797b1225", "node_type": "1", "metadata": {"window": "Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig. ", "original_text": "1a. "}, "hash": "ac27a87523a629d6df6d2ae7900bd3496a46f38aa8c6eace3fc5ecc796e750e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Consider the dataset shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 8340, "end_char_idx": 8375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a347229-2931-4d29-96ad-085e797b1225", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig. ", "original_text": "1a. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccb45413-3a7b-4a07-93cc-c847c0d77033", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Lastly, we close with some concluding remarks in Section 5.  Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n", "original_text": "Consider the dataset shown in Fig. "}, "hash": "905b60f8a40b23d8b768d8f7fa01ea9cdc1b0e6afd45832f4ba6774c72a353bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82e6dd0c-ea41-4687-9d7e-560a7082f385", "node_type": "1", "metadata": {"window": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest. ", "original_text": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. "}, "hash": "926221090db3bf1dc4a87da8454831a1b4a2473710c72f877bc5a3ffbaab25f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1a. ", "mimetype": "text/plain", "start_char_idx": 8375, "end_char_idx": 8379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82e6dd0c-ea41-4687-9d7e-560a7082f385", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest. ", "original_text": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a347229-2931-4d29-96ad-085e797b1225", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discussion is provided throughout the various sections in the paper.\n\n ## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig. ", "original_text": "1a. "}, "hash": "3475887640a53ed8e1eacc52d6c25c11afe1a0a39ed8718976d63fffbba86361", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d961e99-0ed8-4f5b-978a-2b4c852c5743", "node_type": "1", "metadata": {"window": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. ", "original_text": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). "}, "hash": "1a442a8e5c86f0fc1d1bb479aa05880c3e64f88d1cb49b4c9e885b65bfc48625", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. ", "mimetype": "text/plain", "start_char_idx": 8379, "end_char_idx": 8526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d961e99-0ed8-4f5b-978a-2b4c852c5743", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. ", "original_text": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e6dd0c-ea41-4687-9d7e-560a7082f385", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 2 MOTIVATION\nGiven a dataset, we would like to be able to train our Isolation Forest so that it can assign an anomaly score for each of the data points, rank them, and help draw conclusions about the distribution of anomalies as well as the nominal points.  A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest. ", "original_text": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix. "}, "hash": "25a10ba172c3e9e06e58d2510e7bb31feba91c316b6d8bee105a6ad3eb8dc27a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97663869-ff1c-4540-8c6a-3862669b2087", "node_type": "1", "metadata": {"window": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. ", "original_text": "However, as the data points move away from the origin, their anomaly scores should increase. "}, "hash": "c26248aaf154c45a229603ca6892c1206cf96c3ddcd7fd45dc5b7bc4a3943a09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). ", "mimetype": "text/plain", "start_char_idx": 8526, "end_char_idx": 8637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97663869-ff1c-4540-8c6a-3862669b2087", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. ", "original_text": "However, as the data points move away from the origin, their anomaly scores should increase. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d961e99-0ed8-4f5b-978a-2b4c852c5743", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A closer look at scores produced by Isolation Forest however, reveals that these anomaly scores are inconsistent and need some work.\n\n The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. ", "original_text": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0). "}, "hash": "bb2f606adc5ad27e0e36995a28a4acb365edfde677cebd075c5fc2a9c694a2f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e18b7ba3-f91b-4bb1-b447-e9623a86b975", "node_type": "1", "metadata": {"window": "Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. ", "original_text": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n"}, "hash": "0533fc99c7df16c9eda846370879a1195c885776521c20d5db86f890034582b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, as the data points move away from the origin, their anomaly scores should increase. ", "mimetype": "text/plain", "start_char_idx": 8637, "end_char_idx": 8730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e18b7ba3-f91b-4bb1-b447-e9623a86b975", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. ", "original_text": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97663869-ff1c-4540-8c6a-3862669b2087", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The best way to see the problem is to examine very simple datasets where we have an intuition of how the scores should be distributed and what constitutes an anomaly.  Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. ", "original_text": "However, as the data points move away from the origin, their anomaly scores should increase. "}, "hash": "c991d9c0e2c83da792e74cba96bedc6df6996994e868a974f4ebe7e074ea997a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a123c27c-764b-498d-afef-cfe60a65b3d3", "node_type": "1", "metadata": {"window": "1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. ", "original_text": "Fig. "}, "hash": "b40d8e45a3dc2192021524522ad86367320a2b83fdbe31ba5331d663454dc751", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n", "mimetype": "text/plain", "start_char_idx": 8730, "end_char_idx": 8877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a123c27c-764b-498d-afef-cfe60a65b3d3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e18b7ba3-f91b-4bb1-b447-e9623a86b975", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Consider the dataset shown in Fig.  1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. ", "original_text": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n"}, "hash": "16fe8a4e35f0c158d77e83ed3cd3000f669418ffd26c171f35cc8f850dbe3d66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "104cfe51-fb01-429d-a473-86dd6bedd802", "node_type": "1", "metadata": {"window": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. ", "original_text": "1b shows the anomaly score map obtained using the standard Isolation Forest. "}, "hash": "4e05ba3e0fd4df6c4c5782d90b84447f154f3cb4b8fde5f48464614823740db2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 8877, "end_char_idx": 8882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "104cfe51-fb01-429d-a473-86dd6bedd802", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. ", "original_text": "1b shows the anomaly score map obtained using the standard Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a123c27c-764b-498d-afef-cfe60a65b3d3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1a.  This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. ", "original_text": "Fig. "}, "hash": "5e077e812618a86cda821c530a6bb9c3d6a1da9489e2fddf3ac6688816624a61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b180f6e-9061-4c21-a2c9-7bcb8ccf4442", "node_type": "1", "metadata": {"window": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm. ", "original_text": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. "}, "hash": "56b3098423d476275c659e28c7e88c42fc59267a3c4269abf28c33d826c4a869", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1b shows the anomaly score map obtained using the standard Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 8882, "end_char_idx": 8959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b180f6e-9061-4c21-a2c9-7bcb8ccf4442", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm. ", "original_text": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "104cfe51-fb01-429d-a473-86dd6bedd802", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is a two dimensional random dataset sampled from a 2-D normal distribution with zero mean vector and covariance given by the identity matrix.  It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. ", "original_text": "1b shows the anomaly score map obtained using the standard Isolation Forest. "}, "hash": "0390cd951635727ebadba68329561134ece8fd21cca8776b4d34d4100216a485", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65bf92c8-2d19-42d2-8d9a-7a6c7d05fbf8", "node_type": "1", "metadata": {"window": "However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. ", "original_text": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. "}, "hash": "60a2f9c37dfc9cbbf8efaa88e1bede2fe8fe8f2642d737df57691e61d1c1ace5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. ", "mimetype": "text/plain", "start_char_idx": 8959, "end_char_idx": 9092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65bf92c8-2d19-42d2-8d9a-7a6c7d05fbf8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. ", "original_text": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b180f6e-9061-4c21-a2c9-7bcb8ccf4442", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is immediately obvious that a data point should be considered nominal if it falls somewhere close to (0,0).  However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm. ", "original_text": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest. "}, "hash": "47070a2a3d46bb4e73dc66357e0c73e2ef552af7322f2d7d3241429e6e5c6373", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10c9dd39-2182-43cd-a959-1def36c6e202", "node_type": "1", "metadata": {"window": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. ", "original_text": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. "}, "hash": "e176bbedf560e9e13346aa0a4a06403b9e65f96321f320f587ad683842fbc7bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. ", "mimetype": "text/plain", "start_char_idx": 9092, "end_char_idx": 9231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10c9dd39-2182-43cd-a959-1def36c6e202", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. ", "original_text": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65bf92c8-2d19-42d2-8d9a-7a6c7d05fbf8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, as the data points move away from the origin, their anomaly scores should increase.  Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. ", "original_text": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase. "}, "hash": "42117439675f5f310f2b6911291fe600d02e4f2a8e5cfadab877c94fa20afed5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1f64d1d-7152-4d97-8259-5ed9094d64cf", "node_type": "1", "metadata": {"window": "Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig. ", "original_text": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. "}, "hash": "7728cb169d16297c8e0488de7c6512d8b60904ff0e6e26d39c7bf5fbc34d09c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. ", "mimetype": "text/plain", "start_char_idx": 9231, "end_char_idx": 9414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1f64d1d-7152-4d97-8259-5ed9094d64cf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig. ", "original_text": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10c9dd39-2182-43cd-a959-1def36c6e202", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Therefore we expect to see an anomaly score map with an almost circular and symmetric pattern with increasing values as we move radially outward.\n\n Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. ", "original_text": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center. "}, "hash": "d5b319e6167ea2748c5551bac7c057e49f4cd4f4db28c16d0e4eb100c95ad897", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05200d6f-c323-4da7-a506-f88642f91a7d", "node_type": "1", "metadata": {"window": "1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n", "original_text": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. "}, "hash": "85963e6bd7261892de86ebacbc94ab301b1887bccc6b4e30b5f403ebb30f9738", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. ", "mimetype": "text/plain", "start_char_idx": 9414, "end_char_idx": 9614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05200d6f-c323-4da7-a506-f88642f91a7d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n", "original_text": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1f64d1d-7152-4d97-8259-5ed9094d64cf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig. ", "original_text": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin. "}, "hash": "3e09c58b201678f242118c80e808f2f2a0f0e094504d22bd8cbad768793807c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca784510-6a7f-4224-842b-e84fe22db77a", "node_type": "1", "metadata": {"window": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig. ", "original_text": "It is critical to fix this issue for a precise and robust anomaly detection algorithm. "}, "hash": "7fbf184ad9c5c6093562128c058f8303b77ce448bbbdb1553f09f1d0b4af5226", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. ", "mimetype": "text/plain", "start_char_idx": 9614, "end_char_idx": 9763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca784510-6a7f-4224-842b-e84fe22db77a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig. ", "original_text": "It is critical to fix this issue for a precise and robust anomaly detection algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05200d6f-c323-4da7-a506-f88642f91a7d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1b shows the anomaly score map obtained using the standard Isolation Forest.  We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n", "original_text": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm. "}, "hash": "9e369a8ebf655880b3dd7f5b97fbca134b0940762b801cc0b6b10a67bfc2170f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3675a3a5-8170-49a0-880b-017c4a534cad", "node_type": "1", "metadata": {"window": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2. ", "original_text": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. "}, "hash": "2d67957051c4379dbf5faa474a2671c28a87ac4cf3673bda444c9742200d9787", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is critical to fix this issue for a precise and robust anomaly detection algorithm. ", "mimetype": "text/plain", "start_char_idx": 9763, "end_char_idx": 9850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3675a3a5-8170-49a0-880b-017c4a534cad", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2. ", "original_text": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca784510-6a7f-4224-842b-e84fe22db77a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We sample points uniformly within the range of the plot and assign scores to those points based on the trees created for the forest.  We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig. ", "original_text": "It is critical to fix this issue for a precise and robust anomaly detection algorithm. "}, "hash": "452f37c6139438c55cb334f2e52da86c1522300cd64b63ca72a4b62a613a95b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9093a52-4184-4f77-9698-0d64e67c052f", "node_type": "1", "metadata": {"window": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). ", "original_text": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. "}, "hash": "bc4b7104dc540d32238a651d62f066dae4b1f6a7390aa25c19c565b6be2eab22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. ", "mimetype": "text/plain", "start_char_idx": 9850, "end_char_idx": 10105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9093a52-4184-4f77-9698-0d64e67c052f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). ", "original_text": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3675a3a5-8170-49a0-880b-017c4a534cad", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can clearly see that the points in the center get the lowest anomaly score, and as we move radially outward, the score values increase.  However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2. ", "original_text": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm. "}, "hash": "6709d2cb9e4f3fdec568d24f9214e833cb2ffcba7d53a218d0469d635801a107", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c059c82c-0c92-4347-a193-49e45cce8126", "node_type": "1", "metadata": {"window": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n", "original_text": "In this case, the results shown in Fig. "}, "hash": "2e201d7c3f6d2efb1c28e43b984f7fa8b90df42eb09c665e7fced310251e84a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 10105, "end_char_idx": 10263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c059c82c-0c92-4347-a193-49e45cce8126", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n", "original_text": "In this case, the results shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9093a52-4184-4f77-9698-0d64e67c052f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, we also observe rectangular regions of lower anomaly score in the x and y directions, compared to other points that fall roughly at the same radial distance from the center.  Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). ", "original_text": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores. "}, "hash": "b7dc2a0075b7f82978f03db4475f3dda6d5274111aad2dd450598bb0b1693adb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dccbb74f-6324-48be-bcab-3661bfd7ab32", "node_type": "1", "metadata": {"window": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. ", "original_text": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n"}, "hash": "9100f7d9f0192ce07975fd3642f2c8b910a05476f41d641a30076d80ee478d49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, the results shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 10263, "end_char_idx": 10303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dccbb74f-6324-48be-bcab-3661bfd7ab32", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. ", "original_text": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c059c82c-0c92-4347-a193-49e45cce8126", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Based on our understanding of how the data is distributed, the score map should maintain an approximately circular shape for all radii, i.e., similar score values for fixed distances from the origin.  As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n", "original_text": "In this case, the results shown in Fig. "}, "hash": "2ab9965b9e604d876939c77f205d3d0eb9f86a42f0f12f39dcf56cb7a9e8ac4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66086e32-5de3-4c14-ac9b-76da7da821cf", "node_type": "1", "metadata": {"window": "It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig. ", "original_text": "A second example is shown in Fig. "}, "hash": "8da5a11b2b320e7c47ee1a2269c98907748fabe042aa7689fedeff84096424ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n", "mimetype": "text/plain", "start_char_idx": 10303, "end_char_idx": 10399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "66086e32-5de3-4c14-ac9b-76da7da821cf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig. ", "original_text": "A second example is shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dccbb74f-6324-48be-bcab-3661bfd7ab32", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we will see in later sections, the difference in the anomaly score in the x and y directions are indeed an artifacts introduced by the algorithm.  It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. ", "original_text": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n"}, "hash": "2eb3dcad31109c4aafb670f18056fc83113fcaa086e8f0240f620157b2a263e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06f1413a-5e10-4b41-9fda-c06fe72116b2", "node_type": "1", "metadata": {"window": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers. ", "original_text": "2. "}, "hash": "002690fd584c073a47c62e76d2f775addaa2ad6b39a3fa74713bf10ea1375e60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A second example is shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 10399, "end_char_idx": 10433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06f1413a-5e10-4b41-9fda-c06fe72116b2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers. ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66086e32-5de3-4c14-ac9b-76da7da821cf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is critical to fix this issue for a precise and robust anomaly detection algorithm.  One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig. ", "original_text": "A second example is shown in Fig. "}, "hash": "63cff9b2c6426ce2d9ac361ce2ec8b0b3d80290af34cc8c04982a1edc5f5195d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed81ed00-9f3e-4cf2-8e67-205d522ffec1", "node_type": "1", "metadata": {"window": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. ", "original_text": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). "}, "hash": "aa45c3004bb5d8a2941961e7d35217a15ea71edc918b1c218fe30415f9f75b14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 10433, "end_char_idx": 10436, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed81ed00-9f3e-4cf2-8e67-205d522ffec1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. ", "original_text": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06f1413a-5e10-4b41-9fda-c06fe72116b2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "One example of potential problems caused by this artifact is that depending on the threshold of score to label a data point an anomaly, two data points of similar importance can get categorized differently, which reduces the reliability of the algorithm.  Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers. ", "original_text": "2. "}, "hash": "24746167d9e7980175490bfc3f1ad868d152ebdc717bc1be102bcb3444103ef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "145157ff-da45-484e-b32c-527325f4cee0", "node_type": "1", "metadata": {"window": "In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem. ", "original_text": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n"}, "hash": "064230132760e2e49e585d450af1ff35a7bb51365037b1f0fed73eff96158d5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). ", "mimetype": "text/plain", "start_char_idx": 10436, "end_char_idx": 10539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "145157ff-da45-484e-b32c-527325f4cee0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem. ", "original_text": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed81ed00-9f3e-4cf2-8e67-205d522ffec1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Another example might be the case where the user wants to obtain probability density functions for distribution of data points based on their anomaly scores.  In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. ", "original_text": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0). "}, "hash": "46476e06e05de0d6121891349bb2a30c7d8a93294546144e9f2902fd41d53b54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97f3127b-b0fa-4696-ae5c-954e73c82b14", "node_type": "1", "metadata": {"window": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. ", "original_text": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. "}, "hash": "cea46b54ee4ca0538242eec506e1852fbd649eb7ac8807c27b7d06c7795bae5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n", "mimetype": "text/plain", "start_char_idx": 10539, "end_char_idx": 10632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97f3127b-b0fa-4696-ae5c-954e73c82b14", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. ", "original_text": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "145157ff-da45-484e-b32c-527325f4cee0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case, the results shown in Fig.  1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem. ", "original_text": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n"}, "hash": "346a2de2ece4a4074a94068dd88eb642d3bbb929a0a272a62098b85fbbbfefc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47dcedb9-312f-4a82-a0eb-1f75d60c4a3a", "node_type": "1", "metadata": {"window": "A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. ", "original_text": "Not surprisingly we still observe the same artifacts as in Fig. "}, "hash": "19d1273175be909843da255e43bead2f2b2c2307d9b8faee458eed3828b9c712", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. ", "mimetype": "text/plain", "start_char_idx": 10632, "end_char_idx": 10847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47dcedb9-312f-4a82-a0eb-1f75d60c4a3a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. ", "original_text": "Not surprisingly we still observe the same artifacts as in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97f3127b-b0fa-4696-ae5c-954e73c82b14", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1 can certainly not be relied upon for accurate representation of anomaly score distributions.\n\n A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. ", "original_text": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away. "}, "hash": "856c9416579a44e702bfd09acf8bcbc8c021bc9a58381a8bd7cd006f4af19beb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e2fdc6e-0165-478a-8721-9d56118ab5aa", "node_type": "1", "metadata": {"window": "2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n", "original_text": "1 shown as rectangular bands aligned with the cluster centers. "}, "hash": "2a3f5a4bc6a613a229324bc1d53589b9c8cb205786776fedbe12f3be90dd5b16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Not surprisingly we still observe the same artifacts as in Fig. ", "mimetype": "text/plain", "start_char_idx": 10847, "end_char_idx": 10911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e2fdc6e-0165-478a-8721-9d56118ab5aa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n", "original_text": "1 shown as rectangular bands aligned with the cluster centers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47dcedb9-312f-4a82-a0eb-1f75d60c4a3a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A second example is shown in Fig.  2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. ", "original_text": "Not surprisingly we still observe the same artifacts as in Fig. "}, "hash": "3a2699b55300ba82dc00d4a3497712d5ae7da917ef4decf9afdce18328909dd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7515c6b-66d3-4822-b8d2-3b05e7d0e906", "node_type": "1", "metadata": {"window": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig. ", "original_text": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. "}, "hash": "2667dead82f839517014eba0c70e5d1bc80b78d1eda3fb29e639a9ba1cab233b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 shown as rectangular bands aligned with the cluster centers. ", "mimetype": "text/plain", "start_char_idx": 10911, "end_char_idx": 10974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7515c6b-66d3-4822-b8d2-3b05e7d0e906", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig. ", "original_text": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e2fdc6e-0165-478a-8721-9d56118ab5aa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2.  Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n", "original_text": "1 shown as rectangular bands aligned with the cluster centers. "}, "hash": "4f8c65d9c879d6cb716116b5ef94668bfd3aaf5427ee38ffc076dd985213993f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80338afd-6ddf-4e98-af43-4b4814060da2", "node_type": "1", "metadata": {"window": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1. ", "original_text": "This introduces a real problem. "}, "hash": "b50d2f0127e59078fa0870fe30b31c23d99b6712354b9dd53e243d8d597ada16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. ", "mimetype": "text/plain", "start_char_idx": 10974, "end_char_idx": 11124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80338afd-6ddf-4e98-af43-4b4814060da2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1. ", "original_text": "This introduces a real problem. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7515c6b-66d3-4822-b8d2-3b05e7d0e906", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Here we have two separate clusters of normally distributed data concentrated around (0,10) and (10,0).  We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig. ", "original_text": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters. "}, "hash": "ce5091f991fc3c602fa3ec71c8217ceae8ba92ba43c002e8331bd5292eb8bed9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c40ae38-ca12-4d09-be76-fee69dae762d", "node_type": "1", "metadata": {"window": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. ", "original_text": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. "}, "hash": "d5dd9a8b65f16d4d31e19ee98fc25d90b92b9db8383eb96152c346bfebc942c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This introduces a real problem. ", "mimetype": "text/plain", "start_char_idx": 11124, "end_char_idx": 11156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c40ae38-ca12-4d09-be76-fee69dae762d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. ", "original_text": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80338afd-6ddf-4e98-af43-4b4814060da2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We expect very low anomaly scores at the centers of the blobs, and higher scores elsewhere.\n\n The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1. ", "original_text": "This introduces a real problem. "}, "hash": "61f2cfbf59f42e001f9f55cccfe852fb805df03e32bf1cddda3fd50b34ab9e84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28c05430-2d4f-4e9c-95d6-c47143e2b46a", "node_type": "1", "metadata": {"window": "Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores.", "original_text": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. "}, "hash": "08fb835b82f3087726bb87acd80cb0a8fc89a8e9a9cf6d9a6eb9916a7963b51b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. ", "mimetype": "text/plain", "start_char_idx": 11156, "end_char_idx": 11313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28c05430-2d4f-4e9c-95d6-c47143e2b46a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores.", "original_text": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c40ae38-ca12-4d09-be76-fee69dae762d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The anomaly score map clearly shows the two cluster locations and the scores distributed around them, where we can see a concentration of lower scores near the center of the clusters and higher values further away.  Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. ", "original_text": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point. "}, "hash": "26216d663f03dd866b8752a68a40e58e31e2970d8566455a4189928aedaa14ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1722ecb2-61a5-4a13-b495-b6cf426516cd", "node_type": "1", "metadata": {"window": "1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n", "original_text": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n"}, "hash": "8dcc7586d4128a89da660672a8271dad7568bd1c12bb534f5adbda171655f389", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. ", "mimetype": "text/plain", "start_char_idx": 11313, "end_char_idx": 11477, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1722ecb2-61a5-4a13-b495-b6cf426516cd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n", "original_text": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28c05430-2d4f-4e9c-95d6-c47143e2b46a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Not surprisingly we still observe the same artifacts as in Fig.  1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores.", "original_text": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced. "}, "hash": "6ece9c637b10eeb34b07601186ee8649d9f62829db7bb1758a1e6b8b7fb39f5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cea8be2e-a47b-4b78-abb7-192debf509fd", "node_type": "1", "metadata": {"window": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space. ", "original_text": "***\n**Fig. "}, "hash": "6b57897fe80e008b6cdab5854eb67a0ff10841a26b2431c34b4b02c17a587a69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n", "mimetype": "text/plain", "start_char_idx": 11477, "end_char_idx": 11602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cea8be2e-a47b-4b78-abb7-192debf509fd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space. ", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1722ecb2-61a5-4a13-b495-b6cf426516cd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1 shown as rectangular bands aligned with the cluster centers.  Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n", "original_text": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n"}, "hash": "9c1f76066bb99a42f64669b86656a13c8e0e6b67bc592c45dd2b3422736058bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffe9ec8c-e0be-4ff0-a08e-d26c7c931160", "node_type": "1", "metadata": {"window": "This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). ", "original_text": "1. "}, "hash": "f0bb9e50d317d5bce84299c4cf5e5d8a78bc15bfcf1191d0a340a3e2602fbfe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 11602, "end_char_idx": 11613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ffe9ec8c-e0be-4ff0-a08e-d26c7c931160", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). ", "original_text": "1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cea8be2e-a47b-4b78-abb7-192debf509fd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Moreover, we also see that this artifact is amplified at the intersection of the bands, close to (0,0) and (10,10) where we observe \u201cghost\u201d clusters.  This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space. ", "original_text": "***\n**Fig. "}, "hash": "8fb9ae6b8cef1b3568d3ba089372052955fe8f1edda2b3831ce454a07a7f7af4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f055942-0f73-4c6b-bfd7-5c5a6c525a52", "node_type": "1", "metadata": {"window": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n", "original_text": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. "}, "hash": "aca16904f0fc34bb05437aafa022f9e7743c282a11fffd0b6c01a1cb9fb13ac5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. ", "mimetype": "text/plain", "start_char_idx": 11613, "end_char_idx": 11616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0f055942-0f73-4c6b-bfd7-5c5a6c525a52", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n", "original_text": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffe9ec8c-e0be-4ff0-a08e-d26c7c931160", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This introduces a real problem.  In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). ", "original_text": "1. "}, "hash": "bee5bdc87c4c76c9df76888038e546dac5b98fecdfcdda4f9e951514d154cb10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db1d476d-0bfb-4644-85d1-acd981ab18b5", "node_type": "1", "metadata": {"window": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig. ", "original_text": "Darker areas indicate higher anomaly scores."}, "hash": "1cc0825f72e93e66eb63c20a8a7bfa3de1fdd6a545a43e033e1ace32338d8976", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. ", "mimetype": "text/plain", "start_char_idx": 11616, "end_char_idx": 11764, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db1d476d-0bfb-4644-85d1-acd981ab18b5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig. ", "original_text": "Darker areas indicate higher anomaly scores."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f055942-0f73-4c6b-bfd7-5c5a6c525a52", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case, if we observed an anomalous data point that happened to lie near the origin, the algorithm would most likely categorize it as a nominal point.  Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n", "original_text": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix. "}, "hash": "025d1f49dc79fc211531e45d926839424c03f6a5e9c7d9c56735984577e490e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "948237c2-826b-4520-9e94-dd580ae7c979", "node_type": "1", "metadata": {"window": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2. ", "original_text": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n"}, "hash": "93413b5302342aa809dab8670c7c766915eead2242eb2467e9f6ea305df9a450", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Darker areas indicate higher anomaly scores.", "mimetype": "text/plain", "start_char_idx": 11764, "end_char_idx": 11808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "948237c2-826b-4520-9e94-dd580ae7c979", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2. ", "original_text": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db1d476d-0bfb-4644-85d1-acd981ab18b5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Given the nature of these artifacts it is clear that these results cannot be fully trusted for more complex data point distributions where this effect is enhanced.  Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig. ", "original_text": "Darker areas indicate higher anomaly scores."}, "hash": "895dbe26ce8e953916ce746e6cfc7ffb26c88573a5eb38f43f821feb114ef35b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8973d337-5a6b-4dfb-961b-e3d643f5d5b1", "node_type": "1", "metadata": {"window": "***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points.", "original_text": "(b) Anomaly Score Map: A heatmap of the same data space. "}, "hash": "97d44fd7b3a7b70c54522513a1b5602ba94fa973ae463d28442c5dc77d3a63f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n", "mimetype": "text/plain", "start_char_idx": 11808, "end_char_idx": 11928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8973d337-5a6b-4dfb-961b-e3d643f5d5b1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points.", "original_text": "(b) Anomaly Score Map: A heatmap of the same data space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "948237c2-826b-4520-9e94-dd580ae7c979", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Not only does this increase the chances of false positives, it also wrongly indicates a non-existent structure in the data.\n\n ***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2. ", "original_text": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n"}, "hash": "0c92d3ac3bf78577a186c890210b4980ac02c441b396a5dca8795d037fccb978", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88594a4f-b37b-4f95-a9c8-8b15c0d1cefe", "node_type": "1", "metadata": {"window": "1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n", "original_text": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). "}, "hash": "0e26adbf24a58a70337fe823ef93b98d309edfe0e059fbf1c1c33b6a40606c88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Anomaly Score Map: A heatmap of the same data space. ", "mimetype": "text/plain", "start_char_idx": 11928, "end_char_idx": 11985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88594a4f-b37b-4f95-a9c8-8b15c0d1cefe", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n", "original_text": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8973d337-5a6b-4dfb-961b-e3d643f5d5b1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points.", "original_text": "(b) Anomaly Score Map: A heatmap of the same data space. "}, "hash": "0ed6b1a892bd471f3a5b249d5217be20a649e0daaac3056185237355a5bd16f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a2a9854-5a2c-4ae6-b9c8-c948c663cd3c", "node_type": "1", "metadata": {"window": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. ", "original_text": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n"}, "hash": "15e02a4767e40c76e008b4dad2a1702fea8b97c23fb144e43ed5ba1dcdb34283", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). ", "mimetype": "text/plain", "start_char_idx": 11985, "end_char_idx": 12093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a2a9854-5a2c-4ae6-b9c8-c948c663cd3c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. ", "original_text": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88594a4f-b37b-4f95-a9c8-8b15c0d1cefe", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1.  Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n", "original_text": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores). "}, "hash": "423e4965ef43234b1bbbba34c53c5b5f1a01384a5715a2be7894095b9952823a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82e14664-287e-4527-bcef-c0b3efcefc43", "node_type": "1", "metadata": {"window": "Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. ", "original_text": "**Fig. "}, "hash": "def4c357f0926f3d25877c39c47568ecd1839d247b4dd527a905856051ebfe44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n", "mimetype": "text/plain", "start_char_idx": 12093, "end_char_idx": 12285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82e14664-287e-4527-bcef-c0b3efcefc43", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a2a9854-5a2c-4ae6-b9c8-c948c663cd3c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data and anomaly score map produced by Isolation Forest for two dimensional normally distributed points with zero mean and unity covariance matrix.  Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. ", "original_text": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n"}, "hash": "f2826b60b840335946d45f68c3af6bf1b554d74a91400e07308060ec8c9695f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ce19532-3ae2-43d3-8b6c-d2dc9654b1d9", "node_type": "1", "metadata": {"window": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n", "original_text": "2. "}, "hash": "bc096c5414d2b465b7cc3b4896e69284c1277645792c4f668b8ccb5c74f5e72d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 12285, "end_char_idx": 12292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ce19532-3ae2-43d3-8b6c-d2dc9654b1d9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e14664-287e-4527-bcef-c0b3efcefc43", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Darker areas indicate higher anomaly scores. **\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. ", "original_text": "**Fig. "}, "hash": "78e16f4226b893f5104c96372d7789718d7b753cb5f3296f03109651d9305fe4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d172b29-84aa-438d-850b-b4b2710794db", "node_type": "1", "metadata": {"window": "(b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. ", "original_text": "Data points and anomaly score maps of two clusters of normally distributed points."}, "hash": "e3cc79566c7bbaa37ebf093cd856de3a5c61408812b3beac7508de3b47bc286a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 12292, "end_char_idx": 12295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d172b29-84aa-438d-850b-b4b2710794db", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. ", "original_text": "Data points and anomaly score maps of two clusters of normally distributed points."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ce19532-3ae2-43d3-8b6c-d2dc9654b1d9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Normally Distributed Data: A scatter plot of points clustered around the origin (0,0), forming a circular cloud.\n (b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n", "original_text": "2. "}, "hash": "97d7cc803d702cc84047ba85c45a97b60aaf60cbb28165d3fee9a75686bcb729", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd65339f-623e-480a-b400-851fcd438b7a", "node_type": "1", "metadata": {"window": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n", "original_text": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n"}, "hash": "454faeddda17a8e48a7f57dc188ea72751ae79961133aae29a6be2edb068ed61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data points and anomaly score maps of two clusters of normally distributed points.", "mimetype": "text/plain", "start_char_idx": 12295, "end_char_idx": 12377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd65339f-623e-480a-b400-851fcd438b7a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n", "original_text": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d172b29-84aa-438d-850b-b4b2710794db", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Anomaly Score Map: A heatmap of the same data space.  The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. ", "original_text": "Data points and anomaly score maps of two clusters of normally distributed points."}, "hash": "9158ad92f32c98a2000c0f145a0eaeb30b849508d763d9ff64acf95643dc4b62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65dda5bc-b956-4aa9-a4a6-99703dffd816", "node_type": "1", "metadata": {"window": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. ", "original_text": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. "}, "hash": "23d8f1b6699c3cbcd38f0b1e60fda864d4e591ab228721d3b43f220d4f4bcb22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n", "mimetype": "text/plain", "start_char_idx": 12377, "end_char_idx": 12542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65dda5bc-b956-4aa9-a4a6-99703dffd816", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. ", "original_text": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd65339f-623e-480a-b400-851fcd438b7a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The center is bright yellow (low score), surrounded by concentric layers of orange and red (higher scores).  However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n", "original_text": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n"}, "hash": "390d726a99bd66e2509d1df8ef31c74524f0f1bc1596121f6b8152577246dcf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2153655-77b9-467d-8b68-1cfb69a55ba7", "node_type": "1", "metadata": {"window": "**Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case. ", "original_text": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. "}, "hash": "386df923ca8db4f6484810be65acc35369c8be746144fc0df64b7a428f87b431", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. ", "mimetype": "text/plain", "start_char_idx": 12542, "end_char_idx": 12652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2153655-77b9-467d-8b68-1cfb69a55ba7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case. ", "original_text": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65dda5bc-b956-4aa9-a4a6-99703dffd816", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, the shape is not perfectly circular; it is a rounded square with darker red bands extending vertically and horizontally from the center, creating an artifact that resembles a cross.\n\n **Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. ", "original_text": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters. "}, "hash": "36bfef87ae8f9873df6192956b686a6df0b2fbe97fb6a5542f2e33b13de9ceea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46eae430-e1f8-4203-83c1-7f555f0e9e9f", "node_type": "1", "metadata": {"window": "2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. ", "original_text": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n"}, "hash": "864799849472aa8285de9c9d2f9458a8024e135c28f119b297cf36bc07630a4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. ", "mimetype": "text/plain", "start_char_idx": 12652, "end_char_idx": 12761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46eae430-e1f8-4203-83c1-7f555f0e9e9f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. ", "original_text": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2153655-77b9-467d-8b68-1cfb69a55ba7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case. ", "original_text": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally. "}, "hash": "8a93938cd7ba38ab9496e8b6832c8163053dc7df7d7d51a86e7c31e89692ab17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57b7646b-729d-4dc0-8bb9-ace12180e93f", "node_type": "1", "metadata": {"window": "Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. ", "original_text": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. "}, "hash": "85bf09edd1314ec0d9b2fd40e4775125dce9b9c1d02179ab1e8056dc656a9b9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n", "mimetype": "text/plain", "start_char_idx": 12761, "end_char_idx": 12942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57b7646b-729d-4dc0-8bb9-ace12180e93f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. ", "original_text": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46eae430-e1f8-4203-83c1-7f555f0e9e9f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2.  Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. ", "original_text": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n"}, "hash": "5671261c7e42ac47fbc6ba65f093f91caf586c29e579cba11135d044abe53408", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "989b5071-e8e5-415d-b563-1331b1190e36", "node_type": "1", "metadata": {"window": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n", "original_text": "3.\n\n"}, "hash": "00dcdbe58834658b8923b9bee3f386751d209f235419c73694ae6d9034261144", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 12942, "end_char_idx": 13029, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "989b5071-e8e5-415d-b563-1331b1190e36", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n", "original_text": "3.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57b7646b-729d-4dc0-8bb9-ace12180e93f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data points and anomaly score maps of two clusters of normally distributed points. **\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. ", "original_text": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig. "}, "hash": "2a6599d8d59d0d7c09833fcd98217b12da8f86910d84a34ee0492e93095ab002", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bff4f6dc-b8f9-456a-881f-48069af6e029", "node_type": "1", "metadata": {"window": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n", "original_text": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. "}, "hash": "34e2055fc57a87aeb38e6d11f0f1c01a2edb4420ac37233f6b1d830a7a068b83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.\n\n", "mimetype": "text/plain", "start_char_idx": 13029, "end_char_idx": 13033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bff4f6dc-b8f9-456a-881f-48069af6e029", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n", "original_text": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "989b5071-e8e5-415d-b563-1331b1190e36", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Two normally distributed clusters: A scatter plot showing two distinct circular clusters of points, one centered around (0, 10) and the other around (10, 0).\n (b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n", "original_text": "3.\n\n"}, "hash": "93d3d0e07d104fc06390537be18dc76ab510b53fdcb19b1d32ce24ce6811b408", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fad9cff-0275-40aa-82a8-908ff55afb34", "node_type": "1", "metadata": {"window": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. ", "original_text": "It is very important for the algorithm to detect this structure representing a more complex case. "}, "hash": "f950fb89aba70e7d67d5398cdecd8bffce2e374de4c927e24302e5a60f29d1f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. ", "mimetype": "text/plain", "start_char_idx": 13033, "end_char_idx": 13137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0fad9cff-0275-40aa-82a8-908ff55afb34", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. ", "original_text": "It is very important for the algorithm to detect this structure representing a more complex case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff4f6dc-b8f9-456a-881f-48069af6e029", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Anomaly Score Map: A heatmap showing two bright yellow regions (low score) corresponding to the clusters.  From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n", "original_text": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top. "}, "hash": "815039abd8040c3d37c19bbfb3da75e6fafb1c5dcd49657a8a0bd46ad29b419e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d308319-494f-447c-b4de-0fd07763c2a3", "node_type": "1", "metadata": {"window": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. ", "original_text": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. "}, "hash": "f7ddf306607756680b747857582ca5040ceda2e7ec957f75c7bc0846c995cc29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is very important for the algorithm to detect this structure representing a more complex case. ", "mimetype": "text/plain", "start_char_idx": 13137, "end_char_idx": 13235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d308319-494f-447c-b4de-0fd07763c2a3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. ", "original_text": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fad9cff-0275-40aa-82a8-908ff55afb34", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "From these bright spots, rectangular bands of darker red (higher scores) extend vertically and horizontally.  A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. ", "original_text": "It is very important for the algorithm to detect this structure representing a more complex case. "}, "hash": "54601c295edecc832caa330e506629689a059f7de4753737e50fb80dbb75292c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01bbdf27-f0e1-4915-8793-cd5cd59effd3", "node_type": "1", "metadata": {"window": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). ", "original_text": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. "}, "hash": "66f30d27ef1c7ae9614dccd9a415117a4e8c11519d0b915c5c9d2a0ede014322", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. ", "mimetype": "text/plain", "start_char_idx": 13235, "end_char_idx": 13373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01bbdf27-f0e1-4915-8793-cd5cd59effd3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). ", "original_text": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d308319-494f-447c-b4de-0fd07763c2a3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A particularly dark red spot (highest artifactual score) appears at the intersection of these bands, near (0,0) and another at (10,10), creating the appearance of \"ghost\" clusters.\n ***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. ", "original_text": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly. "}, "hash": "9ee25b62c4f2959e38a68bd7dc349aa9ec495701c5d0e27891fef18cdee5aa7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63c8e436-d6bf-478b-a70b-58276f71e6c6", "node_type": "1", "metadata": {"window": "3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension. ", "original_text": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n"}, "hash": "af57b5e53e058387971d452ac36eed18bb79538022117c8c67965fe214880d29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. ", "mimetype": "text/plain", "start_char_idx": 13373, "end_char_idx": 13529, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63c8e436-d6bf-478b-a70b-58276f71e6c6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension. ", "original_text": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01bbdf27-f0e1-4915-8793-cd5cd59effd3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nAs a third example we will look at a dataset with more structure as shown in Fig.  3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). ", "original_text": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before. "}, "hash": "d4b1be2c1ce575223136f789f3e60914b98a8628219bdfb269e5fdb57ea5c885", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48ce4271-4475-4c80-bc89-25c343101aac", "node_type": "1", "metadata": {"window": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. ", "original_text": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n"}, "hash": "12d192d92fff6170c91fd782eb70e0c505bcce8667d9a78564ae826f9fce7baf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n", "mimetype": "text/plain", "start_char_idx": 13529, "end_char_idx": 13650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48ce4271-4475-4c80-bc89-25c343101aac", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. ", "original_text": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63c8e436-d6bf-478b-a70b-58276f71e6c6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3.\n\n In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension. ", "original_text": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n"}, "hash": "575c9759cc15e03618074174c427accaa31d41507bfecf53919537b28e53d8b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0b3662e-eb00-4883-b015-c037cff5f723", "node_type": "1", "metadata": {"window": "It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two. ", "original_text": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. "}, "hash": "551923ae1d5eb7375d95b088c73d81f381d6bba0691f078ecbe4deb416a64774", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n", "mimetype": "text/plain", "start_char_idx": 13650, "end_char_idx": 13797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0b3662e-eb00-4883-b015-c037cff5f723", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two. ", "original_text": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48ce4271-4475-4c80-bc89-25c343101aac", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the data has an inherent structure, the sinusoidal shape with Gaussian noise added on top.  It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. ", "original_text": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n"}, "hash": "bce00669416840a2d754c2e5d8412439eca29f6ef993ee72a980a586d85d58f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e72c957d-a415-4811-97b4-f1655f695b8b", "node_type": "1", "metadata": {"window": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. ", "original_text": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. "}, "hash": "f5a9c6a441f2d6678042d5fb826433c8168447384f5628875925450746dfc9ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. ", "mimetype": "text/plain", "start_char_idx": 13797, "end_char_idx": 13994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e72c957d-a415-4811-97b4-f1655f695b8b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. ", "original_text": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0b3662e-eb00-4883-b015-c037cff5f723", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is very important for the algorithm to detect this structure representing a more complex case.  However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two. ", "original_text": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees. "}, "hash": "a4529b6a8fdfa06e024d2aa60732c2122becca2e6303ff38abf129ec7cc8ae2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "656b2379-f90e-42a1-8a21-b30b1e87c7f7", "node_type": "1", "metadata": {"window": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree. ", "original_text": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). "}, "hash": "e7d84eace31cc7a0192b16865fa8f5228211424db44794e092a54ca3b0c9c912", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. ", "mimetype": "text/plain", "start_char_idx": 13994, "end_char_idx": 14104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "656b2379-f90e-42a1-8a21-b30b1e87c7f7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree. ", "original_text": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e72c957d-a415-4811-97b4-f1655f695b8b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, looking at the anomaly score map generated by the standard Isolation Forest, we can see that the algorithm performs very poorly.  Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. ", "original_text": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree. "}, "hash": "f3ea825b7de4292f347f79ec6433a1b7c940b55541d45f88f804139173ad08c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23a9e6e8-5a25-440e-bef2-af76e1b720df", "node_type": "1", "metadata": {"window": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete. ", "original_text": "It then selects a random value $v$ within the minimum and maximum values in that dimension. "}, "hash": "a37b5df5d5cab0662c59a264840a4f871d0b939f2c568c28ea6f05d3ab09e5e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). ", "mimetype": "text/plain", "start_char_idx": 14104, "end_char_idx": 14244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23a9e6e8-5a25-440e-bef2-af76e1b720df", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete. ", "original_text": "It then selects a random value $v$ within the minimum and maximum values in that dimension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "656b2379-f90e-42a1-8a21-b30b1e87c7f7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Essentially this data is treated as one large rectangular blob with horizontal and vertical bands emanating parallel to the coordinate axes as seen before.  An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree. ", "original_text": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable). "}, "hash": "a3aa76879757d6385045621a47b544ff95007799d10bf9f47908d8f57a17d79a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4c6f38c-afc6-4e19-a936-ddb4287a912a", "node_type": "1", "metadata": {"window": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. ", "original_text": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. "}, "hash": "4ecbcf7561545230a4abb8a1d57e7422165efbaa8284d1001f22c814b21931f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It then selects a random value $v$ within the minimum and maximum values in that dimension. ", "mimetype": "text/plain", "start_char_idx": 14244, "end_char_idx": 14336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4c6f38c-afc6-4e19-a936-ddb4287a912a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. ", "original_text": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23a9e6e8-5a25-440e-bef2-af76e1b720df", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "An anomalous data point that is in between two \u201chills\u201d will get a very low score and be categorized as a nominal point.\n\n In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete. ", "original_text": "It then selects a random value $v$ within the minimum and maximum values in that dimension. "}, "hash": "1e56ee153593d7325f79f70125aaca6f8525dc6d36815f4c21de934b3a10f2ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e07b51e1-87ae-414d-9f08-d27f47a771af", "node_type": "1", "metadata": {"window": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n", "original_text": "In this manner the data on the current node of the tree is split in two. "}, "hash": "5f73973eec3db2ea1ac4f41840b621bf5ae294b348dcf14cf92688994dab69bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. ", "mimetype": "text/plain", "start_char_idx": 14336, "end_char_idx": 14500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e07b51e1-87ae-414d-9f08-d27f47a771af", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n", "original_text": "In this manner the data on the current node of the tree is split in two. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4c6f38c-afc6-4e19-a936-ddb4287a912a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this work, we will discuss why these undesirable features arise, and how we can fix them using an extension of the Isolation Forest algorithm.\n\n ## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. ", "original_text": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch. "}, "hash": "e901f45d3a9afcc37e2097440b5bfefa1b986252afb0e48494dcba65e5839cb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccf52917-be34-4548-8385-778fff08d666", "node_type": "1", "metadata": {"window": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig. ", "original_text": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. "}, "hash": "d84260671fad0cc67cd8a0f4d95dae97159d0c5c6230333551a464127e18e712", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this manner the data on the current node of the tree is split in two. ", "mimetype": "text/plain", "start_char_idx": 14500, "end_char_idx": 14573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ccf52917-be34-4548-8385-778fff08d666", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig. ", "original_text": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e07b51e1-87ae-414d-9f08-d27f47a771af", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 3 GENERALIZATION OF ISOLATION FOREST\n### 3.1 Overview\nThe general algorithm for Isolation Forest [9], [11] starts with the training of the data, which in this case is construction of the trees.  Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n", "original_text": "In this manner the data on the current node of the tree is split in two. "}, "hash": "cf95291b0c7f6f117acfbb4c8f10f26c3c682af44a13054e4b5ef136601aebad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9d36cec-13ce-46cb-aa17-6904632889df", "node_type": "1", "metadata": {"window": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training. ", "original_text": "The process begins again with a new random sub-sample to build another randomized tree. "}, "hash": "5e58b975075c6033d5b83ac43ab7344db146785aa002f843acbd5b61d2d57ce8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. ", "mimetype": "text/plain", "start_char_idx": 14573, "end_char_idx": 14718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9d36cec-13ce-46cb-aa17-6904632889df", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training. ", "original_text": "The process begins again with a new random sub-sample to build another randomized tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccf52917-be34-4548-8385-778fff08d666", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Given a dataset of dimension N, the algorithm chooses a random sub-sample of data to construct a binary tree.  The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig. ", "original_text": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached. "}, "hash": "3f5160bd75c1b04ef5a152db4a44eb08bfed1ee884bb8a93c2c0460534a0faba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3d0c000-8525-4228-bc56-239e161e5423", "node_type": "1", "metadata": {"window": "It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. ", "original_text": "After building a large ensemble of trees, i.e., a forest, the training is complete. "}, "hash": "5d8d2f3049a6db9564c3c90c38bb717a942e7d0fef14e3813ac62769ac28422e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The process begins again with a new random sub-sample to build another randomized tree. ", "mimetype": "text/plain", "start_char_idx": 14718, "end_char_idx": 14806, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3d0c000-8525-4228-bc56-239e161e5423", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. ", "original_text": "After building a large ensemble of trees, i.e., a forest, the training is complete. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9d36cec-13ce-46cb-aa17-6904632889df", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching process of the tree occurs by selecting a random dimension $x_i$ with $i \\in \\{1,2,..., N\\}$ of the data (a single variable).  It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training. ", "original_text": "The process begins again with a new random sub-sample to build another randomized tree. "}, "hash": "1d650d4a45ee1d62e7cee7a01f48eb1380679220be06b6254cee56eec3032a7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "813cb59b-f78f-4ae6-910f-7a1bebb312f4", "node_type": "1", "metadata": {"window": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. ", "original_text": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. "}, "hash": "90f689789f3cd01dba6b601a6e0a360f1752db143a0807eb1ed4fd501f9b837c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After building a large ensemble of trees, i.e., a forest, the training is complete. ", "mimetype": "text/plain", "start_char_idx": 14806, "end_char_idx": 14890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "813cb59b-f78f-4ae6-910f-7a1bebb312f4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. ", "original_text": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3d0c000-8525-4228-bc56-239e161e5423", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It then selects a random value $v$ within the minimum and maximum values in that dimension.  If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. ", "original_text": "After building a large ensemble of trees, i.e., a forest, the training is complete. "}, "hash": "76029557a08ac23d7f77f7778ac6da82ecdac77b7d7e8390c9447116ae896f5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "928087a3-1f6b-4644-b43a-a57a194a7e13", "node_type": "1", "metadata": {"window": "In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig. ", "original_text": "4.\n\n"}, "hash": "e3b191446a9d9767d9d743630c2846d53a6849cfd76b1c29abaf0a6ef7b13cab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 14890, "end_char_idx": 15136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "928087a3-1f6b-4644-b43a-a57a194a7e13", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig. ", "original_text": "4.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "813cb59b-f78f-4ae6-910f-7a1bebb312f4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "If a given data point possesses a value smaller than $v$ for dimension $x_i$, then that point is sent to the left branch, otherwise it is sent to the right branch.  In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. ", "original_text": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig. "}, "hash": "0ee9d9bf569da93e73dd09fb04ae6942b6e62088b2734daf79750e02f2975498", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc2428a1-8c58-45a3-99cf-c6ed7945931d", "node_type": "1", "metadata": {"window": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees. ", "original_text": "Fig. "}, "hash": "eb9e7a074a3c318e6476ae553b02bd8aa6c848709a89cf6cf94b8e9d6e0f6599", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.\n\n", "mimetype": "text/plain", "start_char_idx": 15136, "end_char_idx": 15140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc2428a1-8c58-45a3-99cf-c6ed7945931d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "928087a3-1f6b-4644-b43a-a57a194a7e13", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this manner the data on the current node of the tree is split in two.  This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig. ", "original_text": "4.\n\n"}, "hash": "60c974b11e66acbf960b828da872b5f98e518d1e830270b38644abf8ccfc359b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b19b77f2-2bce-479e-872d-8da66446479f", "node_type": "1", "metadata": {"window": "The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit. ", "original_text": "4a shows a single tree after training. "}, "hash": "9909a8a0de99088b973d68ecbba08e3f0d705d423a4995f45c541eb17067f1a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 15140, "end_char_idx": 15145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b19b77f2-2bce-479e-872d-8da66446479f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit. ", "original_text": "4a shows a single tree after training. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc2428a1-8c58-45a3-99cf-c6ed7945931d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This process of branching is performed recursively over the dataset until a single point is isolated, or a predetermined depth limit is reached.  The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees. ", "original_text": "Fig. "}, "hash": "509df71159a795c6148fab73ac64a79decdc15109d4cf01ad3052207555256ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb61a373-c7b0-49ac-89d5-dde999d5dbfd", "node_type": "1", "metadata": {"window": "After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. ", "original_text": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. "}, "hash": "c670afca20fd2df62bd5f2a5bc7628cd0f8c922de614dd0684e86100df6f54e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4a shows a single tree after training. ", "mimetype": "text/plain", "start_char_idx": 15145, "end_char_idx": 15184, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb61a373-c7b0-49ac-89d5-dde999d5dbfd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. ", "original_text": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b19b77f2-2bce-479e-872d-8da66446479f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The process begins again with a new random sub-sample to build another randomized tree.  After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit. ", "original_text": "4a shows a single tree after training. "}, "hash": "3888bbaac3ecb1ffc9e0ea07f6e83cf02edd27f0b05652741192002a11f8be27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16310032-202d-4de6-a739-f4eddee863b9", "node_type": "1", "metadata": {"window": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones. ", "original_text": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. "}, "hash": "7e797389e03aab14f9b8a631957d85caeee2c3e3007a07bf300d0e341b4d8298", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. ", "mimetype": "text/plain", "start_char_idx": 15184, "end_char_idx": 15324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16310032-202d-4de6-a739-f4eddee863b9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones. ", "original_text": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb61a373-c7b0-49ac-89d5-dde999d5dbfd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After building a large ensemble of trees, i.e., a forest, the training is complete.  During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. ", "original_text": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point. "}, "hash": "a23df3b87e9ee76fc829ccde53f5672e7df0fe9d6584abdb67af15fe50222ccb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a174bf2-96b3-4df7-8b2d-0039771773cc", "node_type": "1", "metadata": {"window": "4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n", "original_text": "In Fig. "}, "hash": "5a34b2cdc1fe9516c831fa34d5f1f2d4f06da156544ec57a9614541b282ad59c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. ", "mimetype": "text/plain", "start_char_idx": 15324, "end_char_idx": 15447, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a174bf2-96b3-4df7-8b2d-0039771773cc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n", "original_text": "In Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16310032-202d-4de6-a739-f4eddee863b9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "During the scoring step, a new candidate data point (or one chosen from the data used to create the trees) is run through all the trees, and an ensemble anomaly score is assigned based on the depth the point reaches in each tree as shown in Fig.  4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones. ", "original_text": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case. "}, "hash": "31d06322d5b0bb97ecef0031f86b4f926f8469b5e015bcdb7db18ec9b07ca25e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30b7e963-702b-4b92-85fb-96cf06abc6af", "node_type": "1", "metadata": {"window": "Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig. ", "original_text": "4b we can see a full forest for an ensemble of 60 trees. "}, "hash": "dfabc939d925ec930ce85e7de67cf9a143d4dfc5d79ca70c0ba6266e2d2496eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Fig. ", "mimetype": "text/plain", "start_char_idx": 15447, "end_char_idx": 15455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30b7e963-702b-4b92-85fb-96cf06abc6af", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig. ", "original_text": "4b we can see a full forest for an ensemble of 60 trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a174bf2-96b3-4df7-8b2d-0039771773cc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4.\n\n Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n", "original_text": "In Fig. "}, "hash": "9f713dc7815532dc8eff9be594f24819db948c19aa070b4a58e44d61393189c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "712a421a-109a-4c7e-b4be-d8766665a435", "node_type": "1", "metadata": {"window": "4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. ", "original_text": "Each radial line represents one tree, while the outer circle represents the maximum depth limit. "}, "hash": "f54a02cd68405ac5a25e83487803826a6a4cf0b270cef73ae60e87a644783f73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4b we can see a full forest for an ensemble of 60 trees. ", "mimetype": "text/plain", "start_char_idx": 15455, "end_char_idx": 15512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "712a421a-109a-4c7e-b4be-d8766665a435", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. ", "original_text": "Each radial line represents one tree, while the outer circle represents the maximum depth limit. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30b7e963-702b-4b92-85fb-96cf06abc6af", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig. ", "original_text": "4b we can see a full forest for an ensemble of 60 trees. "}, "hash": "ff61bcb1cda35bc8c9ce6f452db6bf73a2770bb7277597e898826d57754a7526", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98ac5db2-3682-4d7b-971f-822ecc2818b6", "node_type": "1", "metadata": {"window": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm. ", "original_text": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. "}, "hash": "61f7fb6b3181f0cbd3caeacdb1f0f029fa76e92d086ca1887d43734f4efcdce4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each radial line represents one tree, while the outer circle represents the maximum depth limit. ", "mimetype": "text/plain", "start_char_idx": 15512, "end_char_idx": 15609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98ac5db2-3682-4d7b-971f-822ecc2818b6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm. ", "original_text": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "712a421a-109a-4c7e-b4be-d8766665a435", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4a shows a single tree after training.  The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. ", "original_text": "Each radial line represents one tree, while the outer circle represents the maximum depth limit. "}, "hash": "d457453e9c6d88463445c3060579f2b88628c09cd7a9420aadcb5c21efbee216", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27d28122-2c17-454b-b2e7-36ffeeeba0a2", "node_type": "1", "metadata": {"window": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores. ", "original_text": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones. "}, "hash": "76f99b1baa1b604a8d5233b926721a0e510d5029300e5c55b4d11ce4dccc199c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. ", "mimetype": "text/plain", "start_char_idx": 15609, "end_char_idx": 15754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "27d28122-2c17-454b-b2e7-36ffeeeba0a2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores. ", "original_text": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98ac5db2-3682-4d7b-971f-822ecc2818b6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The red line depicts the trajectory of a single anomalous point traveling down the tree, while the blue line shows that of a nominal point.  The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm. ", "original_text": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point. "}, "hash": "3a19591d31af04680c8c31203ea199b17f034b5407f1695c70d60cf3177d0728", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "585c1931-204f-4fa9-8939-9ba3b1798db8", "node_type": "1", "metadata": {"window": "In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules. ", "original_text": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n"}, "hash": "06ae5563b61322d2976abd5765b204214e725e01ede5d3f7d2c19479566cef6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones. ", "mimetype": "text/plain", "start_char_idx": 15754, "end_char_idx": 15845, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "585c1931-204f-4fa9-8939-9ba3b1798db8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules. ", "original_text": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27d28122-2c17-454b-b2e7-36ffeeeba0a2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The anomalous point is isolated very quickly, but the nominal point travels all the way to the maximum depth in this case.  In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores. ", "original_text": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones. "}, "hash": "04c679c07591623734da30e0d61ef82aa411c705c2311714b7f769058441871d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21997098-69ba-44de-b813-5c4632c53dc3", "node_type": "1", "metadata": {"window": "4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n", "original_text": "Using the two dimensional data from Fig. "}, "hash": "98227eff5a1663db845f108fea34e2e470ec955781a43018c433bcba85946398", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n", "mimetype": "text/plain", "start_char_idx": 15845, "end_char_idx": 15951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21997098-69ba-44de-b813-5c4632c53dc3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n", "original_text": "Using the two dimensional data from Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "585c1931-204f-4fa9-8939-9ba3b1798db8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Fig.  4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules. ", "original_text": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n"}, "hash": "6512f4c5a5c5a1655fb2791b46fa9c8b92d484a578f309d7521d8270e3e2e6af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dbff987-e84b-4e18-96e5-531a01a14971", "node_type": "1", "metadata": {"window": "Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. ", "original_text": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. "}, "hash": "54dcd93163e3027cc6fa68b244cf3f4facc9a6e71a4c47bbe709862321ad81a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the two dimensional data from Fig. ", "mimetype": "text/plain", "start_char_idx": 15951, "end_char_idx": 15992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0dbff987-e84b-4e18-96e5-531a01a14971", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. ", "original_text": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21997098-69ba-44de-b813-5c4632c53dc3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4b we can see a full forest for an ensemble of 60 trees.  Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n", "original_text": "Using the two dimensional data from Fig. "}, "hash": "c494f0719b9ae13dee9f72f6f561f82d87131e1e7b57ae51c5524d1cb2827c0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c961b65-ccce-4105-96a1-8c1de10d992d", "node_type": "1", "metadata": {"window": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n", "original_text": "The data points are then sent down the left or the right branch according to the rules of the algorithm. "}, "hash": "041ad0273d436e123de0b71b9722e06a880bf81a81905d9908cc5a50fd14df96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. ", "mimetype": "text/plain", "start_char_idx": 15992, "end_char_idx": 16220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c961b65-ccce-4105-96a1-8c1de10d992d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n", "original_text": "The data points are then sent down the left or the right branch according to the rules of the algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dbff987-e84b-4e18-96e5-531a01a14971", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Each radial line represents one tree, while the outer circle represents the maximum depth limit.  The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. ", "original_text": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension. "}, "hash": "a6d9d7308ad471a5abcc4482209096b7678fbd913e2dc00d86c78ccf4825003d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5219a81f-2b6f-4d5a-a677-4bb72dfc056b", "node_type": "1", "metadata": {"window": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9]. ", "original_text": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores. "}, "hash": "5a0ee212677a900bd3cf36bc18bbb1d3bf39b240872608f842e94c1f69af0bb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The data points are then sent down the left or the right branch according to the rules of the algorithm. ", "mimetype": "text/plain", "start_char_idx": 16220, "end_char_idx": 16325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5219a81f-2b6f-4d5a-a677-4bb72dfc056b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9]. ", "original_text": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c961b65-ccce-4105-96a1-8c1de10d992d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The red lines are trajectories that the single anomalous point has taken down each tree, and the blue ones show trajectories of a nominal point.  As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n", "original_text": "The data points are then sent down the left or the right branch according to the rules of the algorithm. "}, "hash": "9e9a1ee1d4c27603914d5318e60719bbb1de6a329ed59e35465a97758cf6e12e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57bf8fa4-964a-4d83-9d77-37b672ab0644", "node_type": "1", "metadata": {"window": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. ", "original_text": "So any newly observed data point can traverse down each tree following such trained rules. "}, "hash": "408a31c9ed5fcea6133733061641c1b3155c00b89bd4038b6533ac31a81dfe30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 16325, "end_char_idx": 16426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57bf8fa4-964a-4d83-9d77-37b672ab0644", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. ", "original_text": "So any newly observed data point can traverse down each tree following such trained rules. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5219a81f-2b6f-4d5a-a677-4bb72dfc056b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As can be seen, on average, the blue lines achieve a much larger radius than the red ones.  It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9]. ", "original_text": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores. "}, "hash": "63e69e9be1325e489a8e566dcd0255740e9dd2a8d418142c32d141ed76778e27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "209631a5-001a-4415-983b-a2f1e40e26df", "node_type": "1", "metadata": {"window": "Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig. ", "original_text": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n"}, "hash": "5a99e46248cfd034ccd5683367ab9bcede920832b7916b7916490b21d2040358", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So any newly observed data point can traverse down each tree following such trained rules. ", "mimetype": "text/plain", "start_char_idx": 16426, "end_char_idx": 16517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "209631a5-001a-4415-983b-a2f1e40e26df", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig. ", "original_text": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57bf8fa4-964a-4d83-9d77-37b672ab0644", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is on the basis of this idea that Isolation Forest is able to separate anomalies from nominal points.\n\n Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. ", "original_text": "So any newly observed data point can traverse down each tree following such trained rules. "}, "hash": "14881fc074e3697e997b11c90a2c21fb3b6cdf9771c7c243bb7a238522302ad6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b968ab8-4ba9-40de-88fa-c1c998de1167", "node_type": "1", "metadata": {"window": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. ", "original_text": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. "}, "hash": "dc9a58301e4296501b20237708531fbda20533eecdedfc4e1c580226766fc36d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n", "mimetype": "text/plain", "start_char_idx": 16517, "end_char_idx": 16637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b968ab8-4ba9-40de-88fa-c1c998de1167", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. ", "original_text": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "209631a5-001a-4415-983b-a2f1e40e26df", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Using the two dimensional data from Fig.  1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig. ", "original_text": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n"}, "hash": "b942871fed446b65da4d9923f3c5edd5522c94328eb42e11e8791416dbaa0dcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "882102bc-7c4b-4079-81d6-4fca7778b229", "node_type": "1", "metadata": {"window": "The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1. ", "original_text": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n"}, "hash": "d0a99ab7505bc4258a94c83c17b66d083ee11d9390a55a7ef312d8bfd7980f2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. ", "mimetype": "text/plain", "start_char_idx": 16637, "end_char_idx": 16764, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "882102bc-7c4b-4079-81d6-4fca7778b229", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1. ", "original_text": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b968ab8-4ba9-40de-88fa-c1c998de1167", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1a as a reference, during the training phase, the algorithm will create a tree by recursively picking a random dimension and comparing the value of any given point to a randomly selected cutoff value for the selected dimension.  The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. ", "original_text": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees. "}, "hash": "f2de14a1385ff8ac1f37135b810271bdbdf65589617ff03878cc7d802225c33f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5c4e88b-c197-4e93-bfd4-1208a99ef6fd", "node_type": "1", "metadata": {"window": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created. ", "original_text": "Trees are assigned maximum depth limits as described in [9]. "}, "hash": "8dc4caf85ae2770e7d20f31461c823a4748f999d2e7f3a6b6d8fcdfa9e755628", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n", "mimetype": "text/plain", "start_char_idx": 16764, "end_char_idx": 17089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5c4e88b-c197-4e93-bfd4-1208a99ef6fd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created. ", "original_text": "Trees are assigned maximum depth limits as described in [9]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "882102bc-7c4b-4079-81d6-4fca7778b229", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The data points are then sent down the left or the right branch according to the rules of the algorithm.  By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1. ", "original_text": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n"}, "hash": "85cb3681c901edb374d871d14c3f7d85657d299d37ee067f0ff3e009a20ac78b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82273bac-7beb-4eb1-831e-1ea9bb198d19", "node_type": "1", "metadata": {"window": "So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. ", "original_text": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. "}, "hash": "ff667accb23cbac37b0507a615cd2c407407d13a41bb13d9998ce5e276027034", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Trees are assigned maximum depth limits as described in [9]. ", "mimetype": "text/plain", "start_char_idx": 17089, "end_char_idx": 17150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82273bac-7beb-4eb1-831e-1ea9bb198d19", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. ", "original_text": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5c4e88b-c197-4e93-bfd4-1208a99ef6fd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "By creating many such trees, we can use the average depths of the branches to assign anomaly scores.  So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created. ", "original_text": "Trees are assigned maximum depth limits as described in [9]. "}, "hash": "734928bd22362e37dc8d330c0a5c671ea7e23a5b7d7201df4fe5ee5fa8064ef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c107d54a-9369-4cf6-bb6a-18903bb5b10b", "node_type": "1", "metadata": {"window": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal. ", "original_text": "Fig. "}, "hash": "06cbe7acced2f835dbbdc47cc2a4abaaf2f2034b0898c1b55dccdaa89597418e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. ", "mimetype": "text/plain", "start_char_idx": 17150, "end_char_idx": 17281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c107d54a-9369-4cf6-bb6a-18903bb5b10b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82273bac-7beb-4eb1-831e-1ea9bb198d19", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "So any newly observed data point can traverse down each tree following such trained rules.  The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. ", "original_text": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value. "}, "hash": "e5a4b68e9785eb733b69ff46facd04bca18d1ccf7be7618beab015029b3be487", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62623476-3840-4985-a776-d2ad84e0c556", "node_type": "1", "metadata": {"window": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step). ", "original_text": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. "}, "hash": "3699beda331f3806f6ae46c7ef5aff7feb921fb2c4f26c7bccfb4d131a327b1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 17281, "end_char_idx": 17286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62623476-3840-4985-a776-d2ad84e0c556", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step). ", "original_text": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c107d54a-9369-4cf6-bb6a-18903bb5b10b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The average depth of the branches this data point traverses will be translated to an anomaly score using Equation (1).\n\n s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal. ", "original_text": "Fig. "}, "hash": "dccb60a6e03d5b166068787de850dbde85649e1bf5cca6a74ad78665d90d4182", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24a8f38d-0438-4259-bfd9-77b4d63dd609", "node_type": "1", "metadata": {"window": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n", "original_text": "1. "}, "hash": "4472612279b394d106bd9b4bebfeba3a5c438696f29bc82c604d8bb05b9ca4ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. ", "mimetype": "text/plain", "start_char_idx": 17286, "end_char_idx": 17452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24a8f38d-0438-4259-bfd9-77b4d63dd609", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n", "original_text": "1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62623476-3840-4985-a776-d2ad84e0c556", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "s(x,n) = 2$^{-E(h(x))/c(n)}$, (1)\n\nwhere E(h(x)) is the mean value of the depths a single data point, x, reaches in all trees.  c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step). ", "original_text": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig. "}, "hash": "2bcf1cd937018bcefc89878e723ecba0971acb20e00d398116d5358f911759aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfa4d013-b45c-4271-bab9-6cbd8671a239", "node_type": "1", "metadata": {"window": "Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig. ", "original_text": "The numbers on the branching lines represent the order in which they were created. "}, "hash": "59934518430d9bbe35f9c46da27de6a7bc61eb8bcaee6a74392f21f298f5c5df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. ", "mimetype": "text/plain", "start_char_idx": 17452, "end_char_idx": 17455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bfa4d013-b45c-4271-bab9-6cbd8671a239", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig. ", "original_text": "The numbers on the branching lines represent the order in which they were created. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24a8f38d-0438-4259-bfd9-77b4d63dd609", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "c(n) is the normalizing factor defined as the average depth in an unsuccessful search in a Binary Search Tree (BST):\n\nc(n) = 2H(n - 1) \u2013 (2(n \u2212 1)/n), (2)\n\nwhere H(i) is the harmonic number and can be estimated by ln(i) + 0.5772156649 (Euler\u2019s constant) [11] and n is the number of points used in the construction of trees.\n\n Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n", "original_text": "1. "}, "hash": "5c9fa98ddb5a962fe6072454a69f024f477f318de495dcae67ab0ac5b28ce1fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "609cb7a8-29ea-425a-a8a1-335e947ebef6", "node_type": "1", "metadata": {"window": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3. ", "original_text": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. "}, "hash": "f5f311e628ea897a7386ec5fa4a881e9687986e015a34487e056025a8959802e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The numbers on the branching lines represent the order in which they were created. ", "mimetype": "text/plain", "start_char_idx": 17455, "end_char_idx": 17538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "609cb7a8-29ea-425a-a8a1-335e947ebef6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3. ", "original_text": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfa4d013-b45c-4271-bab9-6cbd8671a239", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Trees are assigned maximum depth limits as described in [9].  In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig. ", "original_text": "The numbers on the branching lines represent the order in which they were created. "}, "hash": "3d70d0cf1979bec82e97d1e66fcc36c9a5e1b8fd5798b45e2ed64ef8a7643eb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "574ccd01-5d70-4089-8d58-b37c5612b97d", "node_type": "1", "metadata": {"window": "Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map.", "original_text": "In this case the random cuts are all either vertical or horizontal. "}, "hash": "f1ca78152df965f7137d53307abdaf68eb70cd13820f981c0317feaf9de2b931", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. ", "mimetype": "text/plain", "start_char_idx": 17538, "end_char_idx": 17712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "574ccd01-5d70-4089-8d58-b37c5612b97d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map.", "original_text": "In this case the random cuts are all either vertical or horizontal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "609cb7a8-29ea-425a-a8a1-335e947ebef6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case a point runs too deep in the tree, the branching process is stopped and the point is assigned the maximum depth value.  Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3. ", "original_text": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated. "}, "hash": "0557640a88e831604035254ec5d74a01623a6f44066153bbf93fdeb6dd326e59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06db4978-fe03-42ae-9e62-641db38e3791", "node_type": "1", "metadata": {"window": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n", "original_text": "This is because of how we define the branching criteria (picking one dimension at each step). "}, "hash": "7ef64b1171c15f795ca56872a7cf190ac481e6a7ff4aed8e6b0fd7d8cf50c248", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case the random cuts are all either vertical or horizontal. ", "mimetype": "text/plain", "start_char_idx": 17712, "end_char_idx": 17780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06db4978-fe03-42ae-9e62-641db38e3791", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n", "original_text": "This is because of how we define the branching criteria (picking one dimension at each step). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "574ccd01-5d70-4089-8d58-b37c5612b97d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map.", "original_text": "In this case the random cuts are all either vertical or horizontal. "}, "hash": "31629564bb6cf0073a0eee70aeab90194683b8809319c565d44de274ac93ebf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c644d8a7-f258-4505-b57f-c6aecb22c5bf", "node_type": "1", "metadata": {"window": "1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space. ", "original_text": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n"}, "hash": "3e5f08837ac80ff4b4daeb86295c2d28984a1abb17494ada3c52f6204ca267fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is because of how we define the branching criteria (picking one dimension at each step). ", "mimetype": "text/plain", "start_char_idx": 17780, "end_char_idx": 17874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c644d8a7-f258-4505-b57f-c6aecb22c5bf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space. ", "original_text": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06db4978-fe03-42ae-9e62-641db38e3791", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5 shows the branching process during the training phase for an anomaly and a nominal example points using the standard Isolation Forest for the data depicted in Fig.  1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n", "original_text": "This is because of how we define the branching criteria (picking one dimension at each step). "}, "hash": "829bc0918abe95977f0c00a8924806aa9392984f9243bc31b9b3d679be02373a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20623066-be97-43cb-8202-f0f851cc96ab", "node_type": "1", "metadata": {"window": "The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n", "original_text": "***\n**Fig. "}, "hash": "52d75d18327f8f1e28b0428290b4b51f3309f0213bff170b7fbb047bba11e591", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n", "mimetype": "text/plain", "start_char_idx": 17874, "end_char_idx": 17987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20623066-be97-43cb-8202-f0f851cc96ab", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c644d8a7-f258-4505-b57f-c6aecb22c5bf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1.  The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space. ", "original_text": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n"}, "hash": "dc6072b175896f9989f1719161c0a80fb77ba1e8c1f477f40dd96b3bc8d91813", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51ee2670-c1bf-4a6a-90e2-37d7bd3b50ce", "node_type": "1", "metadata": {"window": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig. ", "original_text": "3. "}, "hash": "f8b859250d41bb6656cddd38bced9768423828420f93abe91b377f39de600595", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 17987, "end_char_idx": 17998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51ee2670-c1bf-4a6a-90e2-37d7bd3b50ce", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig. ", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20623066-be97-43cb-8202-f0f851cc96ab", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The numbers on the branching lines represent the order in which they were created.  In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n", "original_text": "***\n**Fig. "}, "hash": "f76eec58254c3751ae46be8ea6373fbdce8d7e6e6ad7d1abc5423a79172fa21f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca983990-889d-4ffc-9e95-813e1fe186df", "node_type": "1", "metadata": {"window": "In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4. ", "original_text": "Data with a sinusoidal structure and the anomaly score map."}, "hash": "f4644adbc107a07237bc476acc0a0ecbddaa81d56d08122f620346b236feb94d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 17998, "end_char_idx": 18001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca983990-889d-4ffc-9e95-813e1fe186df", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4. ", "original_text": "Data with a sinusoidal structure and the anomaly score map."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51ee2670-c1bf-4a6a-90e2-37d7bd3b50ce", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of the nominal point, the depth limit was reached since the point is very much at the center of the data and requires many random cuts to be completely isolated.  In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig. ", "original_text": "3. "}, "hash": "5b3eecd8f5732f78498f841fbbb3a19b5da06d93a75930fc297046514c923801", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "274ea0c1-bc20-4563-9efb-277ac9fa26ed", "node_type": "1", "metadata": {"window": "This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. ", "original_text": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n"}, "hash": "afe996a707e0fd3677f185a1355ec4619086d5141a124add439ab8c27231ff70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data with a sinusoidal structure and the anomaly score map.", "mimetype": "text/plain", "start_char_idx": 18001, "end_char_idx": 18060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "274ea0c1-bc20-4563-9efb-277ac9fa26ed", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. ", "original_text": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca983990-889d-4ffc-9e95-813e1fe186df", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the random cuts are all either vertical or horizontal.  This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4. ", "original_text": "Data with a sinusoidal structure and the anomaly score map."}, "hash": "673c723c37e37ef3990fb7f9b8e302877fe690ce38be2447f360ab46f76e26be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42b0582-bfcb-4e5c-a381-ae577906847b", "node_type": "1", "metadata": {"window": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point.", "original_text": "(b) Anomaly Score Map: A heatmap of the data space. "}, "hash": "cd4eecd0477d1aac865f1500428466d139da802098fd8ef98d3f6c5c578db5f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n", "mimetype": "text/plain", "start_char_idx": 18060, "end_char_idx": 18208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e42b0582-bfcb-4e5c-a381-ae577906847b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point.", "original_text": "(b) Anomaly Score Map: A heatmap of the data space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "274ea0c1-bc20-4563-9efb-277ac9fa26ed", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is because of how we define the branching criteria (picking one dimension at each step).  This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. ", "original_text": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n"}, "hash": "9f3a83b406f952c6fb3d302ec26bbba429d200575da0402de34b06b7aa3b7f04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f4df83e-08ba-4f6d-bfb5-59b61e3e686c", "node_type": "1", "metadata": {"window": "***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree. ", "original_text": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n"}, "hash": "23a3dc837de81155262f4f1154a2b95a6b4ce5cde4e1a839e77ca76e74dab9f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Anomaly Score Map: A heatmap of the data space. ", "mimetype": "text/plain", "start_char_idx": 18208, "end_char_idx": 18260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f4df83e-08ba-4f6d-bfb5-59b61e3e686c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree. ", "original_text": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42b0582-bfcb-4e5c-a381-ae577906847b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is also the reason for the strange behavior we observed in the anomaly score maps of the previous section.\n\n ***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point.", "original_text": "(b) Anomaly Score Map: A heatmap of the data space. "}, "hash": "ca1003da4c6156868ba2b09e531357bb667ce9f533e6ffdff311cc3420562f16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "868474f3-edab-4873-a01d-f3d6abf2b3af", "node_type": "1", "metadata": {"window": "3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly. ", "original_text": "**Fig. "}, "hash": "e6979bac62dd33f195e7b022c1bca92a483969133c39afc01bc094e42c99efdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n", "mimetype": "text/plain", "start_char_idx": 18260, "end_char_idx": 18478, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "868474f3-edab-4873-a01d-f3d6abf2b3af", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f4df83e-08ba-4f6d-bfb5-59b61e3e686c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree. ", "original_text": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n"}, "hash": "c2672173c1e421320a3df337effe8538ac635ce5e0bf19ad9c57bf2721e97987", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9579b158-6f26-45ac-a028-fa94958baef4", "node_type": "1", "metadata": {"window": "Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n", "original_text": "4. "}, "hash": "43cd13052517c73b6cbc018457e5e7c3ad64f1f4680faba503c25cf3415e4c54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 18478, "end_char_idx": 18485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9579b158-6f26-45ac-a028-fa94958baef4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "868474f3-edab-4873-a01d-f3d6abf2b3af", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3.  Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly. ", "original_text": "**Fig. "}, "hash": "b2bf462ae13363986ec5c8f883c178b644d91686457ca5d3702d13a3d3123b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faa63901-3931-46d0-91e7-41ae3eba2ba1", "node_type": "1", "metadata": {"window": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. ", "original_text": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. "}, "hash": "64b7d07a3720f309c379867c7e6bba70bc8e453d3253fc7c87460b58189e8536", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 18485, "end_char_idx": 18488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "faa63901-3931-46d0-91e7-41ae3eba2ba1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. ", "original_text": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9579b158-6f26-45ac-a028-fa94958baef4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data with a sinusoidal structure and the anomaly score map. **\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n", "original_text": "4. "}, "hash": "58d56850f86203cd051760c8c733817f7b73763dafd784a4d83d8e314c0297df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fdd2a30-23c2-4faf-b22e-5d12d52b058b", "node_type": "1", "metadata": {"window": "(b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center. ", "original_text": "Red represents an anomaly while blue represents a nominal point."}, "hash": "ddf87986d2e046bd4e0bd596ac2662c3c038f40f43279abda8cc5b74f804bb82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. ", "mimetype": "text/plain", "start_char_idx": 18488, "end_char_idx": 18621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fdd2a30-23c2-4faf-b22e-5d12d52b058b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center. ", "original_text": "Red represents an anomaly while blue represents a nominal point."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faa63901-3931-46d0-91e7-41ae3eba2ba1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Sinusoidal data points with Gaussian noise: A scatter plot where points form a sine wave pattern along the x-axis, with some vertical noise.\n (b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. ", "original_text": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle. "}, "hash": "828a230101501bdbc23bfe18200e5cd0390534e6af99627e490e7eba8c2c005d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d791182-9939-4971-92c7-46bbfebd81b7", "node_type": "1", "metadata": {"window": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n", "original_text": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree. "}, "hash": "1d850877e2567f213ea53c6dd31aa6df00b833b08f29bf63a938804a9373cd37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Red represents an anomaly while blue represents a nominal point.", "mimetype": "text/plain", "start_char_idx": 18621, "end_char_idx": 18685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d791182-9939-4971-92c7-46bbfebd81b7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n", "original_text": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fdd2a30-23c2-4faf-b22e-5d12d52b058b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Anomaly Score Map: A heatmap of the data space.  The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center. ", "original_text": "Red represents an anomaly while blue represents a nominal point."}, "hash": "d5186777c626466f65cdaed7b6e579e495357aeb83856dc45bb96b8067ea2fde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52c56f92-3480-4def-b146-9c1f169beafb", "node_type": "1", "metadata": {"window": "**Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. ", "original_text": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly. "}, "hash": "e4bd3e5cc163eceb01f8ab69d2aaa1f174c0431f337f8cccabea748c8bfdfc98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree. ", "mimetype": "text/plain", "start_char_idx": 18685, "end_char_idx": 18765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52c56f92-3480-4def-b146-9c1f169beafb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. ", "original_text": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d791182-9939-4971-92c7-46bbfebd81b7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The sinusoidal band is bright yellow (low score), but it is enclosed within a larger rectangular region of orange and red (higher scores), failing to capture the empty space between the sine wave's peaks and troughs.\n\n **Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n", "original_text": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree. "}, "hash": "70ffd8a57e619b7aa0fb27b91186cac3d976571ff5997296bdc8ebb5d098b6dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14727c5d-5e82-4a9a-bc68-e89552b077ef", "node_type": "1", "metadata": {"window": "4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig. ", "original_text": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n"}, "hash": "817cc80674f3412cb751f8ab2c20a219544a30fc91ac27076aa16360af453b26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly. ", "mimetype": "text/plain", "start_char_idx": 18765, "end_char_idx": 18871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14727c5d-5e82-4a9a-bc68-e89552b077ef", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig. ", "original_text": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52c56f92-3480-4def-b146-9c1f169beafb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. ", "original_text": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly. "}, "hash": "5c4529d456f6e85b657b904b75b6757aba33efd9d1178c2b3cfedc150df6c6f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3254699-4c22-4932-94c3-e535a0f5d2f7", "node_type": "1", "metadata": {"window": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n", "original_text": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. "}, "hash": "0bd12e874117a8b7807b306e3327462e16511c6d1f9bc08e2abde5f6f4e039a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n", "mimetype": "text/plain", "start_char_idx": 18871, "end_char_idx": 18971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3254699-4c22-4932-94c3-e535a0f5d2f7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n", "original_text": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14727c5d-5e82-4a9a-bc68-e89552b077ef", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4.  Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig. ", "original_text": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n"}, "hash": "4206f00fa300dbc130efbe953125e3b15d4c6deddb1b3387daf8e70c296f63e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdaf9152-3816-4edc-9298-74e50f4649fd", "node_type": "1", "metadata": {"window": "Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. ", "original_text": "Red lines (anomalies) are short, stopping close to the center. "}, "hash": "e28ef51ee9bf2679fb4fe0c8bb97c665c5343d35da3aec7bdfbfa2ccb0ca36ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. ", "mimetype": "text/plain", "start_char_idx": 18971, "end_char_idx": 19110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdaf9152-3816-4edc-9298-74e50f4649fd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. ", "original_text": "Red lines (anomalies) are short, stopping close to the center. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3254699-4c22-4932-94c3-e535a0f5d2f7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Schematic representation of a single tree (a) and a forest (b) where each tree is a radial line from the center to the outer circle.  Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n", "original_text": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center. "}, "hash": "039b3d36dfd35507f526f9f6eee8551c66d171d27068bf838ec8428a1131da0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49b8b3b9-752f-4ab7-9d79-8345d176baf2", "node_type": "1", "metadata": {"window": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes. ", "original_text": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n"}, "hash": "4cf3be9875910b3e804dad71b52a0daf0b52a6f35defd5c9a97c3200f63fcec2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Red lines (anomalies) are short, stopping close to the center. ", "mimetype": "text/plain", "start_char_idx": 19110, "end_char_idx": 19173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49b8b3b9-752f-4ab7-9d79-8345d176baf2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes. ", "original_text": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdaf9152-3816-4edc-9298-74e50f4649fd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Red represents an anomaly while blue represents a nominal point. **\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. ", "original_text": "Red lines (anomalies) are short, stopping close to the center. "}, "hash": "050afe762c3c04bdacefa3be60038211f84c7e6f088e0038405fba2c4bb10e94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4fca938-36df-4df6-8fe6-a3d64eebd2c7", "node_type": "1", "metadata": {"window": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. ", "original_text": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. "}, "hash": "4ed718a4ca081b17cef7d5ed5afcc3d67431b24c1bd54b1ceb30754ed093650a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n", "mimetype": "text/plain", "start_char_idx": 19173, "end_char_idx": 19292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4fca938-36df-4df6-8fe6-a3d64eebd2c7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. ", "original_text": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49b8b3b9-752f-4ab7-9d79-8345d176baf2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Representation of a single tree in a forest: A diagram of a binary tree.  A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes. ", "original_text": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n"}, "hash": "5c3583a30dab949b32f0895945ee90d81a55b38f8c3d45d0b8746efdfa584a21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f727b3-e8bf-4f63-9208-f9f6ae651fcf", "node_type": "1", "metadata": {"window": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. ", "original_text": "Fig. "}, "hash": "c0a3b6f565aaa8c94c4237c95c33fa70ede4b1851ec37ad9ca2a73a9f8f42ab4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. ", "mimetype": "text/plain", "start_char_idx": 19292, "end_char_idx": 19428, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8f727b3-e8bf-4f63-9208-f9f6ae651fcf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4fca938-36df-4df6-8fe6-a3d64eebd2c7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A red path starting from the root terminates after only a few branches, representing an isolated anomaly.  A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. ", "original_text": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria. "}, "hash": "096ae0f2ef1ca3d58161355ec5bbc84bb5ba42b9c8d1a6692d8ef7a3a90dadf3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ace7a391-2c1e-46b3-9d88-5d6054be231b", "node_type": "1", "metadata": {"window": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig. ", "original_text": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n"}, "hash": "cdbbe8dcfbdbd968a141a087a5c2d5db5783f89fa9f77a2f69f32cb927f4f6d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 19428, "end_char_idx": 19433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ace7a391-2c1e-46b3-9d88-5d6054be231b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig. ", "original_text": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8f727b3-e8bf-4f63-9208-f9f6ae651fcf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A blue path starting from the root travels much deeper into the tree, representing a nominal point.\n (b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. ", "original_text": "Fig. "}, "hash": "9d489dced3da927387a3d9a80516bfe8e3070f20f843411175d77d0680b04e89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "341017cf-db7c-4b1a-9355-64603899a384", "node_type": "1", "metadata": {"window": "Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. ", "original_text": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. "}, "hash": "8e24a55207b982c67088d7c4a102ba1f4f976e4658f92ffaf2aa9d2c7a4e5866", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n", "mimetype": "text/plain", "start_char_idx": 19433, "end_char_idx": 19535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "341017cf-db7c-4b1a-9355-64603899a384", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. ", "original_text": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ace7a391-2c1e-46b3-9d88-5d6054be231b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Representation of a full forest where each radial line corresponds to a tree: A circular diagram with lines radiating from the center.  Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig. ", "original_text": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n"}, "hash": "34df10f94ea8e9389e2eb0739e9bff4387f7957fff234f0faeb2e3a907b86a80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdb233cc-64af-4661-91de-9bcfb15c93d1", "node_type": "1", "metadata": {"window": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig. ", "original_text": "Naturally, the branch cuts are simply parallel to the coordinate axes. "}, "hash": "8ff6b4dfd8770cc7df3d2f3ef381500086dc308792c688bb5f64c506ca4eb35b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. ", "mimetype": "text/plain", "start_char_idx": 19535, "end_char_idx": 19647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fdb233cc-64af-4661-91de-9bcfb15c93d1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig. ", "original_text": "Naturally, the branch cuts are simply parallel to the coordinate axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "341017cf-db7c-4b1a-9355-64603899a384", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Red lines (anomalies) are short, stopping close to the center.  Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. ", "original_text": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature. "}, "hash": "96cc0583714e5c97dd7dbe5197da7a29103f46fc6da96e3d3597181186f56aee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b83f725-cfaf-4406-9cc2-18af5dc1e652", "node_type": "1", "metadata": {"window": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. ", "original_text": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. "}, "hash": "88d08bd688d280bdd3421a006fc9d8aeb18e5064ab85c260f2a91c4db3d1d907", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Naturally, the branch cuts are simply parallel to the coordinate axes. ", "mimetype": "text/plain", "start_char_idx": 19647, "end_char_idx": 19718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b83f725-cfaf-4406-9cc2-18af5dc1e652", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. ", "original_text": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdb233cc-64af-4661-91de-9bcfb15c93d1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Blue lines (nominal points) are long, extending to or near the outer circle, which represents the maximum depth limit.\n ***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig. ", "original_text": "Naturally, the branch cuts are simply parallel to the coordinate axes. "}, "hash": "184826808aacc1a54f2e17a4f93d6bcc4c0048a88167bd444d106849ac61c9d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a5a6a79-3097-448d-ab0b-e2255ed91192", "node_type": "1", "metadata": {"window": "Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n", "original_text": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. "}, "hash": "b16ae96599d165ddb4a9dcd53801643943e27598a0bed2c4310d7f559e25717d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. ", "mimetype": "text/plain", "start_char_idx": 19718, "end_char_idx": 19925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2a5a6a79-3097-448d-ab0b-e2255ed91192", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n", "original_text": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b83f725-cfaf-4406-9cc2-18af5dc1e652", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nTo build intuition for why this is, let\u2019s consider one randomly selected, fully grown tree with all its branch rules and criteria.  Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. ", "original_text": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated. "}, "hash": "c71416fac91d6141ba0ce2e52f270461d0ad3f02de08d11b5c1694fa4b39205e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a4a61ca-dab3-47df-8116-fc3964b51ac1", "node_type": "1", "metadata": {"window": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. ", "original_text": "In fact Fig. "}, "hash": "b94ccc5129bdd6d823b19b6bf5b7a850d536ed789ceaac5a8278a7c7cea09970", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. ", "mimetype": "text/plain", "start_char_idx": 19925, "end_char_idx": 20114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a4a61ca-dab3-47df-8116-fc3964b51ac1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. ", "original_text": "In fact Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a5a6a79-3097-448d-ab0b-e2255ed91192", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n", "original_text": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them. "}, "hash": "22114a4c61455bba9a52ebc2cb121109b870b529daa19672279e5accd9091070", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22384e10-5620-4f71-b41a-4f0cc529a206", "node_type": "1", "metadata": {"window": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps. ", "original_text": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. "}, "hash": "be5ee2a05329992dbe67a4152c37443d569ec0cdddafd5dd7296c512c872e546", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact Fig. ", "mimetype": "text/plain", "start_char_idx": 20114, "end_char_idx": 20127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22384e10-5620-4f71-b41a-4f0cc529a206", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps. ", "original_text": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a4a61ca-dab3-47df-8116-fc3964b51ac1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6 shows the branch cuts generated for a tree for the three examples we have introduced in Section 2.\n\n Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. ", "original_text": "In fact Fig. "}, "hash": "f0831589f632a3c3c5657c39b2b66bd2968106706f1e5f509f418be653e573ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13432d35-dc56-4ce0-8576-490133f376bd", "node_type": "1", "metadata": {"window": "Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig. ", "original_text": "In Fig. "}, "hash": "0442b218f5da747dfa8665e2d1b4db6af93a360d170a363b300b709df1c933a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. ", "mimetype": "text/plain", "start_char_idx": 20127, "end_char_idx": 20253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13432d35-dc56-4ce0-8576-490133f376bd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig. ", "original_text": "In Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22384e10-5620-4f71-b41a-4f0cc529a206", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Note that in each step, we pick a random feature (dimension), $x_i$, and a random value, $v$, for this feature.  Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps. ", "original_text": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced. "}, "hash": "2ff37a73313f98821ca74bbe6aa0b16cc9d77e73e89d7b8cfab6bee8f43b2d09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "595c449e-ff96-4b90-8f6a-2dcb6e7dd097", "node_type": "1", "metadata": {"window": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case. ", "original_text": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. "}, "hash": "f47f179c92223d8d1107eda9fe90dd1cfc651525b093d87000e7c0e38b9d62ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Fig. ", "mimetype": "text/plain", "start_char_idx": 20253, "end_char_idx": 20261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "595c449e-ff96-4b90-8f6a-2dcb6e7dd097", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case. ", "original_text": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13432d35-dc56-4ce0-8576-490133f376bd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Naturally, the branch cuts are simply parallel to the coordinate axes.  But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig. ", "original_text": "In Fig. "}, "hash": "7216be6435bfdcbf1b6ec9509b8c911ccebda36fe20b2f1b40efe10f53faf34f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f8529b3-975c-430d-90e1-c7130228ab20", "node_type": "1", "metadata": {"window": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. ", "original_text": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n"}, "hash": "7d491b0cb70c7a7ebb2431b44ab7aca4d7c5e17605374771df1b47c680b0a35b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. ", "mimetype": "text/plain", "start_char_idx": 20261, "end_char_idx": 20471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f8529b3-975c-430d-90e1-c7130228ab20", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. ", "original_text": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "595c449e-ff96-4b90-8f6a-2dcb6e7dd097", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "But as we move down the branches of the tree and data is divided by these lines, the range of possible values for v decreases and so the lines tend to cluster where most of the data points are concentrated.  However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case. ", "original_text": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset. "}, "hash": "9b770156fca0fbeaa0ff48d775a9f50355fddeca9c0901cb785ea7452ce8f530", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4efe4953-4841-41cb-8a06-d8a7599c3919", "node_type": "1", "metadata": {"window": "In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b. ", "original_text": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. "}, "hash": "a64cf885948c38cc69c7ad2aa578f0391bdeaec250803c69a4331508b4a90e6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n", "mimetype": "text/plain", "start_char_idx": 20471, "end_char_idx": 20610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4efe4953-4841-41cb-8a06-d8a7599c3919", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b. ", "original_text": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f8529b3-975c-430d-90e1-c7130228ab20", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, because of the constraint that the branch cuts are only vertical and horizontal, regions that don\u2019t necessarily contain many data points end up with many branch cuts through them.  In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. ", "original_text": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n"}, "hash": "1fb377920287ab1af4acec03357050206d343fc2defa8cfa0aa159a6b635b1bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f955c5cb-ecab-4413-a747-db8202fcacbc", "node_type": "1", "metadata": {"window": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n", "original_text": "7b as seen in previous anomaly score maps. "}, "hash": "ccea7a16d4e3d551385d679de51c56b03a2434c11d14fae585bb8401ebdbc453", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. ", "mimetype": "text/plain", "start_char_idx": 20610, "end_char_idx": 20776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f955c5cb-ecab-4413-a747-db8202fcacbc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n", "original_text": "7b as seen in previous anomaly score maps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4efe4953-4841-41cb-8a06-d8a7599c3919", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In fact Fig.  7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b. ", "original_text": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig. "}, "hash": "3020689322f14fca9dbb12c18447f89f154f4669903c917cb7ce1e885cffb7b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a275d96f-8944-4d7f-af39-486f8f836658", "node_type": "1", "metadata": {"window": "In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm. ", "original_text": "Fig. "}, "hash": "8690e70879da051e96ca0c97174bb5fa095b81b97cf9f50a8c97ee55a4bce66e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7b as seen in previous anomaly score maps. ", "mimetype": "text/plain", "start_char_idx": 20776, "end_char_idx": 20819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a275d96f-8944-4d7f-af39-486f8f836658", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f955c5cb-ecab-4413-a747-db8202fcacbc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7 shows a typical distribution of the only possible branching lines (hyperplanes in general) that could possibly be produced.  In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n", "original_text": "7b as seen in previous anomaly score maps. "}, "hash": "bd76fb5af60935b02df4618059245dc2390167ecf91c4b9de441328a3a5f91d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74904a89-c510-4fd0-8b80-b3fe60c335e8", "node_type": "1", "metadata": {"window": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. ", "original_text": "7c shows an even more extreme case. "}, "hash": "2f419a301ceb3635be4cb1c1901c22b90da520057576e57d78a62df2a265a757", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 20819, "end_char_idx": 20824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74904a89-c510-4fd0-8b80-b3fe60c335e8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. ", "original_text": "7c shows an even more extreme case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a275d96f-8944-4d7f-af39-486f8f836658", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Fig.  7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm. ", "original_text": "Fig. "}, "hash": "01692586d81edcad1ae27e1fd2ca593c4e80a150a7ff76723ccdcff1f8226bd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ff7bedb-ae56-4ced-8740-2f411ab6adcf", "node_type": "1", "metadata": {"window": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). ", "original_text": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. "}, "hash": "165e42c59ed30fb9cb1eecabddbfa20793a7cc778f6a80de06656b6b3d4e63b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7c shows an even more extreme case. ", "mimetype": "text/plain", "start_char_idx": 20824, "end_char_idx": 20860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ff7bedb-ae56-4ced-8740-2f411ab6adcf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). ", "original_text": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74904a89-c510-4fd0-8b80-b3fe60c335e8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7a, a data point near (4,0) will be subject to many more branching operations compared to a point that falls near (3,3), despite the fact that they are both anomalies with respect to the center of the dataset.  As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. ", "original_text": "7c shows an even more extreme case. "}, "hash": "4151217acc81cd6115bc779ff4ea76035199b4be04747fe9489f5f7d4e62478d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a43c948-67d4-4b34-91b7-5575fc4bc08a", "node_type": "1", "metadata": {"window": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. ", "original_text": "3b. "}, "hash": "bd037e98cbfa9d827e1b944907c4632cd9d9f05f1b4ba2a8c546e896df074812", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. ", "mimetype": "text/plain", "start_char_idx": 20860, "end_char_idx": 21077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a43c948-67d4-4b34-91b7-5575fc4bc08a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. ", "original_text": "3b. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ff7bedb-ae56-4ced-8740-2f411ab6adcf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As the number of branching operations is directly related to the anomaly score, these two points might end up with very different scores.\n\n The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). ", "original_text": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig. "}, "hash": "735d432484506763fce5762fe27c11954102b26383f24ba46c01c0b5be27b37f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "578bc75e-301b-4250-a15c-a95dff242d75", "node_type": "1", "metadata": {"window": "7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. ", "original_text": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n"}, "hash": "f5cb229c3d7272c864ecbff5dfc362bedd6b900f4eda691ba68b38e7f854ec1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3b. ", "mimetype": "text/plain", "start_char_idx": 21077, "end_char_idx": 21081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "578bc75e-301b-4250-a15c-a95dff242d75", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. ", "original_text": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a43c948-67d4-4b34-91b7-5575fc4bc08a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The same phenomenon happens in the other two cases, but it is even worsened as these branch cuts intersect and form \u201cghost\u201d cluster regions, e.g., near (0,0) in Fig.  7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. ", "original_text": "3b. "}, "hash": "cc87ff88dd4cc25d3f362313108b5910faf343acc4e6fbfc074b8e3a6d05a174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31293b83-23ef-46ac-93b3-bd0865a5b016", "node_type": "1", "metadata": {"window": "Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. ", "original_text": "We propose two different ways of addressing this limitation to provide a much more robust algorithm. "}, "hash": "fd3b0bc2cdec879547a5512abae2676eb09e948651b7db4594f307539633d16c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n", "mimetype": "text/plain", "start_char_idx": 21081, "end_char_idx": 21223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31293b83-23ef-46ac-93b3-bd0865a5b016", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. ", "original_text": "We propose two different ways of addressing this limitation to provide a much more robust algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "578bc75e-301b-4250-a15c-a95dff242d75", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7b as seen in previous anomaly score maps.  Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. ", "original_text": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n"}, "hash": "56132bc4ad286979d4a82881d1ff2adbcb9290986b9c73eca416ce2540b8607e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3920fe9f-c348-4e18-b2c9-73204a8610a8", "node_type": "1", "metadata": {"window": "7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. ", "original_text": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. "}, "hash": "dbdecc9f35f77f279c8d929a03869e762e460711ccb6f26cd094aab9bf3e1711", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We propose two different ways of addressing this limitation to provide a much more robust algorithm. ", "mimetype": "text/plain", "start_char_idx": 21223, "end_char_idx": 21324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3920fe9f-c348-4e18-b2c9-73204a8610a8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. ", "original_text": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31293b83-23ef-46ac-93b3-bd0865a5b016", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. ", "original_text": "We propose two different ways of addressing this limitation to provide a much more robust algorithm. "}, "hash": "ae8ac018cee8b2a352e2bf60e119ed5fd20efc0335b164ac6d1f9d386b222e6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13a9f1cb-3a23-4dc8-9697-d0e51a42653f", "node_type": "1", "metadata": {"window": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n", "original_text": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). "}, "hash": "2dfad760058a8862b674e976ad1c82f9fe7bcb1589bf860d76ce585052001f07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. ", "mimetype": "text/plain", "start_char_idx": 21324, "end_char_idx": 21441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13a9f1cb-3a23-4dc8-9697-d0e51a42653f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n", "original_text": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3920fe9f-c348-4e18-b2c9-73204a8610a8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7c shows an even more extreme case.  The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. ", "original_text": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification. "}, "hash": "0cca757f46496914fcd06efec8992e02e8ae4fb548f4833b077f56742694af92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3625c747-1ebe-440a-a53f-2ddbe45cb7b7", "node_type": "1", "metadata": {"window": "3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig. ", "original_text": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. "}, "hash": "a578f306d2154374f06d9e76d55cf5e7cf1a84b102d1417c36f8d467e5c05905", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). ", "mimetype": "text/plain", "start_char_idx": 21441, "end_char_idx": 21590, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3625c747-1ebe-440a-a53f-2ddbe45cb7b7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig. ", "original_text": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13a9f1cb-3a23-4dc8-9697-d0e51a42653f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The data repeats itself over one feature many times, and so the branch cuts in one direction are created in a much more biased fashion resulting in an inability to decipher the structure of the data as we saw in Fig.  3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n", "original_text": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane). "}, "hash": "8c45d3da3eb9675fe386a89f12e31c64932c274b8459d8eb375a8af7d1e7279b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8989ad28-52b8-46bb-8559-7a8fb6d60fd0", "node_type": "1", "metadata": {"window": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5. ", "original_text": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. "}, "hash": "0dbc7161bf314d3df529558858db2174d9bc4ece6b6ab84afbda5183ab261fc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. ", "mimetype": "text/plain", "start_char_idx": 21590, "end_char_idx": 21742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8989ad28-52b8-46bb-8559-7a8fb6d60fd0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5. ", "original_text": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3625c747-1ebe-440a-a53f-2ddbe45cb7b7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3b.  It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig. ", "original_text": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping. "}, "hash": "d80b91ff5eb9285caa3ecb03f6b2d7ed2952d15f452de296a80574c8d9278f44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a22799b-a42f-43c5-b89f-ad025b694c00", "node_type": "1", "metadata": {"window": "We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point). ", "original_text": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. "}, "hash": "d4299018d7df64d6beee610a2adac22fdea4351f43af52545300923be5549015", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. ", "mimetype": "text/plain", "start_char_idx": 21742, "end_char_idx": 21923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a22799b-a42f-43c5-b89f-ad025b694c00", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point). ", "original_text": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8989ad28-52b8-46bb-8559-7a8fb6d60fd0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is then evident that regions of similar anomalous properties receive very different branching operations throughout the training process.\n\n We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5. ", "original_text": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node. "}, "hash": "44af3737d1cff1cb0b900138a1fbe4348d2cd2f0aee593c6508eca153f1d7e28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da717e2e-43bf-4a05-806c-58189a48492d", "node_type": "1", "metadata": {"window": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated. ", "original_text": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. "}, "hash": "0b68e4f64517eed5612a1f326b83fa7b8fc40c250c75e4af4b6b0304d17fe345", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. ", "mimetype": "text/plain", "start_char_idx": 21923, "end_char_idx": 22066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da717e2e-43bf-4a05-806c-58189a48492d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated. ", "original_text": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a22799b-a42f-43c5-b89f-ad025b694c00", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We propose two different ways of addressing this limitation to provide a much more robust algorithm.  In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point). ", "original_text": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method. "}, "hash": "05841d42e8aa1fb5606327f21ab11bd3b345db8d8ad077c6079223f503c83dc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "746b8b00-76d7-4e04-9ce1-e963a5a1234d", "node_type": "1", "metadata": {"window": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point. ", "original_text": "In the following two sub-sections we explore and present each method.\n\n"}, "hash": "da268a774f23b7469325ade369ac900daf71803b10d9a89d1974f0d167237708", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. ", "mimetype": "text/plain", "start_char_idx": 22066, "end_char_idx": 22208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "746b8b00-76d7-4e04-9ce1-e963a5a1234d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point. ", "original_text": "In the following two sub-sections we explore and present each method.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da717e2e-43bf-4a05-806c-58189a48492d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the first method, the standard Isolation Forest algorithm is allowed to run as is, but with a small modification.  Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated. ", "original_text": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here. "}, "hash": "94392bee8403650c355911897a85653c3f4f85c54f4a83febf3a43698aba4bec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ce2155c-30ed-4b78-a04b-cc51e8d870b3", "node_type": "1", "metadata": {"window": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point). ", "original_text": "***\n**Fig. "}, "hash": "9f2e076b99732e5b419be950429ae36cd387c4fadcedb65da7079b557fd83688", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the following two sub-sections we explore and present each method.\n\n", "mimetype": "text/plain", "start_char_idx": 22208, "end_char_idx": 22279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ce2155c-30ed-4b78-a04b-cc51e8d870b3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point). ", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "746b8b00-76d7-4e04-9ce1-e963a5a1234d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Each time it picks a sub-sample of the training data to create a tree, the training data has to undergo a random transformation (rotation in plane).  In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point. ", "original_text": "In the following two sub-sections we explore and present each method.\n\n"}, "hash": "8bb07ecbb5d2d5a38685ec6997eb66f050c23d5cb4f70176aa0ee15576354df7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7966e37c-8c64-4ebf-a78f-0e49539e2699", "node_type": "1", "metadata": {"window": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point. ", "original_text": "5. "}, "hash": "eb6675a39c943fb909b0344dc0667d7cdfb416fc0fde73630f68b83b77b29631", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 22279, "end_char_idx": 22290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7966e37c-8c64-4ebf-a78f-0e49539e2699", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point. ", "original_text": "5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ce2155c-30ed-4b78-a04b-cc51e8d870b3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case each tree has to carry the information about this transformation as it will be necessary in the scoring stage, increasing the bookkeeping.  In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point). ", "original_text": "***\n**Fig. "}, "hash": "f3a650cf534ea1deca70863ce4a87ad8e410e96279076daa326f33091925575e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77c406fb-3fb6-4c22-9b0b-9aac245fdc06", "node_type": "1", "metadata": {"window": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated. ", "original_text": "5a shows the branching process for an anomalous data point (red point). "}, "hash": "042cb307322ae00138d54c211754139b726acaa2b3e98966a666f9567cf97010", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. ", "mimetype": "text/plain", "start_char_idx": 22290, "end_char_idx": 22293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77c406fb-3fb6-4c22-9b0b-9aac245fdc06", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated. ", "original_text": "5a shows the branching process for an anomalous data point (red point). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7966e37c-8c64-4ebf-a78f-0e49539e2699", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the second method we modify and generalize the branching process by allowing the branch cuts to occur in random directions with respect the current data points on the tree node.  Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point. ", "original_text": "5. "}, "hash": "fe89b774db39f216d441410e5209d990f3a1f9af5a9713cdbd8ef7999b73c86f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5300b65-aba1-4538-bd0f-584db8d8d9ea", "node_type": "1", "metadata": {"window": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process.", "original_text": "The branching takes place until the point is isolated. "}, "hash": "a26b3d03f63713a4c623c6c24ee54e9a5d5f426caae384ad4c2ec00138796264", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5a shows the branching process for an anomalous data point (red point). ", "mimetype": "text/plain", "start_char_idx": 22293, "end_char_idx": 22365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5300b65-aba1-4538-bd0f-584db8d8d9ea", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process.", "original_text": "The branching takes place until the point is isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77c406fb-3fb6-4c22-9b0b-9aac245fdc06", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Under this perspective, the latter method is the truly general case, while the former can be considered a special case of this general method.  In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated. ", "original_text": "5a shows the branching process for an anomalous data point (red point). "}, "hash": "1697ff827760f638369cfeef0345a1802a3acea069a82b6ada12708325ea14f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fc3e830-d35c-4548-a61a-ff939b33d119", "node_type": "1", "metadata": {"window": "In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud. ", "original_text": "In this case it only took three random cuts to isolate the point. "}, "hash": "ee279206b2d05a4d3b546c8959c0f99b833431f7b781bf928b205e6343d17eaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The branching takes place until the point is isolated. ", "mimetype": "text/plain", "start_char_idx": 22365, "end_char_idx": 22420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6fc3e830-d35c-4548-a61a-ff939b33d119", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud. ", "original_text": "In this case it only took three random cuts to isolate the point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5300b65-aba1-4538-bd0f-584db8d8d9ea", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In fact we will show in the rest of the paper that the standard Isolation Forest is also a special case of this general method proposed here.  In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process.", "original_text": "The branching takes place until the point is isolated. "}, "hash": "e489be4b1370ee898f0f1d1133cad940372cd379645af85e5f74db609d0e77b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34e6afd3-dd94-483e-b884-b7e9e49874d8", "node_type": "1", "metadata": {"window": "***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n", "original_text": "5b shows the same branching process for a nominal point (red point). "}, "hash": "e4b4998bb9228943e65ebb5bfb0121c0c9b710d92af9a3041c44f82970f5b280", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case it only took three random cuts to isolate the point. ", "mimetype": "text/plain", "start_char_idx": 22420, "end_char_idx": 22486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34e6afd3-dd94-483e-b884-b7e9e49874d8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n", "original_text": "5b shows the same branching process for a nominal point (red point). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fc3e830-d35c-4548-a61a-ff939b33d119", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the following two sub-sections we explore and present each method.\n\n ***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud. ", "original_text": "In this case it only took three random cuts to isolate the point. "}, "hash": "6f6b08d3d053a60fcd3b06ce19af16cef6f38c7bf4602fe1aa9b420c062e7cee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94269e6c-b1b4-4d62-8c40-355cfaf9ee7a", "node_type": "1", "metadata": {"window": "5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud. ", "original_text": "Since the point is buried deep inside the data, it takes many cuts to isolate the point. "}, "hash": "b4e362b7b16533c8a91c817d1de353615c1cf428ecbbe8c1986377ef9624bdaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5b shows the same branching process for a nominal point (red point). ", "mimetype": "text/plain", "start_char_idx": 22486, "end_char_idx": 22555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94269e6c-b1b4-4d62-8c40-355cfaf9ee7a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud. ", "original_text": "Since the point is buried deep inside the data, it takes many cuts to isolate the point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34e6afd3-dd94-483e-b884-b7e9e49874d8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n", "original_text": "5b shows the same branching process for a nominal point (red point). "}, "hash": "d8baf6f90f9da0922a510a5fab2e6b9c1b34e77c3812eb6853c97a278aaa5a2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bf917cc-a782-4d0e-a71c-f5a5cdf8ddad", "node_type": "1", "metadata": {"window": "5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n", "original_text": "In this case the depth limit of the tree was reached before the point was completely isolated. "}, "hash": "3bed3a8d589a48b8aa7e97af80e0913e1ad6bf96cf191910d6af745b2bf42780", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the point is buried deep inside the data, it takes many cuts to isolate the point. ", "mimetype": "text/plain", "start_char_idx": 22555, "end_char_idx": 22644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0bf917cc-a782-4d0e-a71c-f5a5cdf8ddad", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n", "original_text": "In this case the depth limit of the tree was reached before the point was completely isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94269e6c-b1b4-4d62-8c40-355cfaf9ee7a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5.  5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud. ", "original_text": "Since the point is buried deep inside the data, it takes many cuts to isolate the point. "}, "hash": "9ea682aad5567f64bf7f98df2464d215e60adddc995f2e6d65cb55ed90676082", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b0780e7-ff1a-4fd3-9454-67c5f8f3b851", "node_type": "1", "metadata": {"window": "The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig. ", "original_text": "The numbers on the lines represent the order of branching process."}, "hash": "f871ca43d946c307dd0e863ce9556c6573eb100321c0a7c0cfdbc1f1dfccf401", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case the depth limit of the tree was reached before the point was completely isolated. ", "mimetype": "text/plain", "start_char_idx": 22644, "end_char_idx": 22739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b0780e7-ff1a-4fd3-9454-67c5f8f3b851", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig. ", "original_text": "The numbers on the lines represent the order of branching process."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bf917cc-a782-4d0e-a71c-f5a5cdf8ddad", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5a shows the branching process for an anomalous data point (red point).  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n", "original_text": "In this case the depth limit of the tree was reached before the point was completely isolated. "}, "hash": "15c128e82f955743f9694727fdf9b8e04c003dc5747b7e949bdba167cee23fdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d553ce82-ef84-4992-8f1b-cc2f1d4d14d7", "node_type": "1", "metadata": {"window": "In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6. ", "original_text": "**\n(a) Anomaly point: A scatter plot of a data cloud. "}, "hash": "4ef17993ffb438a5de75294231dc6f33e267b8b6be44f5ad46a125ca7c98dc69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The numbers on the lines represent the order of branching process.", "mimetype": "text/plain", "start_char_idx": 22739, "end_char_idx": 22805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d553ce82-ef84-4992-8f1b-cc2f1d4d14d7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6. ", "original_text": "**\n(a) Anomaly point: A scatter plot of a data cloud. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b0780e7-ff1a-4fd3-9454-67c5f8f3b851", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig. ", "original_text": "The numbers on the lines represent the order of branching process."}, "hash": "c2ee0c2630558695b23376fc64ddc54ef7ddd6d89708ddc315bf8a62fd6ff020", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24bda172-3904-4497-9c5a-91360491d339", "node_type": "1", "metadata": {"window": "5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase. ", "original_text": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n"}, "hash": "dda905d484aef3ae2a5b05e7f7d9be59aa87f9a15f69a1a6042f20e181e1b87a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Anomaly point: A scatter plot of a data cloud. ", "mimetype": "text/plain", "start_char_idx": 22805, "end_char_idx": 22859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24bda172-3904-4497-9c5a-91360491d339", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase. ", "original_text": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d553ce82-ef84-4992-8f1b-cc2f1d4d14d7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case it only took three random cuts to isolate the point.  5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6. ", "original_text": "**\n(a) Anomaly point: A scatter plot of a data cloud. "}, "hash": "7443201aa8062636d16f9634919eba0ebc64468b0594369d0640efffa181175b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc15cf30-4866-4975-89dc-73222fe2b46c", "node_type": "1", "metadata": {"window": "Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension). ", "original_text": "(b) Nominal point: The same data cloud. "}, "hash": "b032a2bd735ad1230a795d542b3e22832e5c2a029eee9fe280287ded5dd58225", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n", "mimetype": "text/plain", "start_char_idx": 22859, "end_char_idx": 22986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc15cf30-4866-4975-89dc-73222fe2b46c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension). ", "original_text": "(b) Nominal point: The same data cloud. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24bda172-3904-4497-9c5a-91360491d339", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5b shows the same branching process for a nominal point (red point).  Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase. ", "original_text": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n"}, "hash": "3ee4220c94cd7cb99d9f97b6cf084f16a1272464a6306c34ec3aa9946106d9b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8266bc21-30c9-4199-9a32-0f3d75ffb3c9", "node_type": "1", "metadata": {"window": "In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside.", "original_text": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n"}, "hash": "94c8a4717194b3c39ee759fc5ef3b7d98d44be4f3ef6ae0ee3eb1a7315f57f65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Nominal point: The same data cloud. ", "mimetype": "text/plain", "start_char_idx": 22986, "end_char_idx": 23026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8266bc21-30c9-4199-9a32-0f3d75ffb3c9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside.", "original_text": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc15cf30-4866-4975-89dc-73222fe2b46c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since the point is buried deep inside the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension). ", "original_text": "(b) Nominal point: The same data cloud. "}, "hash": "29855e83d2996e1df2f12a8cd3050a24e7fea9baa682f85b15c405a73145fc63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf00a810-382f-436e-80c1-a5454b5faca5", "node_type": "1", "metadata": {"window": "The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. ", "original_text": "**Fig. "}, "hash": "bef36d6e5e656c029203841a3c3dc56537217397c701468da26ca1fb0902a688", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n", "mimetype": "text/plain", "start_char_idx": 23026, "end_char_idx": 23203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf00a810-382f-436e-80c1-a5454b5faca5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8266bc21-30c9-4199-9a32-0f3d75ffb3c9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the depth limit of the tree was reached before the point was completely isolated.  The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside.", "original_text": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n"}, "hash": "39c32bd414f53a2064a3b0f804e21d8780fa58dffef922af22fa6730ab96501a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a809edb8-f820-4031-bef6-41e5a53b3c65", "node_type": "1", "metadata": {"window": "**\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n", "original_text": "6. "}, "hash": "d886e132a5276b29a3c4ac1520f0e654a9543628d011c4ee446bfe3afffd1434", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 23203, "end_char_idx": 23210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a809edb8-f820-4031-bef6-41e5a53b3c65", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n", "original_text": "6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf00a810-382f-436e-80c1-a5454b5faca5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The numbers on the lines represent the order of branching process. **\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. ", "original_text": "**Fig. "}, "hash": "9620b1852f3bf627d4b46411ab48902d46388c22a8aa0c0316b6cb536d5a9841", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d884436-b5f7-42a4-afde-ac4526e26fde", "node_type": "1", "metadata": {"window": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n", "original_text": "Branch cuts generated by the standard Isolation Forest during the training phase. "}, "hash": "bb03f301af9bd147e7062a5c652e855d9e0fb71e906a398af248c8018486e87b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. ", "mimetype": "text/plain", "start_char_idx": 23210, "end_char_idx": 23213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d884436-b5f7-42a4-afde-ac4526e26fde", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n", "original_text": "Branch cuts generated by the standard Isolation Forest during the training phase. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a809edb8-f820-4031-bef6-41e5a53b3c65", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Anomaly point: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n", "original_text": "6. "}, "hash": "2d922d2a69c2cd1c6b78e4c3d91aec7808886d8be4f507a37601b3b7343da6e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afdacf22-0e9e-42d3-ac16-bbe88b0b6be6", "node_type": "1", "metadata": {"window": "(b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n", "original_text": "In each step, a random value is selected from a random feature (dimension). "}, "hash": "9bfd8d091f273f478438639dd910544d9baea177e4d5db22a5c9828c422ce846", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Branch cuts generated by the standard Isolation Forest during the training phase. ", "mimetype": "text/plain", "start_char_idx": 23213, "end_char_idx": 23295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "afdacf22-0e9e-42d3-ac16-bbe88b0b6be6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n", "original_text": "In each step, a random value is selected from a random feature (dimension). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d884436-b5f7-42a4-afde-ac4526e26fde", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A single red point on the periphery is isolated by three lines: one horizontal (labeled 1) and two vertical (labeled 2 and 3).\n (b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n", "original_text": "Branch cuts generated by the standard Isolation Forest during the training phase. "}, "hash": "7c21119a4af8d7dc9e31a276b418ecbd38f44500b7b0929207c9be9c992b19d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17b2a29a-dfde-4513-a64c-dedebc7d1fa9", "node_type": "1", "metadata": {"window": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig. ", "original_text": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside."}, "hash": "d8d68dfce02c3f713281c3f5c51a7cfad49476754f5ec3dabf7501ea20b718be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In each step, a random value is selected from a random feature (dimension). ", "mimetype": "text/plain", "start_char_idx": 23295, "end_char_idx": 23371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17b2a29a-dfde-4513-a64c-dedebc7d1fa9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig. ", "original_text": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afdacf22-0e9e-42d3-ac16-bbe88b0b6be6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Nominal point: The same data cloud.  A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n", "original_text": "In each step, a random value is selected from a random feature (dimension). "}, "hash": "f65607e11e287bf80203b3c0701216af42377a8bfa2b02eb7a09a1f8efaae061", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e35530f-a5c3-42c4-ae10-a0708bbd2724", "node_type": "1", "metadata": {"window": "**Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7. ", "original_text": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. "}, "hash": "d805df54897eaa1bfb66f1dada5a081b4ce5e32c0434c5df833be447e0c6bbd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside.", "mimetype": "text/plain", "start_char_idx": 23371, "end_char_idx": 23510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e35530f-a5c3-42c4-ae10-a0708bbd2724", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7. ", "original_text": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b2a29a-dfde-4513-a64c-dedebc7d1fa9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A red point in the center is surrounded by numerous horizontal and vertical lines (labeled 1 through 10), showing a deep branching process that has not yet isolated the point.\n\n **Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig. ", "original_text": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside."}, "hash": "115ac2e0cf842b38ae0b86ea9a9d7134b6065c97323b70b395b140646f6e3721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fddf8c16-dbbd-4d5e-a8cc-f48f3f2c61fa", "node_type": "1", "metadata": {"window": "6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts. ", "original_text": "The lines are denser in the center where the data is concentrated.\n"}, "hash": "a2d06616197f0f3b1db8bcec2ffba7606b47f65343d29fca6b8f0614e1336b58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. ", "mimetype": "text/plain", "start_char_idx": 23510, "end_char_idx": 23638, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fddf8c16-dbbd-4d5e-a8cc-f48f3f2c61fa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts. ", "original_text": "The lines are denser in the center where the data is concentrated.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e35530f-a5c3-42c4-ae10-a0708bbd2724", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7. ", "original_text": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts. "}, "hash": "6773253bc36356d62d64981075e9c98917b21d54b7ba56b9bcc69b9da6937779", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb2277c1-7239-4637-84e2-8b89d6d66a2a", "node_type": "1", "metadata": {"window": "Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities.", "original_text": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n"}, "hash": "5fa596c58d3b3172d270a9ceed615c3246b9c0d010500bb7a3f971f14572c657", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lines are denser in the center where the data is concentrated.\n", "mimetype": "text/plain", "start_char_idx": 23638, "end_char_idx": 23705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb2277c1-7239-4637-84e2-8b89d6d66a2a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities.", "original_text": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fddf8c16-dbbd-4d5e-a8cc-f48f3f2c61fa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6.  Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts. ", "original_text": "The lines are denser in the center where the data is concentrated.\n"}, "hash": "3566f2799d2d8d09943ee560d338c66f2be2ecf497dec1de53cd091cb0abf302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f11db3d-42c5-4d5e-92bf-ad8263f2762f", "node_type": "1", "metadata": {"window": "In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n", "original_text": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n"}, "hash": "46ae346a86b4a22c189b12316b4d5b3b13c13c48962b5c28ba4a306af95c5bd5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n", "mimetype": "text/plain", "start_char_idx": 23705, "end_char_idx": 23844, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f11db3d-42c5-4d5e-92bf-ad8263f2762f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n", "original_text": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb2277c1-7239-4637-84e2-8b89d6d66a2a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Branch cuts generated by the standard Isolation Forest during the training phase.  In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities.", "original_text": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n"}, "hash": "87d2724fe6dd49a2b1855d0100f6a270131191776b45b51365969994f425b0ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "871ea619-c3c0-4104-ac2f-f3593776d7ac", "node_type": "1", "metadata": {"window": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n", "original_text": "**Fig. "}, "hash": "a9613f36749f289718f0bc0aaa46432ebee203c89fd69cac5641c3324da1d4d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n", "mimetype": "text/plain", "start_char_idx": 23844, "end_char_idx": 23979, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "871ea619-c3c0-4104-ac2f-f3593776d7ac", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f11db3d-42c5-4d5e-92bf-ad8263f2762f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In each step, a random value is selected from a random feature (dimension).  Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n", "original_text": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n"}, "hash": "60999db03c31caf5922b5f9861ee9383e02ba45e3dca09919b9846f2065f261d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e878ad7-3188-4d21-a20b-53f12d7727a7", "node_type": "1", "metadata": {"window": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n", "original_text": "7. "}, "hash": "c8829fadda60807f17b8b8edf192d764aacf29189a611324f99115bf999a4228", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 23979, "end_char_idx": 23986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e878ad7-3188-4d21-a20b-53f12d7727a7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n", "original_text": "7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "871ea619-c3c0-4104-ac2f-f3593776d7ac", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Then the training data points are determined to go down the left or right branches of the tree based on what side of that line they reside. **\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n", "original_text": "**Fig. "}, "hash": "85ddfd46491e3ff5574396429c87f7671d2d26d7070f912240a62646f6ba0eb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e7c59fd-90dd-46af-b496-532a786387bd", "node_type": "1", "metadata": {"window": "The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. ", "original_text": "A typical distribution of the only possible branch cuts. "}, "hash": "7764862dcaeddbe2407e36804b8b5d6b404ecde60163ebd2b12ec16afc87e584", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7. ", "mimetype": "text/plain", "start_char_idx": 23986, "end_char_idx": 23989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e7c59fd-90dd-46af-b496-532a786387bd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. ", "original_text": "A typical distribution of the only possible branch cuts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e878ad7-3188-4d21-a20b-53f12d7727a7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization of the data space containing many horizontal and vertical lines, which are the branch cuts.  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n", "original_text": "7. "}, "hash": "825f3f3748b70fb9070390c9f77f29e9ef52623afddc0dc08cc7c020aa31dfaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24c4e300-1f93-4455-9b9d-281cd98dbdee", "node_type": "1", "metadata": {"window": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. ", "original_text": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities."}, "hash": "7573f9ac42be933db1fad9a0f4ac4f4cfc8daf9d44ab6f95cc8fa8c954392b70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A typical distribution of the only possible branch cuts. ", "mimetype": "text/plain", "start_char_idx": 23989, "end_char_idx": 24046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24c4e300-1f93-4455-9b9d-281cd98dbdee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. ", "original_text": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e7c59fd-90dd-46af-b496-532a786387bd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. ", "original_text": "A typical distribution of the only possible branch cuts. "}, "hash": "92eecf0d2af7704c82c70f983fbbfc5d69aabaed7764aa400a5e51c6ce29e904", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e04b121-f021-4ac7-a914-f33e5647c194", "node_type": "1", "metadata": {"window": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. ", "original_text": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n"}, "hash": "bd69626ad806af97d85f934d72966120b943f93c0d258a14d19e03317c512118", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities.", "mimetype": "text/plain", "start_char_idx": 24046, "end_char_idx": 24255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e04b121-f021-4ac7-a914-f33e5647c194", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. ", "original_text": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24c4e300-1f93-4455-9b9d-281cd98dbdee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing many horizontal and vertical branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. ", "original_text": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities."}, "hash": "cf0b12061d45d6bdb1c3ed4e8735cec1f9fe76828137761360737f748ffb44a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "859fc47e-d761-40eb-9e50-635ffc3443c2", "node_type": "1", "metadata": {"window": "**Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. ", "original_text": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n"}, "hash": "bdd75489ab9d1cb442835c7e360e72953e71255a4a1b54830c465d5cf4f63689", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n", "mimetype": "text/plain", "start_char_idx": 24255, "end_char_idx": 24393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "859fc47e-d761-40eb-9e50-635ffc3443c2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. ", "original_text": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e04b121-f021-4ac7-a914-f33e5647c194", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoidal: A visualization showing many horizontal and vertical branch cuts, concentrated along the path of the sinusoidal data.\n\n **Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. ", "original_text": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n"}, "hash": "0ad6365e33a45dbfee8a348f85aa9c85554433ec5413943f6e1a4996b87c6b02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8629d3d0-e1d0-4fa2-9f02-da9a5cebb173", "node_type": "1", "metadata": {"window": "7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n", "original_text": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n"}, "hash": "829e6ea58fc94cfe044221e524e13a4f76844cf0c1c7556df3c958eaf1773cb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n", "mimetype": "text/plain", "start_char_idx": 24393, "end_char_idx": 24533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8629d3d0-e1d0-4fa2-9f02-da9a5cebb173", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n", "original_text": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "859fc47e-d761-40eb-9e50-635ffc3443c2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. ", "original_text": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n"}, "hash": "bec02938cccd7f575037187ee4bbb048925ab27690baa81006616663c2204fe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82857b62-a458-4ae0-b1ed-f6f261d9314b", "node_type": "1", "metadata": {"window": "A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees. ", "original_text": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. "}, "hash": "a89639b410136535a7e7e6671d1f9416d68ca8f4037eebb7b63511e4348b8bcf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n", "mimetype": "text/plain", "start_char_idx": 24533, "end_char_idx": 24666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82857b62-a458-4ae0-b1ed-f6f261d9314b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees. ", "original_text": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8629d3d0-e1d0-4fa2-9f02-da9a5cebb173", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7.  A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n", "original_text": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n"}, "hash": "8eff78796fc24729b82e5feea9779fbb43d0819cade0d9ccdf3ef919d32690a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ab8ab65-92ff-44d2-a0de-8ed81aed5c9e", "node_type": "1", "metadata": {"window": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias. ", "original_text": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. "}, "hash": "5d4b274e98f4609d68a45577b3208f82b26fa4459fafec0e2cff9e6558a7cc67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. ", "mimetype": "text/plain", "start_char_idx": 24666, "end_char_idx": 24832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ab8ab65-92ff-44d2-a0de-8ed81aed5c9e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias. ", "original_text": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82857b62-a458-4ae0-b1ed-f6f261d9314b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A typical distribution of the only possible branch cuts.  The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees. ", "original_text": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing. "}, "hash": "462e4207f3be2eb1df83d5b4052fde45e414d0b0ede3e1882ff985727b8d9568", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45f9f776-ba1b-4a44-8e7f-20391bcf89b8", "node_type": "1", "metadata": {"window": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. ", "original_text": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. "}, "hash": "b3b701bb91c8fe7c017256e57347e72944a8f0373d801ec4c98a4299ec1bc49d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. ", "mimetype": "text/plain", "start_char_idx": 24832, "end_char_idx": 25080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45f9f776-ba1b-4a44-8e7f-20391bcf89b8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. ", "original_text": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ab8ab65-92ff-44d2-a0de-8ed81aed5c9e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The biased treatment of various regions in the domain of the data accumulate as many trees are generated, and as a result, regions of similar anomaly scores are subject to very different scoring possibilities. **\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias. ", "original_text": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle. "}, "hash": "af0d7ca2dbf01ed8baf92836443d9e43b9046501e2be602ea7f11edb699bd093", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4a7cfe2-e879-46b2-855c-2670c74ada66", "node_type": "1", "metadata": {"window": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. ", "original_text": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. "}, "hash": "a3d2d4d19f337aecb87e5e0404bffb3875a073ca0bdd866276d7ed233fd2cf6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. ", "mimetype": "text/plain", "start_char_idx": 25080, "end_char_idx": 25215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4a7cfe2-e879-46b2-855c-2670c74ada66", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. ", "original_text": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45f9f776-ba1b-4a44-8e7f-20391bcf89b8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization showing a high density of horizontal and vertical lines concentrated at the center of the data space.\n (b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. ", "original_text": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact. "}, "hash": "3eda03530cf7f7798d47a7a4a18da60cea96fcb057c27546a4634008c4f317f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d5da937-3e4f-4cee-a864-4e15acdc0dc6", "node_type": "1", "metadata": {"window": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n", "original_text": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n"}, "hash": "a2c6a574e50ec4312fb0acde19e94824599bf3c82fe3e431ddce7baaacb6e963", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. ", "mimetype": "text/plain", "start_char_idx": 25215, "end_char_idx": 25415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d5da937-3e4f-4cee-a864-4e15acdc0dc6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n", "original_text": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4a7cfe2-e879-46b2-855c-2670c74ada66", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing a high density of horizontal and vertical lines, concentrated around the two cluster locations.\n (c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. ", "original_text": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches. "}, "hash": "14521abd3306332330a9998edcc20541b9a35c041ef94298c16b43ab294f52c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c49445e4-0bf6-46f2-929a-a1c290951801", "node_type": "1", "metadata": {"window": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. ", "original_text": "With this method, in each case, the same bias exists as before, but only for single trees. "}, "hash": "d6dca0420108b3e55d830b9b2c473995c271229611f472f3569d475676ed1cd5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n", "mimetype": "text/plain", "start_char_idx": 25415, "end_char_idx": 25517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c49445e4-0bf6-46f2-929a-a1c290951801", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. ", "original_text": "With this method, in each case, the same bias exists as before, but only for single trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d5da937-3e4f-4cee-a864-4e15acdc0dc6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoidal: A visualization showing a high density of horizontal and vertical lines concentrated along the sinusoidal data path.\n ***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n", "original_text": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n"}, "hash": "394f52f0e10e33a7488027fb8f1cc24c4b5ee6ad6d0187db0a34af4d352408e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b745c99e-e072-453a-be53-d44841d2e880", "node_type": "1", "metadata": {"window": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n", "original_text": "Each tree carries with itself a different bias. "}, "hash": "f6a253acc63054070f462285a7c61375f6108e731ac889996d7e2f0d12a2a155", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With this method, in each case, the same bias exists as before, but only for single trees. ", "mimetype": "text/plain", "start_char_idx": 25517, "end_char_idx": 25608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b745c99e-e072-453a-be53-d44841d2e880", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n", "original_text": "Each tree carries with itself a different bias. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c49445e4-0bf6-46f2-929a-a1c290951801", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\n### 3.2 Rotated Trees\nBefore we present the completely general case, we discuss rotation of the data in order to minimize the effect of the rectangular slicing.  In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. ", "original_text": "With this method, in each case, the same bias exists as before, but only for single trees. "}, "hash": "690718d409fbff89aee022d8563a1b806490a35d3d7227fc8f3f8a4c9743c9c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90c55e7d-7993-4c7e-b995-68af7c27b8f7", "node_type": "1", "metadata": {"window": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n", "original_text": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. "}, "hash": "f9b7b1bf43653629716baef5d1d93f3a88018b0d7a473d74be1e79757a08e051", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each tree carries with itself a different bias. ", "mimetype": "text/plain", "start_char_idx": 25608, "end_char_idx": 25656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90c55e7d-7993-4c7e-b995-68af7c27b8f7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n", "original_text": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b745c99e-e072-453a-be53-d44841d2e880", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this approach the construction of each individual tree takes place using the standard Isolation Forest, but before that happens, the sub-sample of the data that is used to construct each one of the trees, is rotated in plane, by a random angle.  As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n", "original_text": "Each tree carries with itself a different bias. "}, "hash": "1732dbbd87a496ec83979ddc80957f007ad00ec3c25b963181b60b21a3c8c2e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1224dc3c-c48e-4ed2-bf23-3a44dc78e799", "node_type": "1", "metadata": {"window": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n", "original_text": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. "}, "hash": "0b8d395cf37625737820ac442c395a200dcdad3b95c5f33ad04df51818b17b8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. ", "mimetype": "text/plain", "start_char_idx": 25656, "end_char_idx": 25792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1224dc3c-c48e-4ed2-bf23-3a44dc78e799", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n", "original_text": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90c55e7d-7993-4c7e-b995-68af7c27b8f7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As such, each tree is \u201ctwisted\u201d in a unique way, differently than all the other trees, while keeping the structure of the data intact.  Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n", "original_text": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness. "}, "hash": "818a89890774602ad2fb8738a0365b5b6654aab77e9b11a39b1d6585a0888c3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd72ba87-1390-4d7f-9675-affd660d6d11", "node_type": "1", "metadata": {"window": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D. ", "original_text": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n"}, "hash": "8d6061ea3825e89b34736f1f190c98ab19dfaaaec43bc215bb5032a3cf8dc8d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. ", "mimetype": "text/plain", "start_char_idx": 25792, "end_char_idx": 25927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd72ba87-1390-4d7f-9675-affd660d6d11", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D. ", "original_text": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1224dc3c-c48e-4ed2-bf23-3a44dc78e799", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Once the training is complete, and a we are computing the scores of the data points, the same transformation has to take place for each tree before the point is allowed to run down the tree branches.  This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n", "original_text": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results. "}, "hash": "8a6be2de80cb99b72d76c6c782e26f8a1981596093a2419ade62474622aeefd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd52a4e7-7630-465e-95f3-1d4f098d160c", "node_type": "1", "metadata": {"window": "With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n", "original_text": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. "}, "hash": "c1c0c076146dbd743e62528c623a0f7cd884959d71956c799ea34f8e5d90ef7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n", "mimetype": "text/plain", "start_char_idx": 25927, "end_char_idx": 26158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd52a4e7-7630-465e-95f3-1d4f098d160c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n", "original_text": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd72ba87-1390-4d7f-9675-affd660d6d11", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This approach does indeed improve the performance quite a bit as we will see in the results section.\n\n With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D. ", "original_text": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n"}, "hash": "bc420e94d44912c3f8ec8c965f31da245f1a40dc52589733d0bae90adac6089b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee014ae3-fdb4-411d-916c-2d18bad25699", "node_type": "1", "metadata": {"window": "Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n", "original_text": "In a sense the problem is not resolved, but only averaged out.\n"}, "hash": "73487de60fd950d4ab1a5fcb22cee63e206c48d50982f63f6573e16b733ccd3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. ", "mimetype": "text/plain", "start_char_idx": 26158, "end_char_idx": 26296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ee014ae3-fdb4-411d-916c-2d18bad25699", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n", "original_text": "In a sense the problem is not resolved, but only averaged out.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd52a4e7-7630-465e-95f3-1d4f098d160c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "With this method, in each case, the same bias exists as before, but only for single trees.  Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n", "original_text": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm. "}, "hash": "b5c521c026f864e3695b2d6703306c372d91b0a428135b91802205a78ccf25b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "479c9862-8cf2-4572-8e6c-3ac143719038", "node_type": "1", "metadata": {"window": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. ", "original_text": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n"}, "hash": "a2543160d736a7f45788aa63b8d461dd9afb7874414814e500837cdeedfa2ed1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In a sense the problem is not resolved, but only averaged out.\n", "mimetype": "text/plain", "start_char_idx": 26296, "end_char_idx": 26359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "479c9862-8cf2-4572-8e6c-3ac143719038", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. ", "original_text": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee014ae3-fdb4-411d-916c-2d18bad25699", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Each tree carries with itself a different bias.  When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n", "original_text": "In a sense the problem is not resolved, but only averaged out.\n"}, "hash": "3fe16898ee5d1089707f836c49458a9ed57c8cbe7588f2d5c1b327c3ccbcb9db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fa10fb8-50cf-4343-a042-3bb4d185503a", "node_type": "1", "metadata": {"window": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n", "original_text": "4)  It is prone to simple errors if datasets lack symmetries.\n"}, "hash": "347563ba8c477b5ae3408266fc23b32948957acf74cec8b74b646a45b53702fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n", "mimetype": "text/plain", "start_char_idx": 26359, "end_char_idx": 26462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fa10fb8-50cf-4343-a042-3bb4d185503a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n", "original_text": "4)  It is prone to simple errors if datasets lack symmetries.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "479c9862-8cf2-4572-8e6c-3ac143719038", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "When the aggregate score is computed, the total sum of the biases is averaged out resulting in a large improvement in score robustness.  However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. ", "original_text": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n"}, "hash": "68397b61a55c073537bd002f3995dd79d1aba65c2e7b80588236f0a6861524f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c95fe94-1a79-4925-9a8a-1a7eb5c88d3d", "node_type": "1", "metadata": {"window": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. ", "original_text": "5)  The rotation is not obvious in higher dimensions than 2-D. "}, "hash": "2680d318de81f09d4c99af717433308e73f950845d2830daa02fc87abebbce53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4)  It is prone to simple errors if datasets lack symmetries.\n", "mimetype": "text/plain", "start_char_idx": 26462, "end_char_idx": 26524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c95fe94-1a79-4925-9a8a-1a7eb5c88d3d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. ", "original_text": "5)  The rotation is not obvious in higher dimensions than 2-D. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fa10fb8-50cf-4343-a042-3bb4d185503a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, this is not the ideal solution for fixing problems presented above, despite the fact that it seems to produce better results.  Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n", "original_text": "4)  It is prone to simple errors if datasets lack symmetries.\n"}, "hash": "ac50b78736e6527bf4939acfebd145a67fa30f3a7349631389df29d3ae33536f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80b113a2-312c-477e-8785-39437be17de2", "node_type": "1", "metadata": {"window": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. ", "original_text": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n"}, "hash": "a59ca7656c0fe86ad77189566db11aed8f6902f1fa35b2d7c3ca6a511071fb99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5)  The rotation is not obvious in higher dimensions than 2-D. ", "mimetype": "text/plain", "start_char_idx": 26524, "end_char_idx": 26587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80b113a2-312c-477e-8785-39437be17de2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. ", "original_text": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c95fe94-1a79-4925-9a8a-1a7eb5c88d3d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Some reasons as to why this method is less desirable are:\n\n1)  Each tree has to be tagged with its unique rotation so that when we are scoring observed data, we can compensate for the rotation in the coordinates of the data point.\n 2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. ", "original_text": "5)  The rotation is not obvious in higher dimensions than 2-D. "}, "hash": "9345ef7b315b33e574648f3f5b9b820c02a6a43bb0b2d800db2023b256fb76cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "391832ea-2239-421b-aa8f-a98e0c46bde6", "node_type": "1", "metadata": {"window": "In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. ", "original_text": "6)  There is extra bookkeeping and meta data store for each tree.\n\n"}, "hash": "1c0a55f4f401d8d5f4fb35e4c1edef926d75d4119c00649c96af3ebac43cee2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n", "mimetype": "text/plain", "start_char_idx": 26587, "end_char_idx": 26797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "391832ea-2239-421b-aa8f-a98e0c46bde6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. ", "original_text": "6)  There is extra bookkeeping and meta data store for each tree.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80b113a2-312c-477e-8785-39437be17de2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2)  Even though the ensemble results seem good, each tree still suffers from the rectangular bias introduced by the underlying algorithm.  In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. ", "original_text": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n"}, "hash": "be4e04c8d259e73ca2dda952efde8f7999bd85b8207f0b2413b1121cec2c5593", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e452bc9-c6d4-4ec5-9126-682254ff33f5", "node_type": "1", "metadata": {"window": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. ", "original_text": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. "}, "hash": "071c626c6d231c32e44fe69372b039aab5f796aa23361466c36c06107d67f678", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6)  There is extra bookkeeping and meta data store for each tree.\n\n", "mimetype": "text/plain", "start_char_idx": 26797, "end_char_idx": 26864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e452bc9-c6d4-4ec5-9126-682254ff33f5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. ", "original_text": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "391832ea-2239-421b-aa8f-a98e0c46bde6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In a sense the problem is not resolved, but only averaged out.\n 3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. ", "original_text": "6)  There is extra bookkeeping and meta data store for each tree.\n\n"}, "hash": "d6ae452546ef43af2e390551dc41465ab968b087bd7d6aaac50eb49158b0f4ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daa8a083-1289-4d26-a75d-57e4d8633fb1", "node_type": "1", "metadata": {"window": "4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. ", "original_text": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n"}, "hash": "0645bdc9cab31e13f809f9b19b53d2adec7415f5b94985a4e6c0923735f651bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. ", "mimetype": "text/plain", "start_char_idx": 26864, "end_char_idx": 27012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "daa8a083-1289-4d26-a75d-57e4d8633fb1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. ", "original_text": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e452bc9-c6d4-4ec5-9126-682254ff33f5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3)  This approach can become cumbersome to apply especially with large datasets and higher dimensions.\n 4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. ", "original_text": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place. "}, "hash": "2b9b4e55d174bf69804a57cb0e501ffe9a1a321df592476e2007300127cbf9f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab9c03fa-da5b-4efa-a735-b3364c422070", "node_type": "1", "metadata": {"window": "5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig. ", "original_text": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. "}, "hash": "6525d963e7fe36c051b3312b0e23c28df0989a0510d6e83b1ecd13951f8f9284", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n", "mimetype": "text/plain", "start_char_idx": 27012, "end_char_idx": 27183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab9c03fa-da5b-4efa-a735-b3364c422070", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig. ", "original_text": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "daa8a083-1289-4d26-a75d-57e4d8633fb1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4)  It is prone to simple errors if datasets lack symmetries.\n 5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. ", "original_text": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n"}, "hash": "ea187dbd0cafee12f88f234b3424d403de2c7bd72892a70dc9970cdfea74cb2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20acb043-3c06-417d-9f51-30df1bdb7064", "node_type": "1", "metadata": {"window": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n", "original_text": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. "}, "hash": "1d740f481d1ff6e14a618b22b26bf8c75fe421918ccb072142a6e00ee6d91397", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. ", "mimetype": "text/plain", "start_char_idx": 27183, "end_char_idx": 27292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20acb043-3c06-417d-9f51-30df1bdb7064", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n", "original_text": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab9c03fa-da5b-4efa-a735-b3364c422070", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5)  The rotation is not obvious in higher dimensions than 2-D.  For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig. ", "original_text": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values. "}, "hash": "d1a65ecab4ab412d823daa32d9e5751259fbe1a61160192e2e021fedc44ff152", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "030a17b9-bd5b-4d19-954f-5c496ce1c520", "node_type": "1", "metadata": {"window": "6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. ", "original_text": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. "}, "hash": "f9f76dc0a503141c07289567d5055dc88e4c9d9066e5de644d71f5eeea36ad8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. ", "mimetype": "text/plain", "start_char_idx": 27292, "end_char_idx": 27404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "030a17b9-bd5b-4d19-954f-5c496ce1c520", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. ", "original_text": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20acb043-3c06-417d-9f51-30df1bdb7064", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For each tree we can pick a random axis in the space and perform planar rotation around that axis, but there are many other choices that can be made, which might result in inconsistencies among different runs.\n 6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n", "original_text": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections. "}, "hash": "7bc38f67b17cd04c3fbf6f3729a533e8ec13ac7d7853de64a212bb4fae9466aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e5d0b0b-5a2e-4f0f-9002-4cc620af2d9f", "node_type": "1", "metadata": {"window": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. ", "original_text": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. "}, "hash": "33557db91f9d4077022f7e5247eb49df6e2426840c0dc1b9ef25e8d9b1d6d934", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. ", "mimetype": "text/plain", "start_char_idx": 27404, "end_char_idx": 27550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6e5d0b0b-5a2e-4f0f-9002-4cc620af2d9f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. ", "original_text": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "030a17b9-bd5b-4d19-954f-5c496ce1c520", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6)  There is extra bookkeeping and meta data store for each tree.\n\n A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. ", "original_text": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map. "}, "hash": "36ab910e5ac64103ed2ddd79185beb9bd8a727bc1538da2fa92baeceda0d7518", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de130102-5c58-4161-bc74-57cbabe49dc5", "node_type": "1", "metadata": {"window": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. ", "original_text": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. "}, "hash": "5961182b34c90d24f7fc1df22f76d3b757cb1c53da22c4e47fa1a77eebd48c60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. ", "mimetype": "text/plain", "start_char_idx": 27550, "end_char_idx": 27716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de130102-5c58-4161-bc74-57cbabe49dc5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. ", "original_text": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e5d0b0b-5a2e-4f0f-9002-4cc620af2d9f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A much more robust fix to the problem can be achieved by truly randomizing the branching process in addition to the randomization already in place.  That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. ", "original_text": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d. "}, "hash": "dc615b81feb91ed0787a38942d4f9118f84c7c26b92273c479deabcd0b9a78f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef475996-1b27-4d5c-b067-e4389d854f3f", "node_type": "1", "metadata": {"window": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n", "original_text": "8 for both cases of anomaly and nominal data points, analogous to Fig. "}, "hash": "a03d7ddb44faf271ccd1e9f16869890f98a620444cb4b1bbfd087dc9733b8746", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. ", "mimetype": "text/plain", "start_char_idx": 27716, "end_char_idx": 27819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef475996-1b27-4d5c-b067-e4389d854f3f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n", "original_text": "8 for both cases of anomaly and nominal data points, analogous to Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de130102-5c58-4161-bc74-57cbabe49dc5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "That is, instead of simply running the Isolation Forest for each tree using a rotated sub-sample, we select different angles for the data slices at each branching point.\n\n ### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. ", "original_text": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig. "}, "hash": "8ff2bc86158878e49e6cbe53337dfe4abf79949dcbc84586fb9780b1aecbc7e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d81b154b-e346-4466-bc0e-b871453fb03e", "node_type": "1", "metadata": {"window": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. ", "original_text": "5.\n\n"}, "hash": "b363d1c68be7e397d3ec264116daba0497939c95af9a243c93dcb0e2c564ea6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 for both cases of anomaly and nominal data points, analogous to Fig. ", "mimetype": "text/plain", "start_char_idx": 27819, "end_char_idx": 27890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d81b154b-e346-4466-bc0e-b871453fb03e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. ", "original_text": "5.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef475996-1b27-4d5c-b067-e4389d854f3f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 3.3 Extended Isolation Forest\nIsolation Forest relies on randomness in selection of features and values.  Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n", "original_text": "8 for both cases of anomaly and nominal data points, analogous to Fig. "}, "hash": "ea201bd32d1adab41a0b27676aabef9ac1295bd3df437bf12a812aec2c07bbc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1955d4b-6a61-4adc-987c-c11fc9758875", "node_type": "1", "metadata": {"window": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. ", "original_text": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. "}, "hash": "049582ed76279ce35ad83067eaa8611f1a35554a70668b29ee603dbf67d1d8a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.\n\n", "mimetype": "text/plain", "start_char_idx": 27890, "end_char_idx": 27894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1955d4b-6a61-4adc-987c-c11fc9758875", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. ", "original_text": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d81b154b-e346-4466-bc0e-b871453fb03e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since anomalous points are \u201cfew and different\u201d, they quickly stand out with respect to these random selections.  But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. ", "original_text": "5.\n\n"}, "hash": "c12fc3b31c19cb0398e56f993b193cdda1e19541216424d9735e3ca6131434b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95e281bf-3543-4387-a1f9-3ac54aacb2ec", "node_type": "1", "metadata": {"window": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere. ", "original_text": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. "}, "hash": "c565ac32d411548fa2ee5024e9a5b02b48423719c5ea1045ba9425492f4eacd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. ", "mimetype": "text/plain", "start_char_idx": 27894, "end_char_idx": 28056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95e281bf-3543-4387-a1f9-3ac54aacb2ec", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere. ", "original_text": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1955d4b-6a61-4adc-987c-c11fc9758875", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "But as we have seen, the branch cuts are always either horizontal or vertical, and this introduces a bias and artifacts in the anomaly score map.  There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. ", "original_text": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated. "}, "hash": "75093ce7716198d982ea529d89d25d4c10a2807d8bf77c410148c80819e6be1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9486d5e-c222-40b5-a9be-889511f4798a", "node_type": "1", "metadata": {"window": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. ", "original_text": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. "}, "hash": "b675ce7f101f145067deb6594b15e825a88886f37fa1639f39fefff533819b5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. ", "mimetype": "text/plain", "start_char_idx": 28056, "end_char_idx": 28302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9486d5e-c222-40b5-a9be-889511f4798a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. ", "original_text": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95e281bf-3543-4387-a1f9-3ac54aacb2ec", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "There is no fundamental reason in the algorithm that requires this restriction, and so at each branching point, we can select a branch cut that has a random \u201cslope\u201d.  A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere. ", "original_text": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data. "}, "hash": "c8c8d58a90240c80536a08ad1ee639f8d14386ac7d36a0541b308a841f08375c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48bc9da2-3e3a-4873-871f-7e04b33684b2", "node_type": "1", "metadata": {"window": "8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. ", "original_text": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n"}, "hash": "a504854ff79c2930915202bc96349a520b5b9d5713a87e9ceafe6443ee04a18c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. ", "mimetype": "text/plain", "start_char_idx": 28302, "end_char_idx": 28575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48bc9da2-3e3a-4873-871f-7e04b33684b2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. ", "original_text": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9486d5e-c222-40b5-a9be-889511f4798a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A single branching process for branch cuts with random slopes in our 2-D example is visualized in Fig.  8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. ", "original_text": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data. "}, "hash": "6d95c0d58dcb67708e596da97b4f9747c20d3e2ffd3347f41ad59b4877ed567d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9832bf8-d779-4503-8e56-8e36a86d1d74", "node_type": "1", "metadata": {"window": "5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig. ", "original_text": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. "}, "hash": "52ec2d3d790a43459061b38769c054ef51c058d8f95cb6162506a58aa3c9adfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n", "mimetype": "text/plain", "start_char_idx": 28575, "end_char_idx": 28851, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a9832bf8-d779-4503-8e56-8e36a86d1d74", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig. ", "original_text": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48bc9da2-3e3a-4873-871f-7e04b33684b2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8 for both cases of anomaly and nominal data points, analogous to Fig.  5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. ", "original_text": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n"}, "hash": "a47a592a72f5d91d9c45f3e76e0581d4ef5a39ed4a7791135fb811850c64e16d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb3a9e36-b6e0-4e8c-92a4-718d0a439f68", "node_type": "1", "metadata": {"window": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. ", "original_text": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. "}, "hash": "64d33d224ef21f71961ca5d7c083cf94f9b26a3760e7217e7fcb79c72f105272", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. ", "mimetype": "text/plain", "start_char_idx": 28851, "end_char_idx": 29011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb3a9e36-b6e0-4e8c-92a4-718d0a439f68", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. ", "original_text": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9832bf8-d779-4503-8e56-8e36a86d1d74", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5.\n\n Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig. ", "original_text": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere. "}, "hash": "60f8361df55f611178aa8429c6bb1123321b753e799404f9d050a73adbd97c3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d3d6988-aad0-48b5-bc1b-a460b07b5bba", "node_type": "1", "metadata": {"window": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig. ", "original_text": "This results in a uniform selection of points on the N-sphere. "}, "hash": "ac6c945e11f09ec665bac8e4fcc92312fd91f542f8fcd6be771b6c366786f17c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. ", "mimetype": "text/plain", "start_char_idx": 29011, "end_char_idx": 29154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d3d6988-aad0-48b5-bc1b-a460b07b5bba", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig. ", "original_text": "This results in a uniform selection of points on the N-sphere. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb3a9e36-b6e0-4e8c-92a4-718d0a439f68", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Similarly to the standard Isolation Forest algorithm, anomalies can be isolated very quickly, whereas the nominal points require many branch cuts to be isolated.  In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. ", "original_text": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8]. "}, "hash": "2bda072b0dea93a121d293c4152fe6163e71e360cecde42413b14ed3c021b657", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bb7c0b9-8bc7-4629-a0a2-46c9448f5afb", "node_type": "1", "metadata": {"window": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig. ", "original_text": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. "}, "hash": "15ad00bc4c5f601ada3cdb203d21c3297466a370ca8a5d25aa1d6c0b9b68632b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This results in a uniform selection of points on the N-sphere. ", "mimetype": "text/plain", "start_char_idx": 29154, "end_char_idx": 29217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bb7c0b9-8bc7-4629-a0a2-46c9448f5afb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig. ", "original_text": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d3d6988-aad0-48b5-bc1b-a460b07b5bba", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of the standard Isolation Forest algorithm, the selection of the branch cuts requires two pieces of information: 1) a random feature or coordinate, and 2) a random value for the feature from the range of available values in the data.  For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig. ", "original_text": "This results in a uniform selection of points on the N-sphere. "}, "hash": "07dd6d858bd8785ab4e86fdbdc4c968ef7e4c7680756269b7308334badbaa64f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e0d4fb-d469-4099-aefc-85010076a6b4", "node_type": "1", "metadata": {"window": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6. ", "original_text": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. "}, "hash": "351b141fd94ddc60749e80b396e93a72ab3d10a6de74105cb96bb76db39ee5f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. ", "mimetype": "text/plain", "start_char_idx": 29217, "end_char_idx": 29348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99e0d4fb-d469-4099-aefc-85010076a6b4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6. ", "original_text": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bb7c0b9-8bc7-4629-a0a2-46c9448f5afb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For the extended case, the selection of the branch cuts still only requires two pieces of information, but they are: 1) a random slope for the branch cut, and 2) a random intercept for the branch cut which is chosen from the range of available values of the training data.  Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig. ", "original_text": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point. "}, "hash": "f5691cccdb4def7de133a4d6669b8e28b8afa86af68405fd34932af0077b36f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "895b485f-f7e3-4933-b986-a386a17ea71d", "node_type": "1", "metadata": {"window": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree. ", "original_text": "As we saw in Fig. "}, "hash": "d87c300647be8aebcb3bbf6f5a07ed8771c47ed03ad50bed6c5008182c6f3ca4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. ", "mimetype": "text/plain", "start_char_idx": 29348, "end_char_idx": 29670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "895b485f-f7e3-4933-b986-a386a17ea71d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree. ", "original_text": "As we saw in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99e0d4fb-d469-4099-aefc-85010076a6b4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Notice that this is simpler than the tree rotations in the previous section, because in that case in addition to the two pieces of information at each branch cut, each tree needs to store the information for transforming the data so that it can be used in the scoring stage.\n\n For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6. ", "original_text": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch. "}, "hash": "f577a156bd4fd6be1caae0c99dba5c3599c87bcb2a090c58e89bb5f63f2b2fb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e31b430b-66ec-47c9-9fc7-bb6a03276faf", "node_type": "1", "metadata": {"window": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. ", "original_text": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. "}, "hash": "678f71c8aa1762f4a8d06a63e75661c682a3cc4a8ea82d8ae5bdc3835dfd873b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we saw in Fig. ", "mimetype": "text/plain", "start_char_idx": 29670, "end_char_idx": 29688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e31b430b-66ec-47c9-9fc7-bb6a03276faf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. ", "original_text": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "895b485f-f7e3-4933-b986-a386a17ea71d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For an N dimensional dataset, selecting a random slope for the branch cut is the same as choosing a normal vector, $\\vec{n}$, uniformly over the unit N-Sphere.  This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree. ", "original_text": "As we saw in Fig. "}, "hash": "9836ae7ec2ae50a49567c57073149e0635a4e06e89cb6804703eb56539a87f03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "947c6c8f-0f9d-4ed9-81fa-ae1d8fe440d3", "node_type": "1", "metadata": {"window": "This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n", "original_text": "Fig. "}, "hash": "a243bac602d2d0dfe6a9247735bad6e8dd561da9a39c80cc28ee9f60a9289007", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. ", "mimetype": "text/plain", "start_char_idx": 29688, "end_char_idx": 29809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "947c6c8f-0f9d-4ed9-81fa-ae1d8fe440d3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e31b430b-66ec-47c9-9fc7-bb6a03276faf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This can easily be accomplished by drawing a random number for each coordinate of $\\vec{n}$ from the standard normal distribution N(0, 1) [8].  This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. ", "original_text": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached. "}, "hash": "7795bd939385878c3ee98244211274e4cb52c58a79f1d233c7b54478492cf284", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dc53d98-78e3-4f54-bb41-9eecfe1b9fed", "node_type": "1", "metadata": {"window": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig. ", "original_text": "9 is analogous to Fig. "}, "hash": "b6202c46f1cc33264664a91b8e75f6f62f35e702dbd6f7d9b41b1f17ef63990e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 29809, "end_char_idx": 29814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3dc53d98-78e3-4f54-bb41-9eecfe1b9fed", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig. ", "original_text": "9 is analogous to Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "947c6c8f-0f9d-4ed9-81fa-ae1d8fe440d3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This results in a uniform selection of points on the N-sphere.  For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n", "original_text": "Fig. "}, "hash": "8bb21f8c3cb4174bab9e6890272b38bf23a3638dda07e81ca5df6567d4b405b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72c9407-b81d-474f-87d7-906647850b60", "node_type": "1", "metadata": {"window": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8. ", "original_text": "6. "}, "hash": "b2f85acfd408eabc5c49a63b1d3a2fd466c752bd7e1ae89f7f0bc32933897528", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 is analogous to Fig. ", "mimetype": "text/plain", "start_char_idx": 29814, "end_char_idx": 29837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b72c9407-b81d-474f-87d7-906647850b60", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8. ", "original_text": "6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dc53d98-78e3-4f54-bb41-9eecfe1b9fed", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For the intercept, $\\vec{p}$, we simply draw from a uniform distribution over the range of values present at each branching point.  Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig. ", "original_text": "9 is analogous to Fig. "}, "hash": "18ae58d113eaf21977842294eb514847ef62427eb4158bd74aaa1807de3683e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2902bc16-7dd0-49e0-a563-6675ed01e86c", "node_type": "1", "metadata": {"window": "As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest. ", "original_text": "The branch cuts are shown for training of a single tree. "}, "hash": "369375fbbf265dcef5889761cc1abbdb17fffe30e62f570310880e0b80ed36c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. ", "mimetype": "text/plain", "start_char_idx": 29837, "end_char_idx": 29840, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2902bc16-7dd0-49e0-a563-6675ed01e86c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest. ", "original_text": "The branch cuts are shown for training of a single tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b72c9407-b81d-474f-87d7-906647850b60", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Once these two pieces of information are determined, the branching criteria for the data splitting for a given point $\\vec{x}$ is as follows:\n\n$(\\vec{x} \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0$, (3)\n\nIf the condition is satisfied, the data point $\\vec{x}$ is passed to the left branch, otherwise it moves down to the right branch.  As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8. ", "original_text": "6. "}, "hash": "a2acb036136b70a6c4d1b50f1875821647513a1d0ecad1b705cf404e8b030ed3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17bc7895-3354-4181-85bb-e8758345e45f", "node_type": "1", "metadata": {"window": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig. ", "original_text": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. "}, "hash": "59c6d5aa109c03c8de9ac2cd93d7dc53e4b87df120ea98c9be98525e411925ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The branch cuts are shown for training of a single tree. ", "mimetype": "text/plain", "start_char_idx": 29840, "end_char_idx": 29897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17bc7895-3354-4181-85bb-e8758345e45f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig. ", "original_text": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2902bc16-7dd0-49e0-a563-6675ed01e86c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we saw in Fig.  8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest. ", "original_text": "The branch cuts are shown for training of a single tree. "}, "hash": "30a53ffce61ea79d25bbced5e25e23dc6a20432e177ef6fbd2dbaf80a6be22a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9def6c5-e989-4089-9fb4-49ec56fc320d", "node_type": "1", "metadata": {"window": "Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point. ", "original_text": "6.\n\n"}, "hash": "33cdd270e72df21da409f78cf9798be0caae650c0a6b5a511c5068d586d7de26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. ", "mimetype": "text/plain", "start_char_idx": 29897, "end_char_idx": 30032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9def6c5-e989-4089-9fb4-49ec56fc320d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point. ", "original_text": "6.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17bc7895-3354-4181-85bb-e8758345e45f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8, the branch cuts are drawn with random slopes until they isolate the data points, or until the depth limit is reached.  Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig. ", "original_text": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig. "}, "hash": "29d21c2472702ef2fa4a640377f18f67df023125d7236c09cdcc0f50ced4e38a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bab08915-0a8f-4c7d-9cbb-fc72ed80e3a1", "node_type": "1", "metadata": {"window": "9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated. ", "original_text": "***\n**Fig. "}, "hash": "9aadfd52f8a5d04747fe05337437b8be3e4c6c73acf060f1b07240c534d23d75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.\n\n", "mimetype": "text/plain", "start_char_idx": 30032, "end_char_idx": 30036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bab08915-0a8f-4c7d-9cbb-fc72ed80e3a1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated. ", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9def6c5-e989-4089-9fb4-49ec56fc320d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point. ", "original_text": "6.\n\n"}, "hash": "2ec3f1d7acb8cc900050f4fe86cb89dbb601e7cb24e779b3c909193f2158bf43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b3a1f2b-73e6-483f-bf8a-d7a05251e2e4", "node_type": "1", "metadata": {"window": "6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point. ", "original_text": "8. "}, "hash": "298dcde7e0cc140c4ebda1490e464bc16655d7c018ce9867bb12d0a28a3bf50d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 30036, "end_char_idx": 30047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b3a1f2b-73e6-483f-bf8a-d7a05251e2e4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point. ", "original_text": "8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bab08915-0a8f-4c7d-9cbb-fc72ed80e3a1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "9 is analogous to Fig.  6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated. ", "original_text": "***\n**Fig. "}, "hash": "679cc4c6c29dde675f2224bd45f50a799d312909d9667bc68e638b499562a20f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8d84c9e-9881-4216-9cbb-94b181b020d4", "node_type": "1", "metadata": {"window": "The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point. ", "original_text": "Branching process for the Extended Isolation Forest. "}, "hash": "2dc30ad911b5766833f08d46b4494f5a18779800a12014de36d32d8818972846", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8. ", "mimetype": "text/plain", "start_char_idx": 30047, "end_char_idx": 30050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a8d84c9e-9881-4216-9cbb-94b181b020d4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point. ", "original_text": "Branching process for the Extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b3a1f2b-73e6-483f-bf8a-d7a05251e2e4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6.  The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point. ", "original_text": "8. "}, "hash": "cfa0c3a8cc902331b560e2ac7deb6580c4480a29f19e976f98dc8856df5751e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63ed91e1-5140-4d26-97b0-f0ac7649cd4e", "node_type": "1", "metadata": {"window": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point. ", "original_text": "Fig. "}, "hash": "0137e7bf1f335ca66ca68ced5417e87c4a8c45186ebe3d396a1eb3495de7b28e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Branching process for the Extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 30050, "end_char_idx": 30103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63ed91e1-5140-4d26-97b0-f0ac7649cd4e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8d84c9e-9881-4216-9cbb-94b181b020d4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branch cuts are shown for training of a single tree.  We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point. ", "original_text": "Branching process for the Extended Isolation Forest. "}, "hash": "7b297391c7aea9f2cc8848b2cf5e1daa7b9870b12d2f313c028cb08b2ec84f77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43ab8689-f923-4fbb-8a9f-2bb4dc0fc06d", "node_type": "1", "metadata": {"window": "6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated.", "original_text": "8a shows the branching process for an anomalous data point. "}, "hash": "e90a679e7e7c8fa8e44e543a5b5ed127cd75c5c229ed510eb8486187d15c3558", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 30103, "end_char_idx": 30108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "43ab8689-f923-4fbb-8a9f-2bb4dc0fc06d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated.", "original_text": "8a shows the branching process for an anomalous data point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63ed91e1-5140-4d26-97b0-f0ac7649cd4e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We note that despite the randomness of the process, higher density points are better represented schematically in comparison with Fig.  6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point. ", "original_text": "Fig. "}, "hash": "32ed1330918da8a98a5bbdcd7201edabe95e6e307929a2f5581e713d29e7a9bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2997e1eb-f197-4c8c-973b-005ab9efa315", "node_type": "1", "metadata": {"window": "***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud. ", "original_text": "The branching takes place until the point is isolated. "}, "hash": "fd2ca5d20a622a076dcc8e6b6605688787c1208e013c0486427236e6a6d3cd36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8a shows the branching process for an anomalous data point. ", "mimetype": "text/plain", "start_char_idx": 30108, "end_char_idx": 30168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2997e1eb-f197-4c8c-973b-005ab9efa315", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud. ", "original_text": "The branching takes place until the point is isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43ab8689-f923-4fbb-8a9f-2bb4dc0fc06d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6.\n\n ***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated.", "original_text": "8a shows the branching process for an anomalous data point. "}, "hash": "db814468b7541cca2d076757f2e715e04749e7aadb30ee4f132b477469964f1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "153fdd3f-9b7c-40c8-bedc-b12b354d4c0e", "node_type": "1", "metadata": {"window": "8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n", "original_text": "In this case it only took three random cuts to isolate the point. "}, "hash": "69ffeb9761248697fcc407f415a6bf00c7f6e7675d8b11a83d57c3a7fe33462c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The branching takes place until the point is isolated. ", "mimetype": "text/plain", "start_char_idx": 30168, "end_char_idx": 30223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "153fdd3f-9b7c-40c8-bedc-b12b354d4c0e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n", "original_text": "In this case it only took three random cuts to isolate the point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2997e1eb-f197-4c8c-973b-005ab9efa315", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud. ", "original_text": "The branching takes place until the point is isolated. "}, "hash": "ddc9e77c41e4092fe2a84d0212fe23ee0e6d5bce17635fa6e07a50e5319b2bd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe79879a-77c6-4d48-af31-10a805c35324", "node_type": "1", "metadata": {"window": "Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud. ", "original_text": "8b shows the same branching process for a nominal point. "}, "hash": "a555ecaddf5ae95ad5ef45fa4294e489e2e0da6b285cf4f9e26aa12efe6ddeaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case it only took three random cuts to isolate the point. ", "mimetype": "text/plain", "start_char_idx": 30223, "end_char_idx": 30289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe79879a-77c6-4d48-af31-10a805c35324", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud. ", "original_text": "8b shows the same branching process for a nominal point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "153fdd3f-9b7c-40c8-bedc-b12b354d4c0e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8.  Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n", "original_text": "In this case it only took three random cuts to isolate the point. "}, "hash": "fb783c7fd2edbf8c0f7486057aadd79b1564d878f7841ced76251a6d6d242006", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4796d2d-72d4-4600-9821-df42bac42003", "node_type": "1", "metadata": {"window": "Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n", "original_text": "Since the point is near the center of the data, it takes many cuts to isolate the point. "}, "hash": "b18de88e25662cf3c8621a587c5a52e0c7f44759b0118aae848dee2e51838ef5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8b shows the same branching process for a nominal point. ", "mimetype": "text/plain", "start_char_idx": 30289, "end_char_idx": 30346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4796d2d-72d4-4600-9821-df42bac42003", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n", "original_text": "Since the point is near the center of the data, it takes many cuts to isolate the point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe79879a-77c6-4d48-af31-10a805c35324", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Branching process for the Extended Isolation Forest.  Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud. ", "original_text": "8b shows the same branching process for a nominal point. "}, "hash": "63230ea85ae3d256ff4e1f037964dbb6cf1ab39c08e185adbe930a7831c64bb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fdcc350-d0fa-4bb0-ba9c-56d6a6bb5d1f", "node_type": "1", "metadata": {"window": "8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig. ", "original_text": "In this case the depth limit of the tree was reached before the point was isolated."}, "hash": "023f4d792e381cdfa188fcc807f48dead2739976b91c130a854a4461590bbce3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the point is near the center of the data, it takes many cuts to isolate the point. ", "mimetype": "text/plain", "start_char_idx": 30346, "end_char_idx": 30435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2fdcc350-d0fa-4bb0-ba9c-56d6a6bb5d1f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig. ", "original_text": "In this case the depth limit of the tree was reached before the point was isolated."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4796d2d-72d4-4600-9821-df42bac42003", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n", "original_text": "Since the point is near the center of the data, it takes many cuts to isolate the point. "}, "hash": "e2395831f178d8b3797185991fd24740204f7b976d69f20a3b3e330f094f53dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e765f6a7-eec7-4301-895c-b94a9b4a7135", "node_type": "1", "metadata": {"window": "The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9. ", "original_text": "**\n(a) Anomaly: A scatter plot of a data cloud. "}, "hash": "0a051133d91abe53c8cda5315a5aec2b7e331259f9fde8b20e8fbfeacf04f235", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case the depth limit of the tree was reached before the point was isolated.", "mimetype": "text/plain", "start_char_idx": 30435, "end_char_idx": 30518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e765f6a7-eec7-4301-895c-b94a9b4a7135", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9. ", "original_text": "**\n(a) Anomaly: A scatter plot of a data cloud. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fdcc350-d0fa-4bb0-ba9c-56d6a6bb5d1f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8a shows the branching process for an anomalous data point.  The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig. ", "original_text": "In this case the depth limit of the tree was reached before the point was isolated."}, "hash": "5950f50d12b61adfaadb89e850eb1975b866ee282258b9c8e6eaa87b549372ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3d9c08a-2f68-44f3-954e-a30d4043df3e", "node_type": "1", "metadata": {"window": "In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest. ", "original_text": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n"}, "hash": "5051242449de18b878822eeb0ff656d01cbadc76109025e14d62aee287c8273f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Anomaly: A scatter plot of a data cloud. ", "mimetype": "text/plain", "start_char_idx": 30518, "end_char_idx": 30566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e3d9c08a-2f68-44f3-954e-a30d4043df3e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest. ", "original_text": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e765f6a7-eec7-4301-895c-b94a9b4a7135", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching takes place until the point is isolated.  In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9. ", "original_text": "**\n(a) Anomaly: A scatter plot of a data cloud. "}, "hash": "4502dc81a4780f891b86e4d5bf7d4c64dbbc08d004a9af8eafae8c9f156ce7f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c9d88e1-5ac5-4863-8b2d-2407748e2bed", "node_type": "1", "metadata": {"window": "8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. ", "original_text": "(b) Nominal: The same data cloud. "}, "hash": "3964a4d41789264ab6e069258bd0603e517b126a5551f04832cfd286a4a1ab8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n", "mimetype": "text/plain", "start_char_idx": 30566, "end_char_idx": 30684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c9d88e1-5ac5-4863-8b2d-2407748e2bed", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. ", "original_text": "(b) Nominal: The same data cloud. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3d9c08a-2f68-44f3-954e-a30d4043df3e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case it only took three random cuts to isolate the point.  8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest. ", "original_text": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n"}, "hash": "3db4cdbcae184f1c223c49ae34e456050596f6410b673b0f77ddce72a21e470d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68773319-36d9-40fe-9bde-553602654a99", "node_type": "1", "metadata": {"window": "Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3).", "original_text": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n"}, "hash": "d4b664665350fcd4a98e392f15ad63a2909ef3a67f683c81a119c2dbb7e49cc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Nominal: The same data cloud. ", "mimetype": "text/plain", "start_char_idx": 30684, "end_char_idx": 30718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "68773319-36d9-40fe-9bde-553602654a99", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3).", "original_text": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c9d88e1-5ac5-4863-8b2d-2407748e2bed", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8b shows the same branching process for a nominal point.  Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. ", "original_text": "(b) Nominal: The same data cloud. "}, "hash": "2a494146da650771d7089e8e478aad4edcf290fe3ad493ce44bbe15085fc343d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a11cb6cd-15b6-45fd-b2bc-6aae7ab5e7cf", "node_type": "1", "metadata": {"window": "In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). ", "original_text": "**Fig. "}, "hash": "469630cc291181be96e81948ae210ae7f820e5c51463ff624f685f9ffd279c40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n", "mimetype": "text/plain", "start_char_idx": 30718, "end_char_idx": 30855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a11cb6cd-15b6-45fd-b2bc-6aae7ab5e7cf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68773319-36d9-40fe-9bde-553602654a99", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since the point is near the center of the data, it takes many cuts to isolate the point.  In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3).", "original_text": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n"}, "hash": "793853676ed93b064e50a514e4b079987ee0eeff373d1f37bb50ceab55b68f39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e6c0feb-8592-4943-992b-c2cb272e31ad", "node_type": "1", "metadata": {"window": "**\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n", "original_text": "9. "}, "hash": "5281d561cb720401a881f7f5c137db12081d49b702d764b28b3d1d601a065d4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 30855, "end_char_idx": 30862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e6c0feb-8592-4943-992b-c2cb272e31ad", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n", "original_text": "9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a11cb6cd-15b6-45fd-b2bc-6aae7ab5e7cf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case the depth limit of the tree was reached before the point was isolated. **\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). ", "original_text": "**Fig. "}, "hash": "819769ada3687f17eea1431c62cdbe71cff7507ee32df1e413a7e9ea16cd4e13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfa4f7dc-f948-408f-87c2-02819e0912c2", "node_type": "1", "metadata": {"window": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n", "original_text": "Branch cuts generated by the Extended Isolation Forest. "}, "hash": "f1aa4d10ac00e234f912d631242768132e6621b26dacc7f7728832fe1429a8cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9. ", "mimetype": "text/plain", "start_char_idx": 30862, "end_char_idx": 30865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfa4f7dc-f948-408f-87c2-02819e0912c2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n", "original_text": "Branch cuts generated by the Extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e6c0feb-8592-4943-992b-c2cb272e31ad", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Anomaly: A scatter plot of a data cloud.  A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n", "original_text": "9. "}, "hash": "df2af4827f91ac4d8e70e79e8192b02f0c991135d23f6d737bd8654d037a5c0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e270ffe8-9267-4171-b923-fd2985474b13", "node_type": "1", "metadata": {"window": "(b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n", "original_text": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. "}, "hash": "52417f7092da9ace1921530ecfcbe0301e8cea455e23e2a99de4cc6534c94889", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Branch cuts generated by the Extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 30865, "end_char_idx": 30921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e270ffe8-9267-4171-b923-fd2985474b13", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n", "original_text": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfa4f7dc-f948-408f-87c2-02819e0912c2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A single red point on the periphery is isolated by three angled lines (cuts with random slopes), labeled 0, 1, and 2.\n (b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n", "original_text": "Branch cuts generated by the Extended Isolation Forest. "}, "hash": "b86d1ec2e5eaa3fe7db2c24551bc3ad26e0b61eb9a8b321f48de4ad3e549b4d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d68fe84a-14cf-4bcc-8ee8-e7f02b9f3c06", "node_type": "1", "metadata": {"window": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig. ", "original_text": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3)."}, "hash": "80de8af8fd80855b3a999f125d1aae36f64080c4e55836fbf738f846fad4ed92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. ", "mimetype": "text/plain", "start_char_idx": 30921, "end_char_idx": 31024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d68fe84a-14cf-4bcc-8ee8-e7f02b9f3c06", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig. ", "original_text": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e270ffe8-9267-4171-b923-fd2985474b13", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Nominal: The same data cloud.  A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n", "original_text": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected. "}, "hash": "15c5b5ce79d2e83236a6070285a9a75bee55335a4b5863059dbb0f6cb920be1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "027c4f53-1bf3-444d-a0b4-842a0e331440", "node_type": "1", "metadata": {"window": "**Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. ", "original_text": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). "}, "hash": "8e8a6c463827e6b939750e395e1eb9f9f954a06806af5c4c0e9786bd386f9858", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3).", "mimetype": "text/plain", "start_char_idx": 31024, "end_char_idx": 31159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "027c4f53-1bf3-444d-a0b4-842a0e331440", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. ", "original_text": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d68fe84a-14cf-4bcc-8ee8-e7f02b9f3c06", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A red point in the center is surrounded by numerous angled lines, showing a deep branching process that has not yet isolated the point.\n\n **Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig. ", "original_text": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3)."}, "hash": "1cf76f13a4330b9396db7a5c469885b3232d63f133c6d14c05908cb6f7fc8806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "def19d45-8594-4c14-80f9-61180e914399", "node_type": "1", "metadata": {"window": "9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7. ", "original_text": "The lines are denser in the center where the data is concentrated.\n"}, "hash": "973798eb3a03e4114a9616d83919a9b952d7a89736601eb61145709a608ed4c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). ", "mimetype": "text/plain", "start_char_idx": 31159, "end_char_idx": 31276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "def19d45-8594-4c14-80f9-61180e914399", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7. ", "original_text": "The lines are denser in the center where the data is concentrated.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "027c4f53-1bf3-444d-a0b4-842a0e331440", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. ", "original_text": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes). "}, "hash": "7da488e66769170edb7cbb5351d4283a4fe673970196fea7b0acb3547c136817", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70ebb91c-dcf8-4a9d-a63d-3f9dd187fb1e", "node_type": "1", "metadata": {"window": "Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n", "original_text": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n"}, "hash": "b7dbb78abc2ae2513e12899a1a6e9bba2aa92356b579e415ecb0f74838792641", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lines are denser in the center where the data is concentrated.\n", "mimetype": "text/plain", "start_char_idx": 31276, "end_char_idx": 31343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70ebb91c-dcf8-4a9d-a63d-3f9dd187fb1e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n", "original_text": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "def19d45-8594-4c14-80f9-61180e914399", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "9.  Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7. ", "original_text": "The lines are denser in the center where the data is concentrated.\n"}, "hash": "2adb9336147f214b4f3753fc45c5189e3a585939b031419df1b7f842a8c1ad0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d543115a-7335-4255-a155-21d14b3c1866", "node_type": "1", "metadata": {"window": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists. ", "original_text": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n"}, "hash": "517c1302407a42232a38b7c279fc3e399066f7890058620a7a5cf3b0961470c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n", "mimetype": "text/plain", "start_char_idx": 31343, "end_char_idx": 31465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d543115a-7335-4255-a155-21d14b3c1866", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists. ", "original_text": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70ebb91c-dcf8-4a9d-a63d-3f9dd187fb1e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Branch cuts generated by the Extended Isolation Forest.  In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n", "original_text": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n"}, "hash": "efd6672afbe0be21459229d32c4883b23ac2f6a33e56775ffab732848962f328", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4ac2887-33f4-4581-b94a-058570752a04", "node_type": "1", "metadata": {"window": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. ", "original_text": "***\n\nFig. "}, "hash": "ffd71e946fa667e2cbdb529c39a52426ab85cdda89013f87dc6489216d97d333", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n", "mimetype": "text/plain", "start_char_idx": 31465, "end_char_idx": 31582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b4ac2887-33f4-4581-b94a-058570752a04", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. ", "original_text": "***\n\nFig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d543115a-7335-4255-a155-21d14b3c1866", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In each step, a normal vector, $\\vec{n}$, along with a random intercept point, $\\vec{p}$, is selected.  A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists. ", "original_text": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n"}, "hash": "57f7418262944d411275159c21eca85e1c6b52091cd043a63e3e0c7db58daf51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52f4c1dc-f5de-44be-886e-6845cfd8749b", "node_type": "1", "metadata": {"window": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. ", "original_text": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. "}, "hash": "49e1e58835fc73b1915b66a61991d62f2aaedf61f9935c35660db525c4e41424", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nFig. ", "mimetype": "text/plain", "start_char_idx": 31582, "end_char_idx": 31592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52f4c1dc-f5de-44be-886e-6845cfd8749b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. ", "original_text": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4ac2887-33f4-4581-b94a-058570752a04", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A data point, $\\vec{x}$, is determined to go down the left or right branches of the tree based on the criteria shown in inequality (3). **\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. ", "original_text": "***\n\nFig. "}, "hash": "14f770df4b9153106324f3fd882a840237fbb83eeee6f2c252d7495b11cda922", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd9d8a91-5dbf-4a26-b36d-e5d76aa57f44", "node_type": "1", "metadata": {"window": "The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest. ", "original_text": "7. "}, "hash": "256f92956cb0decb3d84bde795e02bb954becc232df90c33343d0e57d9b43655", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 31592, "end_char_idx": 31792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd9d8a91-5dbf-4a26-b36d-e5d76aa57f44", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest. ", "original_text": "7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52f4c1dc-f5de-44be-886e-6845cfd8749b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization of the data space containing many angled lines (branch cuts with random slopes).  The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. ", "original_text": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig. "}, "hash": "b438d124764701784b299bc7da216749c3b278c7d33a362aa92d94563a9701d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd148d85-9a35-4c89-a040-9f0f521ee688", "node_type": "1", "metadata": {"window": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n", "original_text": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n"}, "hash": "eb80869a4045ad6402289181b14b791de11d2ed86962ec422774000a93940b43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7. ", "mimetype": "text/plain", "start_char_idx": 31792, "end_char_idx": 31795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd148d85-9a35-4c89-a040-9f0f521ee688", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n", "original_text": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd9d8a91-5dbf-4a26-b36d-e5d76aa57f44", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The lines are denser in the center where the data is concentrated.\n (b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest. ", "original_text": "7. "}, "hash": "7582d71197f343cf51452c04c7100da7db021207a7b543c5333a14e2df17741c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a4f674f-1626-4bb1-a2a8-05dcee708246", "node_type": "1", "metadata": {"window": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. ", "original_text": "The important property of concentrating where the data is clustered, persists. "}, "hash": "788b32e11fbf6027e42e1497e19d315f4c8f49e1aac270d04e204ee454163488", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n", "mimetype": "text/plain", "start_char_idx": 31795, "end_char_idx": 31935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a4f674f-1626-4bb1-a2a8-05dcee708246", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. ", "original_text": "The important property of concentrating where the data is clustered, persists. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd148d85-9a35-4c89-a040-9f0f521ee688", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing many angled branch cuts, which are concentrated around the two data clusters.\n (c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n", "original_text": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n"}, "hash": "1f75e03f97e5e48b93f6790df71168e71166d1689b5d59137ef62e4a9da87073", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c6b88ca-89ee-4ae7-8998-230dbdedd05c", "node_type": "1", "metadata": {"window": "***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions. ", "original_text": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. "}, "hash": "2d9bb93ec907d4b82dc417378917c53cd3c77305de2fca80aa8edf24d4301c2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The important property of concentrating where the data is clustered, persists. ", "mimetype": "text/plain", "start_char_idx": 31935, "end_char_idx": 32014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c6b88ca-89ee-4ae7-8998-230dbdedd05c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions. ", "original_text": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a4f674f-1626-4bb1-a2a8-05dcee708246", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoidal: A visualization showing many angled branch cuts, concentrated along the path of the sinusoidal data.\n ***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. ", "original_text": "The important property of concentrating where the data is clustered, persists. "}, "hash": "f468ad561a1071d5c5d2e8ce5bec1ba9751f4186831f6fb9021efe74c88f90a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cecbe2aa-0252-4b02-86e6-39e05d66a937", "node_type": "1", "metadata": {"window": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. ", "original_text": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. "}, "hash": "cdf914d1c9a808d47a5e0f315e651eb98070944fd7334de1ce4a19d213f10a9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. ", "mimetype": "text/plain", "start_char_idx": 32014, "end_char_idx": 32210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cecbe2aa-0252-4b02-86e6-39e05d66a937", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. ", "original_text": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c6b88ca-89ee-4ae7-8998-230dbdedd05c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nFig.  10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions. ", "original_text": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree. "}, "hash": "7b1c15353a78819d257d66b46b51de3b90213f712487795c1c339306724c10f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6872ad5-589c-4cb6-97f6-e45c0735cf69", "node_type": "1", "metadata": {"window": "7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. ", "original_text": "However, there are no regions that artificially receive more attention than the rest. "}, "hash": "3aa557c86353cda657c22392cc9e760465a2ba66ef39140f1e8ae0947d0fe316", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. ", "mimetype": "text/plain", "start_char_idx": 32210, "end_char_idx": 32358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6872ad5-589c-4cb6-97f6-e45c0735cf69", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. ", "original_text": "However, there are no regions that artificially receive more attention than the rest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cecbe2aa-0252-4b02-86e6-39e05d66a937", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10 shows typical plots of all the possible branch cuts that can be produced by the Extended Isolation Forest for each of the examples we have considered thus far, similar to the figures shown in Fig.  7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. ", "original_text": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data. "}, "hash": "934e7af135156315cf4a108c097371ba45b805144993b0b312e59c8c1f30d21c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37c72d3e-5144-4ef9-91d3-0d65eb20cc79", "node_type": "1", "metadata": {"window": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n", "original_text": "The results of this are score maps that are free of artifacts previously observed.\n\n"}, "hash": "782920f790441b28d9b7822546cc22e705f5607d8ef3aaca1ce31d43e3158839", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, there are no regions that artificially receive more attention than the rest. ", "mimetype": "text/plain", "start_char_idx": 32358, "end_char_idx": 32444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37c72d3e-5144-4ef9-91d3-0d65eb20cc79", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n", "original_text": "The results of this are score maps that are free of artifacts previously observed.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6872ad5-589c-4cb6-97f6-e45c0735cf69", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "7.  Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. ", "original_text": "However, there are no regions that artificially receive more attention than the rest. "}, "hash": "2ffd3d4d536e346e1851171527fadbb30f567360f2769e6342f27b0e21d701b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "478f2425-ee0a-4a38-9470-ce6e4d465a39", "node_type": "1", "metadata": {"window": "The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension. ", "original_text": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. "}, "hash": "4cadaf9200f235feb95012e20d4baa2e5423b92f101a740d3ab401dc809c05cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results of this are score maps that are free of artifacts previously observed.\n\n", "mimetype": "text/plain", "start_char_idx": 32444, "end_char_idx": 32528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "478f2425-ee0a-4a38-9470-ce6e4d465a39", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension. ", "original_text": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37c72d3e-5144-4ef9-91d3-0d65eb20cc79", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Unlike the previous case, we can clearly see that branch cuts are possible in all regions of the domain regardless of the coordinate axes.\n\n The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n", "original_text": "The results of this are score maps that are free of artifacts previously observed.\n\n"}, "hash": "42c0e9d307059561fd992d85d506bc27fb14507f499a210bc8ab9fce9acc053b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3399f3f5-18f7-4c35-920a-ca84b53fd30b", "node_type": "1", "metadata": {"window": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. ", "original_text": "However, the algorithm generalizes readily to higher dimensions. "}, "hash": "083ccd58d08b5c6509d2218f3b29403b770040bf600388f3fe9cec0d6953a78b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. ", "mimetype": "text/plain", "start_char_idx": 32528, "end_char_idx": 32672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3399f3f5-18f7-4c35-920a-ca84b53fd30b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. ", "original_text": "However, the algorithm generalizes readily to higher dimensions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "478f2425-ee0a-4a38-9470-ce6e4d465a39", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The important property of concentrating where the data is clustered, persists.  The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension. ", "original_text": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize. "}, "hash": "bbe9151c928db4f8886b77ce95ef46c84f164b7b2540ece0eaf6557c8c16d652", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0af14fff-c048-4b6a-b233-b8eed39ae9c7", "node_type": "1", "metadata": {"window": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes. ", "original_text": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. "}, "hash": "17f8a482bbcad891dc85e02c56266ba2f14ce963f378bb53d526ce2d63327688", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the algorithm generalizes readily to higher dimensions. ", "mimetype": "text/plain", "start_char_idx": 32672, "end_char_idx": 32737, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0af14fff-c048-4b6a-b233-b8eed39ae9c7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes. ", "original_text": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3399f3f5-18f7-4c35-920a-ca84b53fd30b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The intercept points $\\vec{p}$ tend to accumulate where the data is, since the selection of these points gets restricted to available data at each branching point as we move deeper into the tree.  This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. ", "original_text": "However, the algorithm generalizes readily to higher dimensions. "}, "hash": "801b5bfdc5c074be72045cd1c6af84aea968820537149e8bc9b48a755882d65a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d26c2371-e34d-498e-a120-b4016085286e", "node_type": "1", "metadata": {"window": "However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines. ", "original_text": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. "}, "hash": "afb19140edeedf48a132b21a9a1f280c3110e709ac09ada9e94faa83b4c32b46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. ", "mimetype": "text/plain", "start_char_idx": 32737, "end_char_idx": 32831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d26c2371-e34d-498e-a120-b4016085286e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines. ", "original_text": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0af14fff-c048-4b6a-b233-b8eed39ae9c7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This results in more possible branching options where the data is clustered and fewer possible branching where there is less concentration of data.  However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes. ", "original_text": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes. "}, "hash": "95d9af3e79550833e7a6db8609e16699b4a8fa5972d244d6ae8bebfd050b9d82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "586d81d7-15cd-423c-b105-29c60b45f471", "node_type": "1", "metadata": {"window": "The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. ", "original_text": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n"}, "hash": "4a0846a4c3e5ce6531481c3a29dc79c5c4b6f4f1b15ecaa5142dbc76612b5459", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. ", "mimetype": "text/plain", "start_char_idx": 32831, "end_char_idx": 33016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "586d81d7-15cd-423c-b105-29c60b45f471", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. ", "original_text": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d26c2371-e34d-498e-a120-b4016085286e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, there are no regions that artificially receive more attention than the rest.  The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines. ", "original_text": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case. "}, "hash": "a5e1a973fa4d25bfe37f3d572aee068e0ee299ae3187a1f0dc248a24cb58a3bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a1dddb6-c9c4-410c-aeaa-620d6d7fb0bd", "node_type": "1", "metadata": {"window": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n", "original_text": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension. "}, "hash": "3dc8a231de18f049037b6d46d449758f7a909066b8a09cf6b40287a3500a57e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n", "mimetype": "text/plain", "start_char_idx": 33016, "end_char_idx": 33123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a1dddb6-c9c4-410c-aeaa-620d6d7fb0bd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n", "original_text": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "586d81d7-15cd-423c-b105-29c60b45f471", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The results of this are score maps that are free of artifacts previously observed.\n\n ### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. ", "original_text": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n"}, "hash": "2cf00c69bd137c2653411749efc0bc2b36d40ff80f9e474c563e125cdc9614d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "387b590e-dac5-4999-a457-a93f2fb188bc", "node_type": "1", "metadata": {"window": "However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. ", "original_text": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. "}, "hash": "95d4e1c6ea4d907e52c088110033a63fc459383344528fe6fecc9b5572a29a83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension. ", "mimetype": "text/plain", "start_char_idx": 33123, "end_char_idx": 33222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "387b590e-dac5-4999-a457-a93f2fb188bc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. ", "original_text": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a1dddb6-c9c4-410c-aeaa-620d6d7fb0bd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 3.4 High Dimensional Data and Extension Levels\nSo far we have only been looking at two dimensional data because they are easy to visualize.  However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n", "original_text": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension. "}, "hash": "8e69fb2450a1ca2b3abcc0076a6179ae0198cbb0d3e581926eb754e07a3697c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb30465e-b012-449f-84c1-8bd9ac7b6a1c", "node_type": "1", "metadata": {"window": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2). ", "original_text": "This results in hyperplanes that can intersect any of the coordinates axes. "}, "hash": "3f8eb01f4ab1e6dbeab2b036b67a29abbb3023bc5dc3ad6a991cd92d460928ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. ", "mimetype": "text/plain", "start_char_idx": 33222, "end_char_idx": 33334, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb30465e-b012-449f-84c1-8bd9ac7b6a1c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2). ", "original_text": "This results in hyperplanes that can intersect any of the coordinates axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "387b590e-dac5-4999-a457-a93f2fb188bc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, the algorithm generalizes readily to higher dimensions.  In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. ", "original_text": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before. "}, "hash": "de5491bb7d67eff41daaea93372699accca3564d8e5bce79331c22dcd3ce9345", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30b48b51-69df-49e7-b71a-331ff82505f7", "node_type": "1", "metadata": {"window": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). ", "original_text": "For example in the case of two dimensions, it resulted in oblique lines. "}, "hash": "611a41a4acbbc46409f8da8d4f65d560f8065358a5742d21049a21e2cfcf806b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This results in hyperplanes that can intersect any of the coordinates axes. ", "mimetype": "text/plain", "start_char_idx": 33334, "end_char_idx": 33410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30b48b51-69df-49e7-b71a-331ff82505f7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). ", "original_text": "For example in the case of two dimensions, it resulted in oblique lines. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb30465e-b012-449f-84c1-8bd9ac7b6a1c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In this case, the branch cuts are no longer straight line, but N \u2212 1 dimensional hyperplanes.  These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2). ", "original_text": "This results in hyperplanes that can intersect any of the coordinates axes. "}, "hash": "c248ed81fd17a0b9fafbd0658d281d82773fee1a57cefac9f913691dc2bfa4eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e7b6e0e-abca-49eb-a00f-ab373733c9a6", "node_type": "1", "metadata": {"window": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. ", "original_text": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. "}, "hash": "f8ba8345b37d2b9696a66357c59d1abb55dc76aee270834d767b23af624f873b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example in the case of two dimensions, it resulted in oblique lines. ", "mimetype": "text/plain", "start_char_idx": 33410, "end_char_idx": 33483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e7b6e0e-abca-49eb-a00f-ab373733c9a6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. ", "original_text": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30b48b51-69df-49e7-b71a-331ff82505f7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These hyperplanes can still be specified with a random normal vector $\\vec{n}$ and a random intercept point $\\vec{p}$ whose selection are identical to that of the two dimensional case.  The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). ", "original_text": "For example in the case of two dimensions, it resulted in oblique lines. "}, "hash": "b14602e3bf626a9de82707e6b7faf08f958614dfd25dbe960662a1de0b5d8647", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa8a321c-4f9f-4e68-a9ff-5d3bd69927c5", "node_type": "1", "metadata": {"window": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm. ", "original_text": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n"}, "hash": "f418bd6f6a1a7c6c085c28bd77cdae103661fae2bd73746d1ca64cfe95908f3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. ", "mimetype": "text/plain", "start_char_idx": 33483, "end_char_idx": 33614, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aa8a321c-4f9f-4e68-a9ff-5d3bd69927c5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm. ", "original_text": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e7b6e0e-abca-49eb-a00f-ab373733c9a6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The same criteria for branching process specified by inequality (3) applies to the high dimensional case.\n\n It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. ", "original_text": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes. "}, "hash": "99cc16de6d109b08228224223cfd743a758596cce296dccf8fdd9947129db7cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee0f2893-38b6-4a4c-8eca-73444472f87f", "node_type": "1", "metadata": {"window": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. ", "original_text": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. "}, "hash": "0bc4d97d363abea54384f1c9ecb70d1da8fb8378aca051319ea7ca99c275edd4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n", "mimetype": "text/plain", "start_char_idx": 33614, "end_char_idx": 33753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ee0f2893-38b6-4a4c-8eca-73444472f87f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. ", "original_text": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa8a321c-4f9f-4e68-a9ff-5d3bd69927c5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is interesting to note that for an N dimensional dataset we can consider N levels of extension.  In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm. ", "original_text": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n"}, "hash": "13308ee21cfebd2d09def988c49673896b3360c7ae11d8e511e9dc97d83616bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d06428f-866a-4fc0-965d-ef0dd8c0c782", "node_type": "1", "metadata": {"window": "This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig. ", "original_text": "We call that the fully extended case or 2nd Extension (Ex 2). "}, "hash": "f97259a82f35d0399e8540298b3b2207395cd9ab875d1feaf8f7df3e5b603818", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. ", "mimetype": "text/plain", "start_char_idx": 33753, "end_char_idx": 33964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d06428f-866a-4fc0-965d-ef0dd8c0c782", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig. ", "original_text": "We call that the fully extended case or 2nd Extension (Ex 2). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee0f2893-38b6-4a4c-8eca-73444472f87f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the fully extended case, we select our normal vector by drawing each component from N (0, 1) as seen before.  This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. ", "original_text": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes. "}, "hash": "ce60b82e48d39d8455bbb3633aeb784ad92acc0801308e3c4d70790e78398b86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f274ba2d-2c49-4486-ab8d-a3d5be432c8a", "node_type": "1", "metadata": {"window": "For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. ", "original_text": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). "}, "hash": "0d26fd98d2f84881d5184b2cee2c40190dc0f482333af7fed59fb85dde64dce2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We call that the fully extended case or 2nd Extension (Ex 2). ", "mimetype": "text/plain", "start_char_idx": 33964, "end_char_idx": 34026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f274ba2d-2c49-4486-ab8d-a3d5be432c8a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. ", "original_text": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d06428f-866a-4fc0-965d-ef0dd8c0c782", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This results in hyperplanes that can intersect any of the coordinates axes.  For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig. ", "original_text": "We call that the fully extended case or 2nd Extension (Ex 2). "}, "hash": "6346e74ac363c83232952c122a015a56bdf750fb428362e3969cb119451ab468", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68777c34-cff9-4d9f-8ed4-141cf754d815", "node_type": "1", "metadata": {"window": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. ", "original_text": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. "}, "hash": "b5e9f2a8d94f9305773bd608c9aff8a3c518fa3199ae56fcd2f557754f6a906c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). ", "mimetype": "text/plain", "start_char_idx": 34026, "end_char_idx": 34174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "68777c34-cff9-4d9f-8ed4-141cf754d815", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. ", "original_text": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f274ba2d-2c49-4486-ab8d-a3d5be432c8a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For example in the case of two dimensions, it resulted in oblique lines.  However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. ", "original_text": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1). "}, "hash": "f4ba1baf41d977b8ab8b8079c4a81a96de241a90edca8700ede4a2f7ce3bc6cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b89ff267-2d58-4c44-b10a-f74e94ca7cd6", "node_type": "1", "metadata": {"window": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example. ", "original_text": "This case is identically equivalent to the standard Isolation Forest algorithm. "}, "hash": "235aefe14285aad9b12ddc7463d65c62b678a7135480e1acf1600a44853e1dc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. ", "mimetype": "text/plain", "start_char_idx": 34174, "end_char_idx": 34318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b89ff267-2d58-4c44-b10a-f74e94ca7cd6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example. ", "original_text": "This case is identically equivalent to the standard Isolation Forest algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68777c34-cff9-4d9f-8ed4-141cf754d815", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, we can exclude exactly one dimension in specifying the lines so that they are parallel to one of the two coordinate axes.  This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. ", "original_text": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes. "}, "hash": "c520867f163e7b85238809479ffa1b56e803414b917ff0f5a873052cdf56c14f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a6c3afd-cd19-4312-bc46-076f67a04195", "node_type": "1", "metadata": {"window": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. ", "original_text": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. "}, "hash": "00d0cbb6a84f7ca39e36e9a7275cedd020a3e44478fb7b4f9369e7478245a935", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This case is identically equivalent to the standard Isolation Forest algorithm. ", "mimetype": "text/plain", "start_char_idx": 34318, "end_char_idx": 34398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a6c3afd-cd19-4312-bc46-076f67a04195", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. ", "original_text": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b89ff267-2d58-4c44-b10a-f74e94ca7cd6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is simply accomplished by setting a coordinate of the normal vector to zero, in which case we recover the standard Isolation Forest.\n\n In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example. ", "original_text": "This case is identically equivalent to the standard Isolation Forest algorithm. "}, "hash": "eeeff539d64d34c97d55beaf7bb324a2fc7cd21e124767b90b3530999e8d25bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1b1686b-097e-4b7e-8a32-048a50c2749c", "node_type": "1", "metadata": {"window": "We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. ", "original_text": "See Fig. "}, "hash": "6646453ad1fac89e26dff26b3bc7977f3cb627713ec9b8c65b5c4fa4be5eb0dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. ", "mimetype": "text/plain", "start_char_idx": 34398, "end_char_idx": 34540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b1b1686b-097e-4b7e-8a32-048a50c2749c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. ", "original_text": "See Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a6c3afd-cd19-4312-bc46-076f67a04195", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of three dimensional data for example, the fully extended case is when the normal vectors are selected so that the hyperplanes (in this case 2-D planes) are allowed to intersect with all three axes.  We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. ", "original_text": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data. "}, "hash": "c334dbe94e76ebeec7c4ca120685861958261ee52d3536032cec53fffe9de76b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61ce8136-0a80-4c0b-ab96-8a5e6a60764a", "node_type": "1", "metadata": {"window": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n", "original_text": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. "}, "hash": "499665b8288628995ed5da1b4b99b5be2043f12efbb38816dde8a7ab601b4f7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See Fig. ", "mimetype": "text/plain", "start_char_idx": 34540, "end_char_idx": 34549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61ce8136-0a80-4c0b-ab96-8a5e6a60764a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n", "original_text": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1b1686b-097e-4b7e-8a32-048a50c2749c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We call that the fully extended case or 2nd Extension (Ex 2).  If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. ", "original_text": "See Fig. "}, "hash": "9a98d712ca220b0e7d61681da8a5c70fcfe4651557a4d56aacffab2f053b7a4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f51efb08-fae7-41a8-96ca-bbc7d707212f", "node_type": "1", "metadata": {"window": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. ", "original_text": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. "}, "hash": "2c4a25d2d3441308dd5894d311ce90c91c73db33c00adec4ec9ccc94cf2d4545", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 34549, "end_char_idx": 34706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f51efb08-fae7-41a8-96ca-bbc7d707212f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. ", "original_text": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61ce8136-0a80-4c0b-ab96-8a5e6a60764a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "If we reduce the extension level by one, the 2-D planes are always parallel to one of the three coordinates, and we call that 1st Extension (Ex 1).  Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n", "original_text": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest. "}, "hash": "45148f95f1cf608a8c403ef255b00a4c7f62f9cb687d8631ac2dd6d668a44991", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7dcf5ed-9d43-42dc-a9aa-184956df223f", "node_type": "1", "metadata": {"window": "This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n", "original_text": "18 for an example. "}, "hash": "cd899f012a7d4d52f98923acd78cc4418e4ae454514fab292bf59109f783ede0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. ", "mimetype": "text/plain", "start_char_idx": 34706, "end_char_idx": 34807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7dcf5ed-9d43-42dc-a9aa-184956df223f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n", "original_text": "18 for an example. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f51efb08-fae7-41a8-96ca-bbc7d707212f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Reducing the extension yet again, we arrive at 0th extension, which is the case where the random slices are always parallel to two of the axes.  This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. ", "original_text": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig. "}, "hash": "845257c897cf95a55fde94d406f2ea214d112fff951c981ba217235019113e42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53587736-afe2-42fb-b8d1-d1716d64b047", "node_type": "1", "metadata": {"window": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n", "original_text": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. "}, "hash": "eb8405dc5fcf8a7813dd9915323267b96357cd64e6f30db9bcab3b88a8589c4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 for an example. ", "mimetype": "text/plain", "start_char_idx": 34807, "end_char_idx": 34826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "53587736-afe2-42fb-b8d1-d1716d64b047", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n", "original_text": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7dcf5ed-9d43-42dc-a9aa-184956df223f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This case is identically equivalent to the standard Isolation Forest algorithm.  At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n", "original_text": "18 for an example. "}, "hash": "b51a5ed9c70784cec999390d8fcde86a1733ac01a5ed35edc1b4922a8007c64f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2b278bb-09e8-4d76-8751-1b76a486afcf", "node_type": "1", "metadata": {"window": "See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. ", "original_text": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. "}, "hash": "210e0c3ef7084d1c17bd26dad5de53bf9e2abf02163d959297b6f5d9093ba60f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. ", "mimetype": "text/plain", "start_char_idx": 34826, "end_char_idx": 34975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2b278bb-09e8-4d76-8751-1b76a486afcf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. ", "original_text": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53587736-afe2-42fb-b8d1-d1716d64b047", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "At each branching step we essentially have one active coordinate or feature over which a random value is selected to slice the training data.  See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n", "original_text": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different. "}, "hash": "98eda3df062f888ff386141721dce503b0ede3cd5f8770955bae8b44a3bf4205", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88c20423-4ee1-4234-9aa5-bcc4d2feae90", "node_type": "1", "metadata": {"window": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. ", "original_text": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n"}, "hash": "60279559a2247aa4860388a4b1ff7863aafafc301b1bfa4a8d73bf7a96e47455", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. ", "mimetype": "text/plain", "start_char_idx": 34975, "end_char_idx": 35120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88c20423-4ee1-4234-9aa5-bcc4d2feae90", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. ", "original_text": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2b278bb-09e8-4d76-8751-1b76a486afcf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "See Fig.  11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. ", "original_text": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead. "}, "hash": "8ca44f2432329536ad20e3597305e84a1b17e13bbd05e6acd4c53a67be6888fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e657ae5d-dd22-4cdb-a6b3-09a16e932cc3", "node_type": "1", "metadata": {"window": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3). ", "original_text": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. "}, "hash": "86f6419fb804c2a8dbaefe91f8605dfe0f4308943cad22f6fd88e7407a88097e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n", "mimetype": "text/plain", "start_char_idx": 35120, "end_char_idx": 35382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e657ae5d-dd22-4cdb-a6b3-09a16e932cc3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3). ", "original_text": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88c20423-4ee1-4234-9aa5-bcc4d2feae90", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "11\n\nSo for any given N dimensional dataset, the lowest level of extension of the Extended Isolation Forest is coincident with the standard Isolation Forest.  As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. ", "original_text": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n"}, "hash": "037c71b78d0ebc7da2fc3a50bb5556e6fd3f24ac3a8b89c0ad89906da0a0fb4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5f3c0c7-0102-4d8a-9ee2-6406fd018a83", "node_type": "1", "metadata": {"window": "18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change. ", "original_text": "Algorithm 1 remains the same which we show for completeness.\n\n"}, "hash": "615ef3d05dd3859496685cd90998358a06e43e40147da0a7b7ff8c7e553b90e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. ", "mimetype": "text/plain", "start_char_idx": 35382, "end_char_idx": 35494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5f3c0c7-0102-4d8a-9ee2-6406fd018a83", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change. ", "original_text": "Algorithm 1 remains the same which we show for completeness.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e657ae5d-dd22-4cdb-a6b3-09a16e932cc3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As the extension level of the algorithm is increased, the bias of the algorithm is reduced, see Fig.  18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3). ", "original_text": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here. "}, "hash": "50c8fb09bcfd5c5a7f73ac8a02414b522841d901f18d17425b48c0d69ff2364f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60a0bf02-66cc-4596-8f1c-bc84e7be7cf5", "node_type": "1", "metadata": {"window": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. ", "original_text": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n"}, "hash": "76cf5ef0b28b14e6aaf595f77bf60633398864b25d08ac499deb173638f2ac63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 1 remains the same which we show for completeness.\n\n", "mimetype": "text/plain", "start_char_idx": 35494, "end_char_idx": 35556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60a0bf02-66cc-4596-8f1c-bc84e7be7cf5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. ", "original_text": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5f3c0c7-0102-4d8a-9ee2-6406fd018a83", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "18 for an example.  The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change. ", "original_text": "Algorithm 1 remains the same which we show for completeness.\n\n"}, "hash": "f107f1f26c3da5628cd77dca2dafffa7a26c2758e1d49ff09dd82a71282168aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f29cd06-4a73-490b-8bf9-03da8a7cfaa3", "node_type": "1", "metadata": {"window": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. ", "original_text": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. "}, "hash": "d7e19286e6f46607d7da2e0278c4e5a13b13f48d44f89a6712f442387ff77208", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n", "mimetype": "text/plain", "start_char_idx": 35556, "end_char_idx": 36220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f29cd06-4a73-490b-8bf9-03da8a7cfaa3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. ", "original_text": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60a0bf02-66cc-4596-8f1c-bc84e7be7cf5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The idea of having multiple levels of extension can be useful in cases where the dynamic range of the data in various dimensions are very different.  In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. ", "original_text": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n"}, "hash": "99dea5c657b5774b893ac9b099983bf6ddbb8c258bf8c88ff918fbe1e8a3f618", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec4ed8a9-f6bb-423f-81f2-a2f9522c70d0", "node_type": "1", "metadata": {"window": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly. ", "original_text": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. "}, "hash": "b36778d67e15c5ad75138a30b32df5b35cb605eafb333b3db3e03f8d6cf1dfa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. ", "mimetype": "text/plain", "start_char_idx": 36220, "end_char_idx": 36866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ec4ed8a9-f6bb-423f-81f2-a2f9522c70d0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly. ", "original_text": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f29cd06-4a73-490b-8bf9-03da8a7cfaa3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In such cases, reducing the extension level can help in more appropriate selection of split hyperplanes and reducing the computational overhead.  As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. ", "original_text": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point. "}, "hash": "6cca2077a1d1c5168a068192a37dd3f4af23fe4545d794c408c7489ff25fe3c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "040aa40d-b8e0-448c-a772-403c1489bdc4", "node_type": "1", "metadata": {"window": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n", "original_text": "In addition to that, we also change the test condition to reflect inequality (3). "}, "hash": "4ff29ccc7ab9193c5773cbfdcf9a319f276a27548179401670842a85aeb85a6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. ", "mimetype": "text/plain", "start_char_idx": 36866, "end_char_idx": 36991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "040aa40d-b8e0-448c-a772-403c1489bdc4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n", "original_text": "In addition to that, we also change the test condition to reflect inequality (3). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec4ed8a9-f6bb-423f-81f2-a2f9522c70d0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As an extreme case, if we had three dimensional data, but the range in two of the dimensions were much smaller compared to the third (essentially data distributed along a line), the standard Isolation Forest method would probably yield the most optimal result.\n\n ### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly. ", "original_text": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5. "}, "hash": "0a242da72fbdfd0f62f2c3af00c7d3dfe3c93e8e12de0c19f490d2fcf8afa3d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15bf54c8-32ba-49b5-8402-911ebdac5da0", "node_type": "1", "metadata": {"window": "Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig. ", "original_text": "In addition, we have added line 6 which has to do with allowing the extension level to change. "}, "hash": "2b774dc03e514f62f292825c01a58d57716c1020176bef88e9ed6ab2307bc401", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition to that, we also change the test condition to reflect inequality (3). ", "mimetype": "text/plain", "start_char_idx": 36991, "end_char_idx": 37073, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "15bf54c8-32ba-49b5-8402-911ebdac5da0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig. ", "original_text": "In addition, we have added line 6 which has to do with allowing the extension level to change. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "040aa40d-b8e0-448c-a772-403c1489bdc4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 3.5 The Algorithm\nThe are few changes to the original algorithm, published in [9] which are presented here.  Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n", "original_text": "In addition to that, we also change the test condition to reflect inequality (3). "}, "hash": "fbb8830ab8c97edbe5ae23596fb7acce545654e402a0b4db3fa87b3ccfb4da5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680f7e31-f3ca-4a96-9b7c-c89ebe00e9fd", "node_type": "1", "metadata": {"window": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10. ", "original_text": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. "}, "hash": "86e7a5d346f96636da9d9ef1004d83ef5ea8c98dca7da980d255f77b052983a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, we have added line 6 which has to do with allowing the extension level to change. ", "mimetype": "text/plain", "start_char_idx": 37073, "end_char_idx": 37168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "680f7e31-f3ca-4a96-9b7c-c89ebe00e9fd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10. ", "original_text": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15bf54c8-32ba-49b5-8402-911ebdac5da0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Algorithm 1 remains the same which we show for completeness.\n\n **Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig. ", "original_text": "In addition, we have added line 6 which has to do with allowing the extension level to change. "}, "hash": "58613c1e5d9e783dc1bf9e308ab4f3e910f10bea6913c318346c187ae628782a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf488bb9-5412-4414-a128-d8cd1dfbd1cd", "node_type": "1", "metadata": {"window": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. ", "original_text": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. "}, "hash": "3d8054c9da75d3393cee2b495b7b7835fa4973f919e7f7a8f609a18d5ff475a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. ", "mimetype": "text/plain", "start_char_idx": 37168, "end_char_idx": 37325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf488bb9-5412-4414-a128-d8cd1dfbd1cd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. ", "original_text": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680f7e31-f3ca-4a96-9b7c-c89ebe00e9fd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Algorithm 1. i Forest(X, t, \u03c8)**\n**Input:** X - input data, t - number of trees, \u03c8 - sub-sampling size\n**Output:** a set of t iTrees\n1: Initialize Forests\n2: set height limit l = ceiling(log\u2082\u03c8)\n3: **for** i = 1 to t **do**\n4: \u00a0\u00a0\u00a0\u00a0X' \u2190 sample(X, \u03c8)\n5: \u00a0\u00a0\u00a0\u00a0Forest \u2190 Forest \u222a iTree(X', 0, l)\n6: **end for**\n\n**Algorithm 2. iTree(X, e, l)**\n**Input:** X - input data, e - current tree height, l - height limit\n**Output:** an iTree\n1: **if** e \u2265 l or |X| \u2264 1 **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** ex Node{Size \u2190 |X|}\n3: **else**\n4: \u00a0\u00a0\u00a0\u00a0randomly select a normal vector $\\vec{n} \\in \\mathbb{R}^{|X|}$ by drawing each coordinate of $\\vec{n}$ from a standard Gaussian distribution.\n 5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10. ", "original_text": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level. "}, "hash": "53ccbb6dfbd31f2ba5cd71e4942a3b99ecbba351e696a600e05ddc6ed9797659", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f877078-de83-473e-ae14-b9ee3ef9fb8e", "node_type": "1", "metadata": {"window": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints.", "original_text": "In Algorithm 3, we make the changes accordingly. "}, "hash": "5de4f30ca75433790d51a3d2da42c0a2e227caa931c7b783da589c9e52643d09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. ", "mimetype": "text/plain", "start_char_idx": 37325, "end_char_idx": 37553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f877078-de83-473e-ae14-b9ee3ef9fb8e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints.", "original_text": "In Algorithm 3, we make the changes accordingly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf488bb9-5412-4414-a128-d8cd1dfbd1cd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "5: \u00a0\u00a0\u00a0\u00a0randomly select an intercept point $\\vec{p} \\in \\mathbb{R}^{|X|}$ in the range of X\n6: \u00a0\u00a0\u00a0\u00a0set coordinates of $\\vec{n}$ to zero according to extension level\n7: \u00a0\u00a0\u00a0\u00a0$X_l \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} \\le 0)$\n8: \u00a0\u00a0\u00a0\u00a0$X_r \\leftarrow \\text{filter}(X, (X \u2212 \\vec{p}) \\cdot \\vec{n} > 0)$\n9: \u00a0\u00a0\u00a0\u00a0**return** inNode {Left \u2190 iTree(X_l, e + 1, l), Right \u2190 iTree(X_r, e + 1, l), Normal \u2190 $\\vec{n}$, Intercept \u2190 $\\vec{p}$}\n10: **end if**\n\nAs mentioned before, the Extended Isolation Forest algorithm only requires determining two random pieces of information at the branching nodes: the normal vector, and the intercept point.  In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. ", "original_text": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias. "}, "hash": "5905c93ce6a261fd03de5ad62a5ffffa0cebffd73fd29db0d887c718a98fd003", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40454409-df87-420c-9e03-bb7b5703cf5c", "node_type": "1", "metadata": {"window": "In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n", "original_text": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n"}, "hash": "090e8d6f91991ef42556911293a71f68c65c40d7ccb4260419db93eba7875ecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Algorithm 3, we make the changes accordingly. ", "mimetype": "text/plain", "start_char_idx": 37553, "end_char_idx": 37602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40454409-df87-420c-9e03-bb7b5703cf5c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n", "original_text": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f877078-de83-473e-ae14-b9ee3ef9fb8e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Algorithm 2, we modified the two lines that pick a random feature and a random value for that feature with lines 4 and 5.  In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints.", "original_text": "In Algorithm 3, we make the changes accordingly. "}, "hash": "4ce590a0213ccabb59aaaab0b55380e94f846b7b028b8b26590287c8e402f350", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eba771f7-14e7-40a3-b296-e4592e6c20e7", "node_type": "1", "metadata": {"window": "In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n", "original_text": "***\n**Fig. "}, "hash": "e132fa7807139448f77d5c9a38ff3fcbf20f2742ae87c0e829ee059abd76ee3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n", "mimetype": "text/plain", "start_char_idx": 37602, "end_char_idx": 37790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eba771f7-14e7-40a3-b296-e4592e6c20e7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40454409-df87-420c-9e03-bb7b5703cf5c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition to that, we also change the test condition to reflect inequality (3).  In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n", "original_text": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n"}, "hash": "240073ed6e5cc3d5ad8d2f702f971a8f8ad2c4dd1b1808ebca06348b564bc301", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4004a721-698d-4957-b328-bba7e4550af5", "node_type": "1", "metadata": {"window": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n", "original_text": "10. "}, "hash": "7e1b02cf0ccee87ae17c6b86d3641c7b229b8d70589c61b08cf1636f730b85c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 37790, "end_char_idx": 37801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4004a721-698d-4957-b328-bba7e4550af5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n", "original_text": "10. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eba771f7-14e7-40a3-b296-e4592e6c20e7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition, we have added line 6 which has to do with allowing the extension level to change.  With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n", "original_text": "***\n**Fig. "}, "hash": "22bf6f60e0df95ad640c75f60a0a2735689326f280a91bdcd31473738cd6ec59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad2bfdca-9560-4234-a4ef-09fdec46025d", "node_type": "1", "metadata": {"window": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig. ", "original_text": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. "}, "hash": "d2a94381156b401d559c98be0d5bd610c1cbc41297b9d1bcb5705bf1770248eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10. ", "mimetype": "text/plain", "start_char_idx": 37801, "end_char_idx": 37805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad2bfdca-9560-4234-a4ef-09fdec46025d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig. ", "original_text": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4004a721-698d-4957-b328-bba7e4550af5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "With these changes, the algorithm can be used as either the standard Isolation Forest, or as the Extended Isolation Forest with any desired extension level.  This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n", "original_text": "10. "}, "hash": "6acf03240eba62afae154c9342055433bc696d3fca9bef2789d940a901cb6b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1aafe32d-f3d7-4851-bf12-d6b36a1ddafa", "node_type": "1", "metadata": {"window": "In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11. ", "original_text": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints."}, "hash": "03f4b64286384a8ffbdd73346c82db114740061575d08eaab4836f36922a9e5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 37805, "end_char_idx": 37899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1aafe32d-f3d7-4851-bf12-d6b36a1ddafa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11. ", "original_text": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad2bfdca-9560-4234-a4ef-09fdec46025d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This extension customization can be useful in cases, for example, of a complex multidimensional data with low cardinality in one dimension where that feature can be ignored to reduce the introduction of possible selection bias.  In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig. ", "original_text": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest. "}, "hash": "d59c0b70f347c8a84e3e84faa1465239da613a2db36c57552aef80b2554f0cad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67d12d85-ee88-4867-9917-f4dc8d3cbe82", "node_type": "1", "metadata": {"window": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension. ", "original_text": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n"}, "hash": "6bfe1019ee43a6b7475a3c18594e38cfadcf71eefb77e7cd74d70e50c47412a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints.", "mimetype": "text/plain", "start_char_idx": 37899, "end_char_idx": 38029, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67d12d85-ee88-4867-9917-f4dc8d3cbe82", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension. ", "original_text": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1aafe32d-f3d7-4851-bf12-d6b36a1ddafa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In Algorithm 3, we make the changes accordingly.  We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11. ", "original_text": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints."}, "hash": "49b4de64013d14afe0ad478dcfdea5e07739a8ac68330eeec02be0b614155244", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4a4baf2-0d7c-46fd-9ea8-289a8fe4990a", "node_type": "1", "metadata": {"window": "***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest.", "original_text": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n"}, "hash": "757b555f4256bb4d90e5d9e41e065ed21ae78a07f63e78df02678da9a23e24c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n", "mimetype": "text/plain", "start_char_idx": 38029, "end_char_idx": 38178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4a4baf2-0d7c-46fd-9ea8-289a8fe4990a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest.", "original_text": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67d12d85-ee88-4867-9917-f4dc8d3cbe82", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We now have to receive the normal and intercept point from each tree, and use the appropriate test condition to set off the recursion for figuring out path length (depth of each branch).\n\n ***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension. ", "original_text": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n"}, "hash": "89e70dc73cf203023a0932e244e9496de79cfe26b0fb48b6b528e670447d7f3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5181a939-7105-4958-8e19-b857fc5ded88", "node_type": "1", "metadata": {"window": "10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n", "original_text": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n"}, "hash": "48f6973aa01fe8fccbc149e21ed50f7736b7b22abe1f19ab329e5cfce9c4c621", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n", "mimetype": "text/plain", "start_char_idx": 38178, "end_char_idx": 38300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5181a939-7105-4958-8e19-b857fc5ded88", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n", "original_text": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4a4baf2-0d7c-46fd-9ea8-289a8fe4990a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest.", "original_text": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n"}, "hash": "b7b2a43f6837fa3290b5a65d7dd621d2fff99f5abd073eca7634304b5cb2746f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f90e088-471b-42ed-9b4f-851690dd212c", "node_type": "1", "metadata": {"window": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n", "original_text": "**Fig. "}, "hash": "66f54154d3817bbf33652df48ac155bb283dabe2dd3c2f01700e9bf9ecb1276a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n", "mimetype": "text/plain", "start_char_idx": 38300, "end_char_idx": 38415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f90e088-471b-42ed-9b4f-851690dd212c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5181a939-7105-4958-8e19-b857fc5ded88", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10.  A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n", "original_text": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n"}, "hash": "707f051deed705362fd270a182197ee3fda0f3c3a4bb5d20eac03903d2a4507c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c02745d-bebc-4059-9f3e-00fe77ddd2ea", "node_type": "1", "metadata": {"window": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n", "original_text": "11. "}, "hash": "f04a13b036b2e3013920d83814952b4cc7fd0da82500f10967e1ee2aa1a295cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 38415, "end_char_idx": 38422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c02745d-bebc-4059-9f3e-00fe77ddd2ea", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n", "original_text": "11. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f90e088-471b-42ed-9b4f-851690dd212c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "A typical distribution of the possible branch cuts for the case of Extended Isolation Forest.  The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n", "original_text": "**Fig. "}, "hash": "e8257cc75b6dbea540ec16fcc0ec0114e762219de0401fde44171e8d2b0d7c70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c024152-3073-457d-a605-bc486e259f4e", "node_type": "1", "metadata": {"window": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3. ", "original_text": "Sample branch hyperplane for three dimensional data for each level of extension. "}, "hash": "9ddde435f1ead9213977541bfeba686273326ec76b130b652187b80d4fd655b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11. ", "mimetype": "text/plain", "start_char_idx": 38422, "end_char_idx": 38426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c024152-3073-457d-a605-bc486e259f4e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3. ", "original_text": "Sample branch hyperplane for three dimensional data for each level of extension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c02745d-bebc-4059-9f3e-00fe77ddd2ea", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branch cuts produced can now pass through any part of the region free of biases introduced by the coordinate axes constraints. **\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n", "original_text": "11. "}, "hash": "81b45c61418db2e2bc738beda3f0822cdc2355c05506c77b91ed8295d85b144e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9ce88c0-d7be-478d-87a1-dbccc7dbd6e9", "node_type": "1", "metadata": {"window": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) ", "original_text": "The case of extension 0 11c is identical to the standard Isolation Forest."}, "hash": "ba311e09b3a3ba7f432aaffccab49e3ebdf16902e9b762935f04be60d30bf67a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sample branch hyperplane for three dimensional data for each level of extension. ", "mimetype": "text/plain", "start_char_idx": 38426, "end_char_idx": 38507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a9ce88c0-d7be-478d-87a1-dbccc7dbd6e9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) ", "original_text": "The case of extension 0 11c is identical to the standard Isolation Forest."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c024152-3073-457d-a605-bc486e259f4e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Single blob: A visualization showing a high density of angled lines concentrated at the center of the data space, free of axis-parallel bias.\n (b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3. ", "original_text": "Sample branch hyperplane for three dimensional data for each level of extension. "}, "hash": "9884bfb660c24f1c457768927c8de159e41e834213bc6d068552cc593e221a73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f28ad9c-74db-4157-afb2-7bf38c3204ab", "node_type": "1", "metadata": {"window": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. ", "original_text": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n"}, "hash": "f17687df2c07c8a635081677c2a02de65e5cea061865fe78efd06ced03697b2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The case of extension 0 11c is identical to the standard Isolation Forest.", "mimetype": "text/plain", "start_char_idx": 38507, "end_char_idx": 38581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f28ad9c-74db-4157-afb2-7bf38c3204ab", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. ", "original_text": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9ce88c0-d7be-478d-87a1-dbccc7dbd6e9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Multiple Blobs: A visualization showing a high density of angled lines concentrated around the two cluster locations.\n (c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) ", "original_text": "The case of extension 0 11c is identical to the standard Isolation Forest."}, "hash": "ee1a8aed8c9309429c6d7d86adeb03f598706ea8dba834083d19f193b2095936", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd80765a-29ff-49d1-b546-841ac2dbaf2f", "node_type": "1", "metadata": {"window": "**Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n", "original_text": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n"}, "hash": "912d3ecf0ccaccaa43be974ac38e7cdf72c93f3d8a043b3894266ff1faafb983", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n", "mimetype": "text/plain", "start_char_idx": 38581, "end_char_idx": 38677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd80765a-29ff-49d1-b546-841ac2dbaf2f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n", "original_text": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f28ad9c-74db-4157-afb2-7bf38c3204ab", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Sinusoid: A visualization showing a high density of angled lines concentrated along the sinusoidal data path.\n\n **Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. ", "original_text": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n"}, "hash": "a7914f8dc2abefe97f2e3272907077732160e29ee43b84a3433ac57e5b899220", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "254a27cd-4a64-4867-87e3-9f57e292d1d8", "node_type": "1", "metadata": {"window": "11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. ", "original_text": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n"}, "hash": "b7a2614502ed6156e102b3b015e5a4f8de244d7a2f33e56fd70578065dcc8089", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n", "mimetype": "text/plain", "start_char_idx": 38677, "end_char_idx": 38794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "254a27cd-4a64-4867-87e3-9f57e292d1d8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. ", "original_text": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd80765a-29ff-49d1-b546-841ac2dbaf2f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n", "original_text": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n"}, "hash": "8be6d453a8ee94e2a7cb5195f16c0a9b214e45b0f4b2a06539d2453849af0bf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b4eb041-5fea-4929-8b07-7e936f8cf6a6", "node_type": "1", "metadata": {"window": "Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores. ", "original_text": "***\n\n**Algorithm 3. "}, "hash": "950723d338240f75163d5006857b7350cbd391f560b63e836d693ab9f6cc2211", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n", "mimetype": "text/plain", "start_char_idx": 38794, "end_char_idx": 38895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b4eb041-5fea-4929-8b07-7e936f8cf6a6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores. ", "original_text": "***\n\n**Algorithm 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "254a27cd-4a64-4867-87e3-9f57e292d1d8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "11.  Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. ", "original_text": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n"}, "hash": "8f556fbf10ad7dc8e71a9dc0c2644d47863f837fca358af47942bcb55d07d980", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a664976-8e1e-454b-b410-442a79e81cca", "node_type": "1", "metadata": {"window": "The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n", "original_text": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) "}, "hash": "4bc3ad3a22883de8ad831143bfb5cb74278a44cb7a165411b121fcb08aadb4a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n**Algorithm 3. ", "mimetype": "text/plain", "start_char_idx": 38895, "end_char_idx": 38915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a664976-8e1e-454b-b410-442a79e81cca", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n", "original_text": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b4eb041-5fea-4929-8b07-7e936f8cf6a6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Sample branch hyperplane for three dimensional data for each level of extension.  The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores. ", "original_text": "***\n\n**Algorithm 3. "}, "hash": "aff44c5b75fa346bb07ac5fece410c71ca7353409ff3de9d27fdfcc624a031b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a501d7a7-60ae-44fd-9a1c-0b8bb927a0ec", "node_type": "1", "metadata": {"window": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. ", "original_text": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. "}, "hash": "6f658b872ef6f860d6e79cbb74f37660654fa3326826fa331b14708133d4579d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) ", "mimetype": "text/plain", "start_char_idx": 38915, "end_char_idx": 39154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a501d7a7-60ae-44fd-9a1c-0b8bb927a0ec", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. ", "original_text": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a664976-8e1e-454b-b410-442a79e81cca", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The case of extension 0 11c is identical to the standard Isolation Forest. **\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n", "original_text": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.) "}, "hash": "b2b83866c68cb9f8004fc88dd394d822bf4dabf8d976c46167aa03e138db9cc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d48e1334-f33e-4559-9051-c92e8f0cfb53", "node_type": "1", "metadata": {"window": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm. ", "original_text": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n"}, "hash": "47e2a81cd9c96d66a553624ffb3e63df36b4f346c6ef3792892e56f04e14b4fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. ", "mimetype": "text/plain", "start_char_idx": 39154, "end_char_idx": 39645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d48e1334-f33e-4559-9051-c92e8f0cfb53", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm. ", "original_text": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a501d7a7-60ae-44fd-9a1c-0b8bb927a0ec", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Ex 2: A 3D coordinate system with a plane that is angled with respect to all three axes.\n (b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. ", "original_text": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension. "}, "hash": "4becdde1d085c365a68c5aec46a1db7f4a547e114f6ae7ac12af5f5d66289cfc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b063b024-579b-4edf-b443-2bf2f6a8ff21", "node_type": "1", "metadata": {"window": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. ", "original_text": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. "}, "hash": "cdcbfbe92dc8b1cc424d98d1a3329bec7f33aca91fa0ed286da5afd1ff84e9df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n", "mimetype": "text/plain", "start_char_idx": 39645, "end_char_idx": 39802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b063b024-579b-4edf-b443-2bf2f6a8ff21", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. ", "original_text": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d48e1334-f33e-4559-9051-c92e8f0cfb53", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Ex 1: A 3D coordinate system with a plane that is parallel to one axis but angled with respect to the other two.\n (c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm. ", "original_text": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n"}, "hash": "88b0e456c9e2d319a041d8a5dc11770d00163e204d894679b5c0f712b85e73c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efced33a-2db7-494d-b874-9bcd14d004b7", "node_type": "1", "metadata": {"window": "***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n", "original_text": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores. "}, "hash": "0160d3886bcb99df6f4cf8bdefde7b7b9716d6e932964bff7422625dca3f21f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 39802, "end_char_idx": 39934, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "efced33a-2db7-494d-b874-9bcd14d004b7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n", "original_text": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b063b024-579b-4edf-b443-2bf2f6a8ff21", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Ex 0: A 3D coordinate system with a plane that is parallel to two axes (an axis-parallel plane).\n ***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. ", "original_text": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest. "}, "hash": "95bde363566ba6087c515c21c90d9670e0abf55ad7528c369a59a629dbb7dc1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bbcde42-bf43-4806-9b73-82a4fbfe63bc", "node_type": "1", "metadata": {"window": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space. ", "original_text": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n"}, "hash": "173cb7d3aa35b6f40775a2b74e99884a7f79af961efeb96516fdb733a7461488", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 39934, "end_char_idx": 40023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0bbcde42-bf43-4806-9b73-82a4fbfe63bc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space. ", "original_text": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efced33a-2db7-494d-b874-9bcd14d004b7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\n**Algorithm 3.  PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n", "original_text": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores. "}, "hash": "45e7be9541d741b68362d2e75c58b3bd257fd435b2ee3f58e3c3187b16a47175", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93bc9b2f-f191-4c16-90f8-a7869c22c0d6", "node_type": "1", "metadata": {"window": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig. ", "original_text": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. "}, "hash": "2a16ab3adb71c976843a1a6be46a2197ea238326313c50e647351cbfdd8c4c4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n", "mimetype": "text/plain", "start_char_idx": 40023, "end_char_idx": 40130, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93bc9b2f-f191-4c16-90f8-a7869c22c0d6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig. ", "original_text": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bbcde42-bf43-4806-9b73-82a4fbfe63bc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "PathLength(x,T, e)**\n**Input:** x - an instance, T - an iTree, e - current path length; to be initialized to zero when first called\n**Output:** path length of x\n1: **if** T is an external node **then**\n2: \u00a0\u00a0\u00a0\u00a0**return** e + c(T.size){c(.)  is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space. ", "original_text": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n"}, "hash": "cacf0e14707e89daf3c90035e0a05e636a0487a4ef34f4abbb3040dc96a19ff8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b559392-ab3d-41ef-a169-61519bd612c6", "node_type": "1", "metadata": {"window": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n", "original_text": "The score maps of these examples were the initial motivation for improving and extending the algorithm. "}, "hash": "7070a0265bacd81808c82c0cba5995b355a6d03458fccf2138e195c307df4e74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. ", "mimetype": "text/plain", "start_char_idx": 40130, "end_char_idx": 40237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b559392-ab3d-41ef-a169-61519bd612c6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n", "original_text": "The score maps of these examples were the initial motivation for improving and extending the algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93bc9b2f-f191-4c16-90f8-a7869c22c0d6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "is defined in Equation (2)}\n3: **end if**\n4: \u00a0\u00a0\u00a0\u00a0$\\vec{n} \\leftarrow$ T.Normal\n5: \u00a0\u00a0\u00a0\u00a0$\\vec{p} \\leftarrow$ T.Intercept\n6: \u00a0\u00a0\u00a0\u00a0**if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} \\le 0$ **then**\n7: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.left, e + 1)\n8: \u00a0\u00a0\u00a0\u00a0**else if** $(\\vec{x} - \\vec{p}) \\cdot \\vec{n} > 0$ **then**\n9: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0**return** Path Length(x, T.rigth, e + 1)\n10: **end if**\n\nAs we can see the algorithm can easily be modified to take into account all the necessary changes for this extension.  We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig. ", "original_text": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1. "}, "hash": "f0df409ead3a2a2fa4c0195acc52c1989643427c03c68a327b49f8bad0d60142", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1093090b-4b44-4d54-8214-ff425cbab730", "node_type": "1", "metadata": {"window": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs. ", "original_text": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. "}, "hash": "402357300fb063327f38de7ce6e94af4165a49a35c52e058db8cf1697a378cdd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The score maps of these examples were the initial motivation for improving and extending the algorithm. ", "mimetype": "text/plain", "start_char_idx": 40237, "end_char_idx": 40341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1093090b-4b44-4d54-8214-ff425cbab730", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs. ", "original_text": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b559392-ab3d-41ef-a169-61519bd612c6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We have made publicly available a Python implementation of these algorithms\u00b9 accompanied by example Jupyter notebooks to reproduce the figures in the text.\n\n ## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n", "original_text": "The score maps of these examples were the initial motivation for improving and extending the algorithm. "}, "hash": "f3c764e165574b9885f0cd62c2cbbdfac6592ad9b7e2d3513639dfe0e59fcac8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61cf2396-d07d-4649-b385-5e12d4d08690", "node_type": "1", "metadata": {"window": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map. ", "original_text": "We will use different metrics in order to analyze higher dimensional data.\n\n"}, "hash": "f9cb4b702d70173fac03c6c098324cf0375df4936810324cda10737e57907762", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. ", "mimetype": "text/plain", "start_char_idx": 40341, "end_char_idx": 40491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61cf2396-d07d-4649-b385-5e12d4d08690", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map. ", "original_text": "We will use different metrics in order to analyze higher dimensional data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1093090b-4b44-4d54-8214-ff425cbab730", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 4 RESULTS AND DISCUSSION\nHere we present the results of the Extended Isolation Forest compared to the standard Isolation Forest.  We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs. ", "original_text": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D. "}, "hash": "258cb9e413d674425f3867e67dda28fa3d2c47f5a39285208cb757a9b30136b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd11b169-fa6f-407c-9c29-7fcc5c49c5ea", "node_type": "1", "metadata": {"window": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. ", "original_text": "First consider the case of a single blob normally distributed in 2-D space. "}, "hash": "720b143fd04aea1b14c9ed9ca11226927d2aee001fe08e7e7727e3b6ae9cd793", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will use different metrics in order to analyze higher dimensional data.\n\n", "mimetype": "text/plain", "start_char_idx": 40491, "end_char_idx": 40567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd11b169-fa6f-407c-9c29-7fcc5c49c5ea", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. ", "original_text": "First consider the case of a single blob normally distributed in 2-D space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61cf2396-d07d-4649-b385-5e12d4d08690", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We compare the anomaly score maps, variance plots and convergence of the anomaly scores.  We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map. ", "original_text": "We will use different metrics in order to analyze higher dimensional data.\n\n"}, "hash": "14d8225e634ec8656e5ae22910828af6e1ec3b7e2b98edcfdc5d1f9c2d3eb716", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d50e44e6-25ea-4ea2-b8a6-baf339ef5149", "node_type": "1", "metadata": {"window": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig. ", "original_text": "Fig. "}, "hash": "2067470f6dbd7af819660dcf1ac10ce795041876c34a1570de7eaed92cc23faf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First consider the case of a single blob normally distributed in 2-D space. ", "mimetype": "text/plain", "start_char_idx": 40567, "end_char_idx": 40643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d50e44e6-25ea-4ea2-b8a6-baf339ef5149", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd11b169-fa6f-407c-9c29-7fcc5c49c5ea", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also report on the AUROC and AUPRC values in order to quantify the comparison between the two methods.\n\n ### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. ", "original_text": "First consider the case of a single blob normally distributed in 2-D space. "}, "hash": "55e7de3db2e1c8510186b0d935c2fb0515554da8d96e225e00c41cfbc945f591", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "841da034-8f65-4637-ac5f-33a2767d160c", "node_type": "1", "metadata": {"window": "The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest. ", "original_text": "12 compares the anomaly score maps for all the mentioned methods.\n\n"}, "hash": "cf355a81fcd93a811b9a24cce90c6c9b9e913caf369d163bba42507a95707d44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 40643, "end_char_idx": 40648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "841da034-8f65-4637-ac5f-33a2767d160c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest. ", "original_text": "12 compares the anomaly score maps for all the mentioned methods.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d50e44e6-25ea-4ea2-b8a6-baf339ef5149", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 4.1 Score Maps\nWe first look at the score maps of the two dimensional examples presented in section 1.  The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig. ", "original_text": "Fig. "}, "hash": "f0bf33b18bf87981de9c0cab688ba42c5e0f9eac58f28c645abecfb8ef259696", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65203481-5734-4c38-bd50-d5978ed0711c", "node_type": "1", "metadata": {"window": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example. ", "original_text": "By comparing Figs. "}, "hash": "664870f9124cbc348d8b39f8bf458bd71abd95bffb4347e30dae7028cc802a46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 compares the anomaly score maps for all the mentioned methods.\n\n", "mimetype": "text/plain", "start_char_idx": 40648, "end_char_idx": 40715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65203481-5734-4c38-bd50-d5978ed0711c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example. ", "original_text": "By comparing Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "841da034-8f65-4637-ac5f-33a2767d160c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The score maps of these examples were the initial motivation for improving and extending the algorithm.  Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest. ", "original_text": "12 compares the anomaly score maps for all the mentioned methods.\n\n"}, "hash": "c6e9f8d5628e31ce3f2b2ec7d18b665a12002a425d8f5dcf4caf58f0b512b27a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3cad853-db49-4010-a3a6-c94b5b74fcd8", "node_type": "1", "metadata": {"window": "We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n", "original_text": "12a and 12b we can already see a considerable difference in the anomaly score map. "}, "hash": "ee8ad63f489d79ade4949b23166da6d0211bbee64496e7436fc25fe57b553944", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By comparing Figs. ", "mimetype": "text/plain", "start_char_idx": 40715, "end_char_idx": 40734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3cad853-db49-4010-a3a6-c94b5b74fcd8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n", "original_text": "12a and 12b we can already see a considerable difference in the anomaly score map. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65203481-5734-4c38-bd50-d5978ed0711c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Note that these plots are only possible for two dimensional data, even though it is somewhat manageable to visualize these maps and artifacts in 3-D.  We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example. ", "original_text": "By comparing Figs. "}, "hash": "34947551f8d8c9b091243800cd033e79fb47eef2dc03d542d59b2e730d36d3d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e237af3-efd0-47c4-9860-46294fb9f734", "node_type": "1", "metadata": {"window": "First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig. ", "original_text": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. "}, "hash": "f008d4dcf5eb0c0eb401a52c9345c5be2e7ab0ecc30cccdda75883e9aa9bf97b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12a and 12b we can already see a considerable difference in the anomaly score map. ", "mimetype": "text/plain", "start_char_idx": 40734, "end_char_idx": 40817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6e237af3-efd0-47c4-9860-46294fb9f734", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig. ", "original_text": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3cad853-db49-4010-a3a6-c94b5b74fcd8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We will use different metrics in order to analyze higher dimensional data.\n\n First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n", "original_text": "12a and 12b we can already see a considerable difference in the anomaly score map. "}, "hash": "7fc474c19bf674c53c5473dff81bf900bee01a3620ca1ddac893a88ce7d46d3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e06cb94b-bcde-4e57-bd9a-a6aea4763ca8", "node_type": "1", "metadata": {"window": "Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13. ", "original_text": "Taking it a step further, we look at the score map in Fig. "}, "hash": "d79f28d6d7ba0e49c1419e4b08f0fd3c8ec5adcd912052e4c495c2ef2aab4ef0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. ", "mimetype": "text/plain", "start_char_idx": 40817, "end_char_idx": 40958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e06cb94b-bcde-4e57-bd9a-a6aea4763ca8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13. ", "original_text": "Taking it a step further, we look at the score map in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e237af3-efd0-47c4-9860-46294fb9f734", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "First consider the case of a single blob normally distributed in 2-D space.  Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig. ", "original_text": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example. "}, "hash": "30a5c3d9276f8693ddc8b49c84cfb80759cf1705b3d6dd4da8ba146085318a57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f8c4721-6337-4a49-87de-c42709786f53", "node_type": "1", "metadata": {"window": "12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig. ", "original_text": "12c, which was obtained by the Extended Isolation Forest. "}, "hash": "76da07a6e4e43dfde18ff15c5945696f0181607b7ea23ab39c321af1cb6412b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Taking it a step further, we look at the score map in Fig. ", "mimetype": "text/plain", "start_char_idx": 40958, "end_char_idx": 41017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f8c4721-6337-4a49-87de-c42709786f53", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig. ", "original_text": "12c, which was obtained by the Extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e06cb94b-bcde-4e57-bd9a-a6aea4763ca8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13. ", "original_text": "Taking it a step further, we look at the score map in Fig. "}, "hash": "47ac9043afbd721f47e25c6ef9f3c9e17037fe91ab3c8ad89586683f51ce0ce1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a0043b2-af71-4a1a-9ddc-5e32e74b0fdc", "node_type": "1", "metadata": {"window": "By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n", "original_text": "We can see an anomaly score map as we might expect for this example. "}, "hash": "157aac3d35a549bbe2f3fbd0840f708742fe8538f5b6ff1f18a7b17ea24d903e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12c, which was obtained by the Extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 41017, "end_char_idx": 41075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a0043b2-af71-4a1a-9ddc-5e32e74b0fdc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n", "original_text": "We can see an anomaly score map as we might expect for this example. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f8c4721-6337-4a49-87de-c42709786f53", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12 compares the anomaly score maps for all the mentioned methods.\n\n By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig. ", "original_text": "12c, which was obtained by the Extended Isolation Forest. "}, "hash": "bc518e79b813e85280096e3829caa29fe069cda8805a49236c5f2397949da7f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3398b3d-768a-4c65-8879-5aa6c7cfc7fb", "node_type": "1", "metadata": {"window": "12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs. ", "original_text": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n"}, "hash": "47061136baa1f8ccebb25c97efbaaa6b82e28914520533ca83eece0631c4b147", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can see an anomaly score map as we might expect for this example. ", "mimetype": "text/plain", "start_char_idx": 41075, "end_char_idx": 41144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b3398b3d-768a-4c65-8879-5aa6c7cfc7fb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs. ", "original_text": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a0043b2-af71-4a1a-9ddc-5e32e74b0fdc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "By comparing Figs.  12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n", "original_text": "We can see an anomaly score map as we might expect for this example. "}, "hash": "5e7be38a9f5c808643cd58992e5d9d9fdc4a4fa33a0ce38d8b5b4bc32cded716", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dde28409-8f15-48dd-9586-e650db302adc", "node_type": "1", "metadata": {"window": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. ", "original_text": "Now we turn to the case of multiple blobs presented in Fig. "}, "hash": "11bf822ad7ac2e1ba9c81d3186b2f6a5a700aed5d816992c7508849d4b5926e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n", "mimetype": "text/plain", "start_char_idx": 41144, "end_char_idx": 41357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dde28409-8f15-48dd-9586-e650db302adc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. ", "original_text": "Now we turn to the case of multiple blobs presented in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3398b3d-768a-4c65-8879-5aa6c7cfc7fb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12a and 12b we can already see a considerable difference in the anomaly score map.  The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs. ", "original_text": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n"}, "hash": "23e70319358952f41fad68ecbe70adcb1803a199d92aab60f21760d509b3bbe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba31c876-404a-4324-af71-1197c3a69f00", "node_type": "1", "metadata": {"window": "Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected. ", "original_text": "13. "}, "hash": "9f879105e1fe39d0947bb11dad7f986b5c842c78916654cbac1503c05951d38f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now we turn to the case of multiple blobs presented in Fig. ", "mimetype": "text/plain", "start_char_idx": 41357, "end_char_idx": 41417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba31c876-404a-4324-af71-1197c3a69f00", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected. ", "original_text": "13. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dde28409-8f15-48dd-9586-e650db302adc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The rotation of the sub-sampled data for each tree averages out the artifacts and produces a more symmetric map as expected in this example.  Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. ", "original_text": "Now we turn to the case of multiple blobs presented in Fig. "}, "hash": "a147512d3d998a6d7e01c7e02405fa2a29314ce8753f836f6112f7daa60cdc03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42b5d2ee-b0c8-45fd-a3a6-06cd3d0c352e", "node_type": "1", "metadata": {"window": "12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. ", "original_text": "As we saw for the single blob case and as shown in Fig. "}, "hash": "5f2ac5bf19f0562c03ed18e204dc5c59c7776f6bcd66e1d0115c3089ac05af12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13. ", "mimetype": "text/plain", "start_char_idx": 41417, "end_char_idx": 41421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42b5d2ee-b0c8-45fd-a3a6-06cd3d0c352e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. ", "original_text": "As we saw for the single blob case and as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba31c876-404a-4324-af71-1197c3a69f00", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Taking it a step further, we look at the score map in Fig.  12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected. ", "original_text": "13. "}, "hash": "60c8fa24cd15d78d7657bf3c2fc82dfa7c4e366bb2fbe0d31e45e92aaea85738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce31e43a-3657-40bc-8a42-af20404cd593", "node_type": "1", "metadata": {"window": "We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well. ", "original_text": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n"}, "hash": "5e390b332abcf952d957ecb6ca23cf53a401f3864f2a23961dc149a898c8c09f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we saw for the single blob case and as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 41421, "end_char_idx": 41477, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ce31e43a-3657-40bc-8a42-af20404cd593", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well. ", "original_text": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42b5d2ee-b0c8-45fd-a3a6-06cd3d0c352e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12c, which was obtained by the Extended Isolation Forest.  We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. ", "original_text": "As we saw for the single blob case and as shown in Fig. "}, "hash": "91515e65b5b571f1e745d9d650f7b206d1243cbab94c9d4a79c42bfddd07261a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3438fbe4-db44-4e0c-97e1-acb41ebed193", "node_type": "1", "metadata": {"window": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. ", "original_text": "As shown in Figs. "}, "hash": "4960d67e477edf5bc8d2d682ebfd1d17e0e2821cc8c04d6fbfed467cf7104e0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n", "mimetype": "text/plain", "start_char_idx": 41477, "end_char_idx": 41667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3438fbe4-db44-4e0c-97e1-acb41ebed193", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. ", "original_text": "As shown in Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce31e43a-3657-40bc-8a42-af20404cd593", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can see an anomaly score map as we might expect for this example.  The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well. ", "original_text": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n"}, "hash": "a974d35c13f02887a1699fb0d152ba69684de5486679f2d65be3ef33c0294180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3b063c9-9352-4503-b437-881bfde4c5f0", "node_type": "1", "metadata": {"window": "Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n", "original_text": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. "}, "hash": "bfccc0e0b4b26b3b705b57273e2ffbf79c6bfaf16a764fc2ffb2a397500d152f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As shown in Figs. ", "mimetype": "text/plain", "start_char_idx": 41667, "end_char_idx": 41685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e3b063c9-9352-4503-b437-881bfde4c5f0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n", "original_text": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3438fbe4-db44-4e0c-97e1-acb41ebed193", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The low score artificial bands in the x and y direction are no longer present and the score increases roughly monotonically in all directions as we move radially outward from the center in a quasi-symmetric way.\n\n Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. ", "original_text": "As shown in Figs. "}, "hash": "fe49769228485b72d3f6d8f480adbb65f9d479415c0f9d87a62fad81baced9c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cc3aec8-58a4-4179-af7d-7b435e911675", "node_type": "1", "metadata": {"window": "13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n", "original_text": "The two blobs are clearly scored with low anomaly scores, as expected. "}, "hash": "d3dcba0dc9c133e2b2930764d2e063224e7c6735c06e49a5538921f8923872a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. ", "mimetype": "text/plain", "start_char_idx": 41685, "end_char_idx": 41807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cc3aec8-58a4-4179-af7d-7b435e911675", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n", "original_text": "The two blobs are clearly scored with low anomaly scores, as expected. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3b063c9-9352-4503-b437-881bfde4c5f0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Now we turn to the case of multiple blobs presented in Fig.  13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n", "original_text": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone. "}, "hash": "f41035166778fa74689f902d728b9dce47b6275024531b1220de82d2f0ac5152", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52bd8139-4d86-40b7-a86d-4ddb65fc5b58", "node_type": "1", "metadata": {"window": "As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig. ", "original_text": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. "}, "hash": "7d1c9f2b52100c70afe507cd6f259285bfb2ed0ddb461b6ae54fb567d13616d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two blobs are clearly scored with low anomaly scores, as expected. ", "mimetype": "text/plain", "start_char_idx": 41807, "end_char_idx": 41878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52bd8139-4d86-40b7-a86d-4ddb65fc5b58", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig. ", "original_text": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cc3aec8-58a4-4179-af7d-7b435e911675", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13.  As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n", "original_text": "The two blobs are clearly scored with low anomaly scores, as expected. "}, "hash": "eb7f36aad41128e0a0f153670988249d1bea5764c6a5d33ce9487000ae66e665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac9f4ce4-b25e-44cf-90d8-8ee511ae7f6a", "node_type": "1", "metadata": {"window": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case. ", "original_text": "The Extended Isolation Forest algorithm is able to capture this detail quite well. "}, "hash": "508db046afa56408e47a8a19f5d22d1e31c1d21d3f2b442ffdcdd29a873b20e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. ", "mimetype": "text/plain", "start_char_idx": 41878, "end_char_idx": 41987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac9f4ce4-b25e-44cf-90d8-8ee511ae7f6a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case. ", "original_text": "The Extended Isolation Forest algorithm is able to capture this detail quite well. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52bd8139-4d86-40b7-a86d-4ddb65fc5b58", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we saw for the single blob case and as shown in Fig.  13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig. ", "original_text": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs. "}, "hash": "5ec4dd50b3e44c274a3a06d6032e3df03b2bd5cb0a591fff5def9ef9ad883ac0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4544c713-fd95-4b76-acb3-c0034d2f3f81", "node_type": "1", "metadata": {"window": "As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear. ", "original_text": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. "}, "hash": "ed27bb5f7be8fd47731fed2403fa07a3b3627dd4eb009863d0dc8b8940a00c04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Extended Isolation Forest algorithm is able to capture this detail quite well. ", "mimetype": "text/plain", "start_char_idx": 41987, "end_char_idx": 42070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4544c713-fd95-4b76-acb3-c0034d2f3f81", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear. ", "original_text": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac9f4ce4-b25e-44cf-90d8-8ee511ae7f6a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13a, the creation of branch cuts that were only parallel to the coordinate axes was causing two \u201cghost\u201d regions in the score map, in addition to the bands observed in the single blob case.\n\n As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case. ", "original_text": "The Extended Isolation Forest algorithm is able to capture this detail quite well. "}, "hash": "0348268f5a7f0e7e99792da00ad6e9522c9804742b9d532880e61782e8dbb907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7db17e22-72f5-4cc7-9372-ab6a37d755ee", "node_type": "1", "metadata": {"window": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data. ", "original_text": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n"}, "hash": "0e6c18020197c8862e13abf95ddfcda785da4c5c4ccdec5e2ac12ac15fea41e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. ", "mimetype": "text/plain", "start_char_idx": 42070, "end_char_idx": 42268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7db17e22-72f5-4cc7-9372-ab6a37d755ee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data. ", "original_text": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4544c713-fd95-4b76-acb3-c0034d2f3f81", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As shown in Figs.  13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear. ", "original_text": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution. "}, "hash": "e6f089c00e7e9b6ec2b7deba084a2b3406772a5e9b11c2ca8168a0c5dd18ac9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "786ae0de-73f8-4ef6-aa07-643878ffe31e", "node_type": "1", "metadata": {"window": "The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n", "original_text": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n"}, "hash": "0cd1cfdd434c8209201cd75f8ddc5a7911b8702328e397243dd83e88ab670165", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n", "mimetype": "text/plain", "start_char_idx": 42268, "end_char_idx": 42354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "786ae0de-73f8-4ef6-aa07-643878ffe31e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n", "original_text": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7db17e22-72f5-4cc7-9372-ab6a37d755ee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13b and 13c, in both cases, namely, the Rotated and Extended Isolation Forest, these \u201cghost\u201d regions are completely gone.  The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data. ", "original_text": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n"}, "hash": "135a6c790d0a03295c3371c04e20f0a0242bac3d390fbf7adbe9bcc5458c22a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5bc022b-9e3f-4858-b7d5-cef7dee78989", "node_type": "1", "metadata": {"window": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. ", "original_text": "Fig. "}, "hash": "705c5cdd4928ab280a6ea5e12a7fc593e1cdd683d41cda0c6e7b7360718bddff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n", "mimetype": "text/plain", "start_char_idx": 42354, "end_char_idx": 42553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5bc022b-9e3f-4858-b7d5-cef7dee78989", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "786ae0de-73f8-4ef6-aa07-643878ffe31e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The two blobs are clearly scored with low anomaly scores, as expected.  The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n", "original_text": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n"}, "hash": "c9e2a2c6df5378af9d9ec6e3a7c7707496f51c08f0936bb59fd27561041a8d23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8864436c-46f9-4390-9051-c4ef0076e2e5", "node_type": "1", "metadata": {"window": "The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12. ", "original_text": "14 shows a comparison of the results for this case. "}, "hash": "34ddbfc67a09210b7526d50bd5c879afa9a6e98b75f550fcdfa3ddb9815b0735", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 42553, "end_char_idx": 42558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8864436c-46f9-4390-9051-c4ef0076e2e5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12. ", "original_text": "14 shows a comparison of the results for this case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5bc022b-9e3f-4858-b7d5-cef7dee78989", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The interesting feature to notice here is the higher anomaly score region directly in between the two blobs.  The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. ", "original_text": "Fig. "}, "hash": "c4c30f9e74a8153e16253bfec5c18e44bf3e2717c767a8c5e34b4a2f4dd57ec2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d16944ee-e5bd-4a72-a48a-a0f91680feae", "node_type": "1", "metadata": {"window": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob.", "original_text": "The improvement in both cases over the Standard Isolation Forest is clear. "}, "hash": "cda2d4a77bc077bac6e9bafa2812d17f1b3922c621aea636702ffd1766801a96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 shows a comparison of the results for this case. ", "mimetype": "text/plain", "start_char_idx": 42558, "end_char_idx": 42610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d16944ee-e5bd-4a72-a48a-a0f91680feae", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob.", "original_text": "The improvement in both cases over the Standard Isolation Forest is clear. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8864436c-46f9-4390-9051-c4ef0076e2e5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The Extended Isolation Forest algorithm is able to capture this detail quite well.  This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12. ", "original_text": "14 shows a comparison of the results for this case. "}, "hash": "55f88d513522413608e7f5cd3ee387fd55322d077477517583052460801bad56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30e770b3-48c6-464b-aded-6ff7c46477e4", "node_type": "1", "metadata": {"window": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n", "original_text": "The structure of the data is better preserved, and the anomaly score map is representative of the data. "}, "hash": "fec8ebac044e085fcd268675651d16aad49b128f5653b4953815d813be212a4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The improvement in both cases over the Standard Isolation Forest is clear. ", "mimetype": "text/plain", "start_char_idx": 42610, "end_char_idx": 42685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30e770b3-48c6-464b-aded-6ff7c46477e4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n", "original_text": "The structure of the data is better preserved, and the anomaly score map is representative of the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d16944ee-e5bd-4a72-a48a-a0f91680feae", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This additional feature is important as this region can be considered as close to nominal given the proximity to the blobs, but with higher score since it is far from the concentrated distribution.  The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob.", "original_text": "The improvement in both cases over the Standard Isolation Forest is clear. "}, "hash": "41febd1d5204e2c98772e42d07c908df689a0d8171d876c27438010fbfcc27d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e7fa50-3c39-4e3d-a9e3-8f6a444d16d0", "node_type": "1", "metadata": {"window": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n", "original_text": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n"}, "hash": "9c455aa3c599d047be992bb9f6fce9b7f97652fafa3cf0b17908aa723d2d3bf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The structure of the data is better preserved, and the anomaly score map is representative of the data. ", "mimetype": "text/plain", "start_char_idx": 42685, "end_char_idx": 42789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b6e7fa50-3c39-4e3d-a9e3-8f6a444d16d0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n", "original_text": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30e770b3-48c6-464b-aded-6ff7c46477e4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The Standard Isolation Forest fails in capturing this extra structure from the data.\n\n As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n", "original_text": "The structure of the data is better preserved, and the anomaly score map is representative of the data. "}, "hash": "b2b6a2665d12ccbf13e0a8a3a7c6bf2bec30b12ecdbc52ea2cb6c5c5af4629fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a226742b-f91e-45ab-acd9-d8f9d17944ec", "node_type": "1", "metadata": {"window": "Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. ", "original_text": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. "}, "hash": "a215551eb25348565f29f4825d8f016e0d4ffc5a514ac5b286220a45122126d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n", "mimetype": "text/plain", "start_char_idx": 42789, "end_char_idx": 42918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a226742b-f91e-45ab-acd9-d8f9d17944ec", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. ", "original_text": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e7fa50-3c39-4e3d-a9e3-8f6a444d16d0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As for the sinusoidal data, we observed before that the standard Isolation Forest failed to detect the structure of the data and treated it as one rectangular blob with very wide rectangular bands.\n\n Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n", "original_text": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n"}, "hash": "3fbd57b891a2119246b6b41ed572a16e64ed7542b80a2e7f392413f0fce2f9c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f22b5997-0528-4690-a72e-ffc9e384f3e8", "node_type": "1", "metadata": {"window": "14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n", "original_text": "12. "}, "hash": "3c006853ddc610afaaacd2fc2d241c5a751834271fa5c6dbff58616e8233007f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 42918, "end_char_idx": 42968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f22b5997-0528-4690-a72e-ffc9e384f3e8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n", "original_text": "12. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a226742b-f91e-45ab-acd9-d8f9d17944ec", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. ", "original_text": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig. "}, "hash": "45dafbd8a587793c63a5f387ad762a6dcedabe4c57672bec39dd4aef8a0657ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a5481f3-0290-402d-b8a8-ba54a8c09f80", "node_type": "1", "metadata": {"window": "The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig. ", "original_text": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob."}, "hash": "e96a255d9fd6919e631f5909e8b586ac0d80d5ca6225b16d1e50eac3b5660780", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12. ", "mimetype": "text/plain", "start_char_idx": 42968, "end_char_idx": 42972, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a5481f3-0290-402d-b8a8-ba54a8c09f80", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig. ", "original_text": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f22b5997-0528-4690-a72e-ffc9e384f3e8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14 shows a comparison of the results for this case.  The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n", "original_text": "12. "}, "hash": "3407300fee0b07a3d3eae58fbddb935c91b254cee528a4b76aadee332da47d5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd25a87d-24a0-4f97-9fa8-646873e1bc44", "node_type": "1", "metadata": {"window": "The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13. ", "original_text": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n"}, "hash": "81fa8d7c0e6cd77b8095eafadab3db1e7822ed2d5bc4d0a0a287780e19757b26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob.", "mimetype": "text/plain", "start_char_idx": 42972, "end_char_idx": 43105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd25a87d-24a0-4f97-9fa8-646873e1bc44", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13. ", "original_text": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a5481f3-0290-402d-b8a8-ba54a8c09f80", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The improvement in both cases over the Standard Isolation Forest is clear.  The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig. ", "original_text": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob."}, "hash": "0932cd31ff5a309100fffd02d1c504ae3f116f9dce497bbb1997240b82980a10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eee3fd36-1f45-486a-940a-9a6381e2cbc9", "node_type": "1", "metadata": {"window": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs.", "original_text": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n"}, "hash": "4d10292cfe0fb42ed30eda8451c2bb815ffd08ec9e8bf0e63ff9e727f7b69faf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n", "mimetype": "text/plain", "start_char_idx": 43105, "end_char_idx": 43250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eee3fd36-1f45-486a-940a-9a6381e2cbc9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs.", "original_text": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd25a87d-24a0-4f97-9fa8-646873e1bc44", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The structure of the data is better preserved, and the anomaly score map is representative of the data.  In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13. ", "original_text": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n"}, "hash": "ce0e09a0bf637fd428589a852ab7cca3efc86a8ffb5f373bce420850c837112f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0835891d-39fe-47a8-b2d2-07567cba522e", "node_type": "1", "metadata": {"window": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n", "original_text": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. "}, "hash": "73e8eb7232abe6f899693e70fc41c5cc36e3d23e61d0e76f8b9fb74ba9c8d457", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n", "mimetype": "text/plain", "start_char_idx": 43250, "end_char_idx": 43397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0835891d-39fe-47a8-b2d2-07567cba522e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n", "original_text": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eee3fd36-1f45-486a-940a-9a6381e2cbc9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of the Extended Isolation Forest, the score map tracks the sinusoidal data even more tightly than the rotated case.\n\n ---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs.", "original_text": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n"}, "hash": "20992760fdd240030996b477f3572597e0cfc1b6363fa19bb183c528e655d037", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fdf4e35-4b6d-4def-9c0f-d01f3581bdb6", "node_type": "1", "metadata": {"window": "12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n", "original_text": "The artifacts are completely gone.\n\n"}, "hash": "c805c2bfa98e295397576b7f00fc897e7b3d1c67bc6d486f9265ad8e52d80112", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. ", "mimetype": "text/plain", "start_char_idx": 43397, "end_char_idx": 43546, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2fdf4e35-4b6d-4def-9c0f-d01f3581bdb6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n", "original_text": "The artifacts are completely gone.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0835891d-39fe-47a8-b2d2-07567cba522e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "---\n\u00b9 https://github.com/sahandha/eif\n\n***\n**Fig.  12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n", "original_text": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions. "}, "hash": "8e3401fd240f9c40848c04a4ca9f56ec0a6ec1a8245880d650532e61610142a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23bf8057-d973-4b2d-aecd-3b41c1604009", "node_type": "1", "metadata": {"window": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. ", "original_text": "**Fig. "}, "hash": "5a57abb055ae414ce6a64393c0e05ea35945cfcaee014ac37d3cf0c869f5423e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The artifacts are completely gone.\n\n", "mimetype": "text/plain", "start_char_idx": 43546, "end_char_idx": 43582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23bf8057-d973-4b2d-aecd-3b41c1604009", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fdf4e35-4b6d-4def-9c0f-d01f3581bdb6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12.  Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n", "original_text": "The artifacts are completely gone.\n\n"}, "hash": "ed2b053c5eb2055eddb45f8db03c755a0620f6cc4807f401d1fdcf96901aade6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "385e4785-93fb-4eb1-942e-d97228cbc90e", "node_type": "1", "metadata": {"window": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n", "original_text": "13. "}, "hash": "0b2a4a1b801871b3db68d427e106f9d767ff6b28c34439f724c507afc933000e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 43582, "end_char_idx": 43589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "385e4785-93fb-4eb1-942e-d97228cbc90e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n", "original_text": "13. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23bf8057-d973-4b2d-aecd-3b41c1604009", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Comparison of the Standard Isolation Forest, Rotated Isolation Forest, and Extended Isolation Forest for the case of the single blob. **\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. ", "original_text": "**Fig. "}, "hash": "c1bed2ac7413b0e9daa5e7c0c705a62136bb409f811b642437ed8da1782d5b58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5136bf57-e06a-4c75-b1cf-a7b8a6cd76f7", "node_type": "1", "metadata": {"window": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig. ", "original_text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs."}, "hash": "87df5393674c8bd41162d5e359601aba0bf23979aca8832c132087b9a23211f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13. ", "mimetype": "text/plain", "start_char_idx": 43589, "end_char_idx": 43593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5136bf57-e06a-4c75-b1cf-a7b8a6cd76f7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig. ", "original_text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "385e4785-93fb-4eb1-942e-d97228cbc90e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard IF: A heatmap showing a central bright spot with distinct horizontal and vertical bands of higher anomaly scores (the artifact).\n (b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n", "original_text": "13. "}, "hash": "9658994c52bd8a0278c9e11ef402bb8b63f63feedff32f68c3702db1443fd63d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4df76e13-6c5a-4772-95a0-8938014d92bf", "node_type": "1", "metadata": {"window": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14. ", "original_text": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n"}, "hash": "f4af32493b10cdc8f5972b14f31b142d9e58aa05379949be429680eaf34aea07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs.", "mimetype": "text/plain", "start_char_idx": 43593, "end_char_idx": 43724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4df76e13-6c5a-4772-95a0-8938014d92bf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14. ", "original_text": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5136bf57-e06a-4c75-b1cf-a7b8a6cd76f7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated IF: A heatmap showing a central bright spot that is more circular than in (a), with the axis-parallel artifacts significantly reduced.\n (c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig. ", "original_text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs."}, "hash": "403990fc3b9f0f46dc3aff6f8be613087aedca0f0eb8406e077b0dbfef4c4086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e9c6b72-7297-4d77-aa03-b9e9156943d3", "node_type": "1", "metadata": {"window": "The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid.", "original_text": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n"}, "hash": "893191d4cb55c03c677dc2927885626e3d4b0c287446074b924c341f4cdc7c52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n", "mimetype": "text/plain", "start_char_idx": 43724, "end_char_idx": 43893, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e9c6b72-7297-4d77-aa03-b9e9156943d3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid.", "original_text": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4df76e13-6c5a-4772-95a0-8938014d92bf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended IF: A heatmap showing a nearly perfectly circular central bright spot with smoothly increasing anomaly scores in all radial directions.  The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14. ", "original_text": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n"}, "hash": "9c3db254239e0ce37bbd7911ca55c8194d5b801bf9716a31d36f36ffd3693f37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f609cf0b-79f2-4611-8a80-5bfc11c3818f", "node_type": "1", "metadata": {"window": "**Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n", "original_text": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. "}, "hash": "c44acdf0bcbe994fbe2c61aae325fcade2fe24c566397a3112cc8327ab2d8fe7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n", "mimetype": "text/plain", "start_char_idx": 43893, "end_char_idx": 44037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f609cf0b-79f2-4611-8a80-5bfc11c3818f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n", "original_text": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e9c6b72-7297-4d77-aa03-b9e9156943d3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The artifacts are completely gone.\n\n **Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid.", "original_text": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n"}, "hash": "a9f96fc28376d7e1faa9ac94d203dc109825a82899d9511af00937c47125dfab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aefecd81-8d46-48ef-82c9-573a45fa7555", "node_type": "1", "metadata": {"window": "13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n", "original_text": "The artifacts and \"ghost\" clusters are entirely removed.\n\n"}, "hash": "bc1db802888fefae56a710b2806531580af132de0a08df91306bb6ffaa635c5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. ", "mimetype": "text/plain", "start_char_idx": 44037, "end_char_idx": 44177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aefecd81-8d46-48ef-82c9-573a45fa7555", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n", "original_text": "The artifacts and \"ghost\" clusters are entirely removed.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f609cf0b-79f2-4611-8a80-5bfc11c3818f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n", "original_text": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score. "}, "hash": "7ca297d437c78139d9f366d68eaec3626952a03f95961cddce96d20836ce9cb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ba11006-0d5f-4dc2-af76-6dd5492736ae", "node_type": "1", "metadata": {"window": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n", "original_text": "**Fig. "}, "hash": "29e8ec2aa72d8c4debe96699b93bced5791f4a7ceb0abd3f8b0f3853ed1d0ea5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The artifacts and \"ghost\" clusters are entirely removed.\n\n", "mimetype": "text/plain", "start_char_idx": 44177, "end_char_idx": 44235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ba11006-0d5f-4dc2-af76-6dd5492736ae", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aefecd81-8d46-48ef-82c9-573a45fa7555", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "13.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n", "original_text": "The artifacts and \"ghost\" clusters are entirely removed.\n\n"}, "hash": "9d130b328aff27414274e77e74ed8b440fd17ac6d64750e02d54179a387b1172", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1169c22-d51b-4d59-9a83-f007c6017c81", "node_type": "1", "metadata": {"window": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. ", "original_text": "14. "}, "hash": "9d287d27d47be520874f800f3d1f3ef63cf8d2af1204f888fbe71fdcd6c916a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 44235, "end_char_idx": 44242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c1169c22-d51b-4d59-9a83-f007c6017c81", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. ", "original_text": "14. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ba11006-0d5f-4dc2-af76-6dd5492736ae", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of two blobs. **\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n", "original_text": "**Fig. "}, "hash": "1b2ce30a8eab772e286204aeff4192a862cf5f22dec84bd71025c4eeb3a22fc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b75b236d-f1c2-4744-bc02-ae7b3d669f00", "node_type": "1", "metadata": {"window": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. ", "original_text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid."}, "hash": "e92249a40930828186d797ffb6fc0d91e5da39f1f2a00434a14ac5fe4af03047", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14. ", "mimetype": "text/plain", "start_char_idx": 44242, "end_char_idx": 44246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b75b236d-f1c2-4744-bc02-ae7b3d669f00", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. ", "original_text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1169c22-d51b-4d59-9a83-f007c6017c81", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard IF: A heatmap showing two bright spots for the clusters, but with strong vertical and horizontal bands creating \"ghost\" clusters at their intersections.\n (b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. ", "original_text": "14. "}, "hash": "87390c7bd5250d59551ec60f88b2b6a21f216a17df5096688eb04e6e02094c56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b73bbbad-2db5-4524-8656-207338516fd8", "node_type": "1", "metadata": {"window": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n", "original_text": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n"}, "hash": "d9323d2b11e4aa44f82fbf9174c0b71a393b7862a1995c507f93bed4481e1d18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid.", "mimetype": "text/plain", "start_char_idx": 44246, "end_char_idx": 44376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b73bbbad-2db5-4524-8656-207338516fd8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n", "original_text": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b75b236d-f1c2-4744-bc02-ae7b3d669f00", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated IF: A heatmap showing the two bright spots for the clusters, with the artifactual bands and \"ghost\" clusters significantly reduced.\n (c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. ", "original_text": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid."}, "hash": "4c367aa5dc77faa72d43ec420afe59b82b687ac13a770a331afc88a668a737e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dc47d3e-f4c4-4abd-9280-4b8c5c6ca83f", "node_type": "1", "metadata": {"window": "The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig. ", "original_text": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n"}, "hash": "34a99a029dc1807d9285de57c7b4ae2bb850f508b35c5ebb840172eba67d1c4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n", "mimetype": "text/plain", "start_char_idx": 44376, "end_char_idx": 44505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2dc47d3e-f4c4-4abd-9280-4b8c5c6ca83f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig. ", "original_text": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b73bbbad-2db5-4524-8656-207338516fd8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended IF: A heatmap showing two distinct bright spots for the clusters, with the region between them having a moderately high score.  The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n", "original_text": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n"}, "hash": "4e949aa14b4c86d56631a708427583a573bab7f303e98d9774a78b3033e8db7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23f4e631-1da4-4eda-8a0c-1e9a9098f32f", "node_type": "1", "metadata": {"window": "**Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a. ", "original_text": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n"}, "hash": "6ddf24cff8e79ff3b5806cb7a045bde0e5df2f5a1b48298a09d2a8d7846da08d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n", "mimetype": "text/plain", "start_char_idx": 44505, "end_char_idx": 44634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23f4e631-1da4-4eda-8a0c-1e9a9098f32f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a. ", "original_text": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dc47d3e-f4c4-4abd-9280-4b8c5c6ca83f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The artifacts and \"ghost\" clusters are entirely removed.\n\n **Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig. ", "original_text": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n"}, "hash": "86a460485c34237148176457228d7bca7ff9a6c375cb61806904354059b02df7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6cf22a7-1576-4434-bdad-beaf392de22e", "node_type": "1", "metadata": {"window": "14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case. ", "original_text": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. "}, "hash": "97a44c19512361936e897d171b8a8337f3ff0a67c2aadbff2695325e9c88f611", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n", "mimetype": "text/plain", "start_char_idx": 44634, "end_char_idx": 44766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6cf22a7-1576-4434-bdad-beaf392de22e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case. ", "original_text": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23f4e631-1da4-4eda-8a0c-1e9a9098f32f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a. ", "original_text": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n"}, "hash": "2919130c37cdaaf9f74d9b9f41d6f8a546ac088b1e5cf61bf7b945d95ac6c726", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "834b7e26-d2b2-46f6-b44a-3e06d3f48a4b", "node_type": "1", "metadata": {"window": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles. ", "original_text": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. "}, "hash": "96db0b488c8a6aa5b8b162145ddb18b2218994a6860d8ef97c2232a8259cc81a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. ", "mimetype": "text/plain", "start_char_idx": 44766, "end_char_idx": 44957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "834b7e26-d2b2-46f6-b44a-3e06d3f48a4b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles. ", "original_text": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6cf22a7-1576-4434-bdad-beaf392de22e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14.  Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case. ", "original_text": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one. "}, "hash": "bac6140aca7003b8a599ae93651c81cd2e03f110d83e86c7e97b6f7784bf1254", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "658c811b-b3b6-4523-bdc5-bbd1312624fc", "node_type": "1", "metadata": {"window": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. ", "original_text": "The advantage of this metric is that we can test this for higher dimensional data.\n\n"}, "hash": "0c5494db79313122f42367a18e327c591cc2ca40fe8c58d03546105657e3f1a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. ", "mimetype": "text/plain", "start_char_idx": 44957, "end_char_idx": 45107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "658c811b-b3b6-4523-bdc5-bbd1312624fc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. ", "original_text": "The advantage of this metric is that we can test this for higher dimensional data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "834b7e26-d2b2-46f6-b44a-3e06d3f48a4b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Comparison of the standard Isolation Forest with rotated Isolation Forest, and Extended Isolation Forest for the case of sinusoid. **\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles. ", "original_text": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases. "}, "hash": "091285e8c02fad7f87feaa4dc1dc3d426c6311eeac6de8ea872876cb5244531c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a21b9eb3-95b1-45e4-93fe-bec72536e5aa", "node_type": "1", "metadata": {"window": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. ", "original_text": "Consider our randomly distributed data in two dimensions shown in Fig. "}, "hash": "ff5f545b4a26553f6cb185f6f60a64723fd604211cd73ed40bb93274605c1991", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The advantage of this metric is that we can test this for higher dimensional data.\n\n", "mimetype": "text/plain", "start_char_idx": 45107, "end_char_idx": 45191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a21b9eb3-95b1-45e4-93fe-bec72536e5aa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. ", "original_text": "Consider our randomly distributed data in two dimensions shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "658c811b-b3b6-4523-bdc5-bbd1312624fc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Generic IF: A heatmap showing the data treated as a single rectangular blob, failing to capture the sinusoidal structure.\n (b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. ", "original_text": "The advantage of this metric is that we can test this for higher dimensional data.\n\n"}, "hash": "27d4e5e44b9613cf3b7f1f2a66107af1a1a4ed64f0069d2beed11677f860bfe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31295a43-e2ac-47a1-a196-bc0ee36dc295", "node_type": "1", "metadata": {"window": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n", "original_text": "15a. "}, "hash": "cc4e4d20853c4782f2c8708cf6f59b46e14f276b3f5e8c2b2dc5cfbe270cd21c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Consider our randomly distributed data in two dimensions shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 45191, "end_char_idx": 45262, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31295a43-e2ac-47a1-a196-bc0ee36dc295", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n", "original_text": "15a. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a21b9eb3-95b1-45e4-93fe-bec72536e5aa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated IF: A heatmap that begins to show the sinusoidal structure of the data, though the representation is somewhat broad.\n (c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. ", "original_text": "Consider our randomly distributed data in two dimensions shown in Fig. "}, "hash": "76bd95aa764776df1227bb826d2e5fc1ffbc7759ffdd919b9688a174fa344e7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "750ae4c5-1342-41fa-8bcb-0daa4c89d385", "node_type": "1", "metadata": {"window": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig. ", "original_text": "The blue dots represent the training data, used to create the forest in each case. "}, "hash": "c3cf5c5bb9f7752b3c0e1188fffb131ad8063f40aef4453cf12437d243670a79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15a. ", "mimetype": "text/plain", "start_char_idx": 45262, "end_char_idx": 45267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "750ae4c5-1342-41fa-8bcb-0daa4c89d385", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig. ", "original_text": "The blue dots represent the training data, used to create the forest in each case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31295a43-e2ac-47a1-a196-bc0ee36dc295", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended IF: A heatmap that clearly and tightly follows the sinusoidal path of the data, accurately representing its structure.\n ***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n", "original_text": "15a. "}, "hash": "e45db2a9b539cc6e4b0f88f80d0b32108455c83879e5905f37bb6b1da8c5b087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87bd30e5-c993-4df3-9c93-88891c795c6d", "node_type": "1", "metadata": {"window": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods. ", "original_text": "The gray dots form concentric circles. "}, "hash": "17dcbbd5258f496549f0cdde3bb6f75cfb781bc15edaf566c6675f80502c1dec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue dots represent the training data, used to create the forest in each case. ", "mimetype": "text/plain", "start_char_idx": 45267, "end_char_idx": 45350, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "87bd30e5-c993-4df3-9c93-88891c795c6d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods. ", "original_text": "The gray dots form concentric circles. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "750ae4c5-1342-41fa-8bcb-0daa4c89d385", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\n### 4.2 Variance of the Anomaly Scores\nBesides looking at anomaly score maps, we can consider some other metrics in order to compare the Extended Isolation Forest with the standard one.  One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig. ", "original_text": "The blue dots represent the training data, used to create the forest in each case. "}, "hash": "698993fb0a35c44731eb8065bc542c24a9b2791a16fda041d8e16fb0f7a58b64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d22a846d-510a-4a17-aa0c-ed71b67336f3", "node_type": "1", "metadata": {"window": "The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. ", "original_text": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. "}, "hash": "9390b11f6fb603804868d4d908c110e3dbeced9b09af574f5ce44faf783c1768", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The gray dots form concentric circles. ", "mimetype": "text/plain", "start_char_idx": 45350, "end_char_idx": 45389, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d22a846d-510a-4a17-aa0c-ed71b67336f3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. ", "original_text": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87bd30e5-c993-4df3-9c93-88891c795c6d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "One such metric is looking at the mean and variance of the anomaly scores of points distributed along roughly constant score lines for various cases.  The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods. ", "original_text": "The gray dots form concentric circles. "}, "hash": "19523c259ccf3b6dd2cf50ee68249d48679d4f791f01481d5fd03b6bf20cf7d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "952fa544-c139-4aa5-8316-d7e3108a95a6", "node_type": "1", "metadata": {"window": "Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n", "original_text": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. "}, "hash": "1734ad8189f615788c33191b2f78f6b0f0e3743da1ba2315ff7b76587e37c3d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. ", "mimetype": "text/plain", "start_char_idx": 45389, "end_char_idx": 45501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "952fa544-c139-4aa5-8316-d7e3108a95a6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n", "original_text": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d22a846d-510a-4a17-aa0c-ed71b67336f3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The advantage of this metric is that we can test this for higher dimensional data.\n\n Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. ", "original_text": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant. "}, "hash": "efd59025444f554fd3abbf45e81f93f01561ad02bdf0c36acdbb0249a51e3d3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b717dfcf-d6b9-4c09-8c00-db28cc7c9c1d", "node_type": "1", "metadata": {"window": "15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs. ", "original_text": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n"}, "hash": "c7cae0bc52dd4be0b08c8eda067431a887fb1867783c3ea1ba39f4d244fd34d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. ", "mimetype": "text/plain", "start_char_idx": 45501, "end_char_idx": 45632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b717dfcf-d6b9-4c09-8c00-db28cc7c9c1d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs. ", "original_text": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "952fa544-c139-4aa5-8316-d7e3108a95a6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Consider our randomly distributed data in two dimensions shown in Fig.  15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n", "original_text": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled. "}, "hash": "0f2a3c05d241401999a9a534cebe454b6baa3ea18effcdd9639f2b9d3ae76b08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68f533c2-12f4-4354-932b-1ca176c15811", "node_type": "1", "metadata": {"window": "The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. ", "original_text": "From Fig. "}, "hash": "e44366dc08df14e8663e6ad23692f78474476d414c238878d19c084ccd37ee58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n", "mimetype": "text/plain", "start_char_idx": 45632, "end_char_idx": 45849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "68f533c2-12f4-4354-932b-1ca176c15811", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. ", "original_text": "From Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b717dfcf-d6b9-4c09-8c00-db28cc7c9c1d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15a.  The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs. ", "original_text": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n"}, "hash": "54151aa9c0fe0eac0f505294dcb945b0bce22574d3b0e01cdd40d378a7bde732", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2876c7c-30e0-4f7b-b306-1f607ff75ea5", "node_type": "1", "metadata": {"window": "The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well. ", "original_text": "15b we can see that the mean score varies the same way among the three methods. "}, "hash": "fd3ee1db8374b10a77e096ba601634c9a24beb761e8f00e7958d7f19d88300dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From Fig. ", "mimetype": "text/plain", "start_char_idx": 45849, "end_char_idx": 45859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2876c7c-30e0-4f7b-b306-1f607ff75ea5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well. ", "original_text": "15b we can see that the mean score varies the same way among the three methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68f533c2-12f4-4354-932b-1ca176c15811", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The blue dots represent the training data, used to create the forest in each case.  The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. ", "original_text": "From Fig. "}, "hash": "f6e83d1698def8f87fba8a3f8dbf8454eec61cbc94ad6a8c17c9fb0e183c9dc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a74a527-3d0b-459f-8b58-21d3433c2ec8", "node_type": "1", "metadata": {"window": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms. ", "original_text": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. "}, "hash": "ab8992cf3d1351c64b11d463a2818c84cf8aec7742feb4f33182907e1a4a53b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15b we can see that the mean score varies the same way among the three methods. ", "mimetype": "text/plain", "start_char_idx": 45859, "end_char_idx": 45939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a74a527-3d0b-459f-8b58-21d3433c2ec8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms. ", "original_text": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2876c7c-30e0-4f7b-b306-1f607ff75ea5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The gray dots form concentric circles.  Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well. ", "original_text": "15b we can see that the mean score varies the same way among the three methods. "}, "hash": "df1aee88e960d93034b62ace3ebacc39d8b3376cf75d1d7b33c2f5edc6658aed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b077262c-3425-46b1-81d7-34bbfbd8fbf9", "node_type": "1", "metadata": {"window": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. ", "original_text": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n"}, "hash": "4607dbe2d781cc151086fed52dbf0f2ad883b09d846177f4874293a3f175be39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. ", "mimetype": "text/plain", "start_char_idx": 45939, "end_char_idx": 46027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b077262c-3425-46b1-81d7-34bbfbd8fbf9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. ", "original_text": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a74a527-3d0b-459f-8b58-21d3433c2ec8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since our data is normally distributed, anomaly scores along each circle should remain more or less a constant.  The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms. ", "original_text": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing. "}, "hash": "dbae9bfc569400d9781a9f990b3a318d6767b66da57e737ed69c5022275a666e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fdf58dc-1e01-42cb-8fb6-65e59079b326", "node_type": "1", "metadata": {"window": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. ", "original_text": "As far as the variance of the scores goes, looking at Figs. "}, "hash": "071675463c2f3a456f6869dca5597bda9ce26dd2cf57a2dec733e2a409c90d7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n", "mimetype": "text/plain", "start_char_idx": 46027, "end_char_idx": 46162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8fdf58dc-1e01-42cb-8fb6-65e59079b326", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. ", "original_text": "As far as the variance of the scores goes, looking at Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b077262c-3425-46b1-81d7-34bbfbd8fbf9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The red lines represent the circles at 1\u03c3, 2\u03c3, and 3\u03c3 of the original normal distribution from which the training data is sampled.  We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. ", "original_text": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n"}, "hash": "90302545b8463ba7f9c3893ded88fca49b46ad6a1c35f4e519453a854563a8ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e3a32dc-ac74-4e94-bf76-6d3710e19df4", "node_type": "1", "metadata": {"window": "From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. ", "original_text": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. "}, "hash": "f8565112dde2b454d56f19839c975d8212af7f2bca5aa0aae053fb37d9e3cc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As far as the variance of the scores goes, looking at Figs. ", "mimetype": "text/plain", "start_char_idx": 46162, "end_char_idx": 46222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e3a32dc-ac74-4e94-bf76-6d3710e19df4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. ", "original_text": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fdf58dc-1e01-42cb-8fb6-65e59079b326", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We will run the data from the gray circles through our trained Isolation Forest for the standard, the rotated and the extended cases, and look at the variation of the score as a function of distance from the center.\n\n From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. ", "original_text": "As far as the variance of the scores goes, looking at Figs. "}, "hash": "10a6d0250ca1fae7fb629a4f22f298e88e9650f006b5da7402bcadf557644a35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f9834df-ba72-40c3-8478-f1c10188c97a", "node_type": "1", "metadata": {"window": "15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n", "original_text": "All three methods are able to detect the center of concentration of data quite well. "}, "hash": "b8483c1b82c2e12e1e9665442f889ffb28eff76beb262e5c83593ca9aafa572b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. ", "mimetype": "text/plain", "start_char_idx": 46222, "end_char_idx": 46358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f9834df-ba72-40c3-8478-f1c10188c97a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n", "original_text": "All three methods are able to detect the center of concentration of data quite well. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e3a32dc-ac74-4e94-bf76-6d3710e19df4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "From Fig.  15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. ", "original_text": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases. "}, "hash": "e47380e6ac75522134143970c63f23fb9e3e4de1ab5ffbbb49e48e3d50dffd0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc31b608-735e-476f-8004-8c59df804c30", "node_type": "1", "metadata": {"window": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. ", "original_text": "This however, is not the goal for anomaly detection algorithms. "}, "hash": "50cc06f240dc5cecd89169327b358cef22e3a64037acca7226d6f526af48c9ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All three methods are able to detect the center of concentration of data quite well. ", "mimetype": "text/plain", "start_char_idx": 46358, "end_char_idx": 46443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc31b608-735e-476f-8004-8c59df804c30", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. ", "original_text": "This however, is not the goal for anomaly detection algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f9834df-ba72-40c3-8478-f1c10188c97a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15b we can see that the mean score varies the same way among the three methods.  It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n", "original_text": "All three methods are able to detect the center of concentration of data quite well. "}, "hash": "5999fc43cc7a2bebc19d148a10d147f9b9821ed06daeff625822746fe4129e63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1ddbbc7-9487-4fb9-9b2a-122511abe709", "node_type": "1", "metadata": {"window": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a. ", "original_text": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. "}, "hash": "2dad2773f6541c55e5202697b88a910a341a3eec6b0f8cb9df3db7d852db7d90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This however, is not the goal for anomaly detection algorithms. ", "mimetype": "text/plain", "start_char_idx": 46443, "end_char_idx": 46507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1ddbbc7-9487-4fb9-9b2a-122511abe709", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a. ", "original_text": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc31b608-735e-476f-8004-8c59df804c30", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It increases quickly within 3\u03c3 from the center and then keeps monotonically increasing.  At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. ", "original_text": "This however, is not the goal for anomaly detection algorithms. "}, "hash": "7900d79fa668d2d105946065adef58a6d353833f816abac14746c63a36516fad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62b9b0fa-468a-4c8a-a326-09fe8cc467ae", "node_type": "1", "metadata": {"window": "As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same. ", "original_text": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. "}, "hash": "398a073c8b61f51f8326841bc3aaa884a0c40710b7ac040548b6f7be72b6e51f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. ", "mimetype": "text/plain", "start_char_idx": 46507, "end_char_idx": 46775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62b9b0fa-468a-4c8a-a326-09fe8cc467ae", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same. ", "original_text": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1ddbbc7-9487-4fb9-9b2a-122511abe709", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "At higher values the rate of increase slows down until it converges far away from the center where the blob resembles a single point.\n\n As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a. ", "original_text": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement. "}, "hash": "d85e7ec6eacdf22d754b8ee006b677ca57f2510ea78d4a04fde5bd57165a3ce4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1b8b3e3-35d1-4b2b-9d14-e4c788c6f7c6", "node_type": "1", "metadata": {"window": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true. ", "original_text": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. "}, "hash": "333f2959eb108ab119136977b3b23d80530bceac483398fdad012182e3ab304a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. ", "mimetype": "text/plain", "start_char_idx": 46775, "end_char_idx": 46908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1b8b3e3-35d1-4b2b-9d14-e4c788c6f7c6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true. ", "original_text": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62b9b0fa-468a-4c8a-a326-09fe8cc467ae", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As far as the variance of the scores goes, looking at Figs.  12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same. ", "original_text": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm. "}, "hash": "85c552730f1d0d7c0b609993bb2fb261533cd6ed15bc7179b7652b648bf83b21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84147df-867f-4ffb-8b21-9cde1f2c05bf", "node_type": "1", "metadata": {"window": "All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. ", "original_text": "This increases the chances of false positives in scoring stage.\n\n"}, "hash": "7ffb948ea58608c041d179ec47292be142f0636ddcbb702759c7ae33a3e2ac61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. ", "mimetype": "text/plain", "start_char_idx": 46908, "end_char_idx": 47179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b84147df-867f-4ffb-8b21-9cde1f2c05bf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. ", "original_text": "This increases the chances of false positives in scoring stage.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1b8b3e3-35d1-4b2b-9d14-e4c788c6f7c6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "12 and 15c, we can see for very small anomaly scores (close to the center of the blob), the variance is quite small in all three cases.  All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true. ", "original_text": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not. "}, "hash": "b3384df8cc13f09bd19a06f23f64ab09e73ef4166f0c190e596826a3130a567a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dca01dbe-c917-4ce3-b68d-f4ee7fd47e57", "node_type": "1", "metadata": {"window": "This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. ", "original_text": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. "}, "hash": "8d228a480e822370239708036bc28f70c9a671b67584aefd5d20553351f8be19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This increases the chances of false positives in scoring stage.\n\n", "mimetype": "text/plain", "start_char_idx": 47179, "end_char_idx": 47244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dca01dbe-c917-4ce3-b68d-f4ee7fd47e57", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. ", "original_text": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84147df-867f-4ffb-8b21-9cde1f2c05bf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "All three methods are able to detect the center of concentration of data quite well.  This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. ", "original_text": "This increases the chances of false positives in scoring stage.\n\n"}, "hash": "e77e8e599bde81cbf9151e69b8feac3df6b6d1e276fdb7506c6472f003fadef0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a49312e-3ae1-4537-a5b7-96617c2c4bee", "node_type": "1", "metadata": {"window": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n", "original_text": "16a. "}, "hash": "885fa8bfd70c29be101455dc89164b08b9419b78cd907b9b927e00305f64b42a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 47244, "end_char_idx": 47392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a49312e-3ae1-4537-a5b7-96617c2c4bee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n", "original_text": "16a. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dca01dbe-c917-4ce3-b68d-f4ee7fd47e57", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This however, is not the goal for anomaly detection algorithms.  After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. ", "original_text": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig. "}, "hash": "ab5904430c197744f5c3b4a96bc8172a572e2fe9bee25a113db0d41d8bc8593a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c96579c-9d67-43c5-8b37-d13117530bf6", "node_type": "1", "metadata": {"window": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig. ", "original_text": "We assume data lying on each line to be scored more or less the same. "}, "hash": "b92f25259b9b00acac63434aaa767d749c810e3e644e6c059a11f56477673a47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16a. ", "mimetype": "text/plain", "start_char_idx": 47392, "end_char_idx": 47397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c96579c-9d67-43c5-8b37-d13117530bf6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig. ", "original_text": "We assume data lying on each line to be scored more or less the same. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a49312e-3ae1-4537-a5b7-96617c2c4bee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After about 3\u03c3, the variance among the scores computed by the Extended Isolation Forest, and the case of Isolation Forest with rotated trees, settle to a much smaller value compared to the standard Isolation Forest, which translates to a much more robust measurement.  It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n", "original_text": "16a. "}, "hash": "b3da00fa697f285da5a49b1155c314195a698d53eeadc4f0913126bc27c2e219", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45e67e16-62ae-4153-9ec8-343ec3bedaa2", "node_type": "1", "metadata": {"window": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16. ", "original_text": "This is however not quite true. "}, "hash": "a099626f1974915b18946c852170fe95067a4d1bcf3e10c6ec6bb181edc94a12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We assume data lying on each line to be scored more or less the same. ", "mimetype": "text/plain", "start_char_idx": 47397, "end_char_idx": 47467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45e67e16-62ae-4153-9ec8-343ec3bedaa2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16. ", "original_text": "This is however not quite true. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c96579c-9d67-43c5-8b37-d13117530bf6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It is notable that this inconsistency occurs in regions of high anomaly, which is very important for an anomaly detection algorithm.  In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig. ", "original_text": "We assume data lying on each line to be scored more or less the same. "}, "hash": "6985230b20bc2e784fe8f3cdd3b0a9ea8db3d136d1f5862e0cd4eaff7a0575d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdf923ec-0f41-42be-a4cb-810ae5459186", "node_type": "1", "metadata": {"window": "This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. ", "original_text": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. "}, "hash": "ddfcc2d3c93c1a36c8f902e977989a419939240b89484502eb790393204182b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is however not quite true. ", "mimetype": "text/plain", "start_char_idx": 47467, "end_char_idx": 47499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdf923ec-0f41-42be-a4cb-810ae5459186", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. ", "original_text": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45e67e16-62ae-4153-9ec8-343ec3bedaa2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In short, for data in regions of high anomaly likelihood, the standard Isolation Forest produces anomaly scores which vary largely depending on alignment of these data points with the training data and the axes, rather than whether they are true anomalous points or not.  This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16. ", "original_text": "This is however not quite true. "}, "hash": "9852823eddcd14029c97538e51aeab9cbedde942d2af9d9ca050c500b5ea0cca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c551b9f-3aa4-4e13-b36e-15186860eb7d", "node_type": "1", "metadata": {"window": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data. ", "original_text": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. "}, "hash": "20ca5a3724302f939f6b58fb8aafeac3cd27eadbcc6742b0528a1432a86eceb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. ", "mimetype": "text/plain", "start_char_idx": 47499, "end_char_idx": 47647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c551b9f-3aa4-4e13-b36e-15186860eb7d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data. ", "original_text": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdf923ec-0f41-42be-a4cb-810ae5459186", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This increases the chances of false positives in scoring stage.\n\n We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. ", "original_text": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true. "}, "hash": "9b85458730591bccd5964f34ebfbc5b37bac76be5f7264ba457d2e570bc009a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "971aad00-ab1b-4680-b0c1-482d6106de8e", "node_type": "1", "metadata": {"window": "16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage. ", "original_text": "14c.\n\n"}, "hash": "2faecaa75bf6333f085aa5c5546628b386b97b18179fba2d552110d4b57eb28b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 47647, "end_char_idx": 47794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "971aad00-ab1b-4680-b0c1-482d6106de8e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage. ", "original_text": "14c.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c551b9f-3aa4-4e13-b36e-15186860eb7d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can do a similar study for the sine curve by considering lines at a similar distances above and below where the data points lie as shown in Fig.  16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data. ", "original_text": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig. "}, "hash": "0419ba101ec07c0f60160bfb27e3818ac43787f48cec77a80351cef7e653904c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5008b535-e254-4524-9cf4-d764e142cf47", "node_type": "1", "metadata": {"window": "We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training. ", "original_text": "Nonetheless we performed the analysis as shown in Fig. "}, "hash": "e41fb27911b1dfa72abcbd2d93143e1f6b8e3d0731db845545e5c76fd522521d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14c.\n\n", "mimetype": "text/plain", "start_char_idx": 47794, "end_char_idx": 47800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5008b535-e254-4524-9cf4-d764e142cf47", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training. ", "original_text": "Nonetheless we performed the analysis as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "971aad00-ab1b-4680-b0c1-482d6106de8e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16a.  We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage. ", "original_text": "14c.\n\n"}, "hash": "7d9dd442279df81226e27418a483533c22e9639b9342e3cdf65609eff2b680d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92ac0cfe-247d-42e3-b904-7a29dcdccd2c", "node_type": "1", "metadata": {"window": "This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig. ", "original_text": "16. "}, "hash": "3b222a25baeaa4adb485529721b4dd8d1f551b92fad1a177d3f1d3bbf8547e4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nonetheless we performed the analysis as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 47800, "end_char_idx": 47855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92ac0cfe-247d-42e3-b904-7a29dcdccd2c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig. ", "original_text": "16. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5008b535-e254-4524-9cf4-d764e142cf47", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We assume data lying on each line to be scored more or less the same.  This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training. ", "original_text": "Nonetheless we performed the analysis as shown in Fig. "}, "hash": "c95f2b12154de69815214b2dd80e5440248d6d58f01430336ed7871300a4092a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b986512c-b839-4d95-b0b0-dcfd721df85f", "node_type": "1", "metadata": {"window": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction. ", "original_text": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. "}, "hash": "2b65145cc3341c0c7fd71330bba0377d81202c0058b2646a49c36aedffad5b28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16. ", "mimetype": "text/plain", "start_char_idx": 47855, "end_char_idx": 47859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b986512c-b839-4d95-b0b0-dcfd721df85f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction. ", "original_text": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92ac0cfe-247d-42e3-b904-7a29dcdccd2c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is however not quite true.  If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig. ", "original_text": "16. "}, "hash": "d95d2e1a03c7fcef08999b4d7d272d4d8bf991c6ea8d0f9cbbb27c59c46859a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5f0d81a-5e53-4487-a08d-908f26cf5ee8", "node_type": "1", "metadata": {"window": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently. ", "original_text": "The gray lines represent data points that lie at constant distance from the center of the data. "}, "hash": "e6b866e0a0d1ca0faa9526b730af4c1643582c83c6f4beec8673ec3c828cb0d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. ", "mimetype": "text/plain", "start_char_idx": 47859, "end_char_idx": 48022, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5f0d81a-5e53-4487-a08d-908f26cf5ee8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently. ", "original_text": "The gray lines represent data points that lie at constant distance from the center of the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b986512c-b839-4d95-b0b0-dcfd721df85f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "If the data stretched to infinity in the horizontal direction, or we had periodic boundary conditions in that direction, this would indeed be true.  However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction. ", "original_text": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve. "}, "hash": "522d5fa2eb0cb6d9cd2ca29e63695642588f274e6c7fc5c0dee2e6271c998c74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "965ea872-c301-4dfd-a376-634a6543584b", "node_type": "1", "metadata": {"window": "14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly. ", "original_text": "These are points that are used in the scoring stage. "}, "hash": "b9473d3cdf358bf0ed344a25331118fbb9f0ae4543b3de37b3bd4fd24c5b4a7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The gray lines represent data points that lie at constant distance from the center of the data. ", "mimetype": "text/plain", "start_char_idx": 48022, "end_char_idx": 48118, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "965ea872-c301-4dfd-a376-634a6543584b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly. ", "original_text": "These are points that are used in the scoring stage. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5f0d81a-5e53-4487-a08d-908f26cf5ee8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, since our data is bounded in the horizontal direction, there will be variations in the scores along each line as in the map shown in Fig.  14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently. ", "original_text": "The gray lines represent data points that lie at constant distance from the center of the data. "}, "hash": "94bcb7d25db23044a1b1f243a141a870edff3ced83f0e51ef00a1709807238b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c460d21-35e5-4f52-ba2f-541e9264927c", "node_type": "1", "metadata": {"window": "Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig. ", "original_text": "The blue data points are used for training. "}, "hash": "abc6c010b21ea54e4cffbdc714168bb17b45bb5b1e49e0df631d9dc476b06f43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These are points that are used in the scoring stage. ", "mimetype": "text/plain", "start_char_idx": 48118, "end_char_idx": 48171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c460d21-35e5-4f52-ba2f-541e9264927c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig. ", "original_text": "The blue data points are used for training. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "965ea872-c301-4dfd-a376-634a6543584b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14c.\n\n Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly. ", "original_text": "These are points that are used in the scoring stage. "}, "hash": "5f6d67f1dd5c2fb667f2e9db8ffd93c16bc4ba27198fb8043ddf60ab0b78f869", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d1d432d-9061-40ea-8c1e-99a7c38135dd", "node_type": "1", "metadata": {"window": "16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a. ", "original_text": "Fig. "}, "hash": "64fe2d66aa0bf55882462021e3a3453f8635dd5480bcaaee3f74e72179c52065", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue data points are used for training. ", "mimetype": "text/plain", "start_char_idx": 48171, "end_char_idx": 48215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d1d432d-9061-40ea-8c1e-99a7c38135dd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c460d21-35e5-4f52-ba2f-541e9264927c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Nonetheless we performed the analysis as shown in Fig.  16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig. ", "original_text": "The blue data points are used for training. "}, "hash": "037b24b7cb4994bbfa79c7c0c7f0747071a601fb532a64f85c99d8ce04832653", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c66a14d8-9972-4aa2-92c3-08933f52214d", "node_type": "1", "metadata": {"window": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. ", "original_text": "16b shows the change in the mean score as we move out on both sides in the y direction. "}, "hash": "12023063e9138297a8130e7cb2ac9cb15516a8e82f933539f7623fca0359f37e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 48215, "end_char_idx": 48220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c66a14d8-9972-4aa2-92c3-08933f52214d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. ", "original_text": "16b shows the change in the mean score as we move out on both sides in the y direction. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d1d432d-9061-40ea-8c1e-99a7c38135dd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16.  Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a. ", "original_text": "Fig. "}, "hash": "1330de320a15de68f8bde14e2b3da913ad0bcf9e9d1b076fea15548a6e2e57f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc40c330-b0cd-401a-a874-d67b3d06d3e6", "node_type": "1", "metadata": {"window": "The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. ", "original_text": "We observe that in this case, the mean scores behave differently. "}, "hash": "aaefa4997a2dd3c327abf47d721ac97241c5dbe4ab1c20705a00243488af4279", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16b shows the change in the mean score as we move out on both sides in the y direction. ", "mimetype": "text/plain", "start_char_idx": 48220, "end_char_idx": 48308, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc40c330-b0cd-401a-a874-d67b3d06d3e6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. ", "original_text": "We observe that in this case, the mean scores behave differently. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c66a14d8-9972-4aa2-92c3-08933f52214d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Similarly to the previous plots, the three red lines on each side of the center line correspond to 1\u03c3, 2\u03c3, and 3\u03c3 from the Gaussian noise added to the sine curve.  The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. ", "original_text": "16b shows the change in the mean score as we move out on both sides in the y direction. "}, "hash": "1ec13771b7f3a753b843e36afe63d30448cd8966a4b08057e913d5e1052d1181", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a32cbbe-8cfe-40ae-9caa-cdfdcae9d0d2", "node_type": "1", "metadata": {"window": "These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest. ", "original_text": "The mean score for the standard Isolation forest reaches a constant value very quickly. "}, "hash": "ff6e5650572b4783f80a3021e3f24176742da41bae677523d063c249451d91a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We observe that in this case, the mean scores behave differently. ", "mimetype": "text/plain", "start_char_idx": 48308, "end_char_idx": 48374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a32cbbe-8cfe-40ae-9caa-cdfdcae9d0d2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest. ", "original_text": "The mean score for the standard Isolation forest reaches a constant value very quickly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc40c330-b0cd-401a-a874-d67b3d06d3e6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The gray lines represent data points that lie at constant distance from the center of the data.  These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. ", "original_text": "We observe that in this case, the mean scores behave differently. "}, "hash": "ab80e21fa23d65c9645768f974071ca680f75b483d9337545f4a980993821c47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77627672-cca0-4a02-9544-e7fd01490059", "node_type": "1", "metadata": {"window": "The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable. ", "original_text": "This is consistent with what we observed in the score maps, Fig. "}, "hash": "e1d1d6ecb799efc8810dcaa029c33c6f435ee3d78f89f932ab32f00734932999", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mean score for the standard Isolation forest reaches a constant value very quickly. ", "mimetype": "text/plain", "start_char_idx": 48374, "end_char_idx": 48462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77627672-cca0-4a02-9544-e7fd01490059", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable. ", "original_text": "This is consistent with what we observed in the score maps, Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a32cbbe-8cfe-40ae-9caa-cdfdcae9d0d2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These are points that are used in the scoring stage.  The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest. ", "original_text": "The mean score for the standard Isolation forest reaches a constant value very quickly. "}, "hash": "ee2605cf182a33a6c7c7d2e96ad3c4986aae02ffed2b54ee8242ce06e199e870", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80cecc40-80d6-4814-8b5a-ce1f483e4168", "node_type": "1", "metadata": {"window": "Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. ", "original_text": "14a. "}, "hash": "e1aebfdee36d283bfd0d0e99c25a03e78828feb258a291d936966d2aaf78e82b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is consistent with what we observed in the score maps, Fig. ", "mimetype": "text/plain", "start_char_idx": 48462, "end_char_idx": 48527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80cecc40-80d6-4814-8b5a-ce1f483e4168", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. ", "original_text": "14a. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77627672-cca0-4a02-9544-e7fd01490059", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The blue data points are used for training.  Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable. ", "original_text": "This is consistent with what we observed in the score maps, Fig. "}, "hash": "6654dd45a1ea414857af32a11b3a305180b7e78c789682b24195989ea09d7a07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04dbc094-c7b3-4704-a906-bf8c29c869dd", "node_type": "1", "metadata": {"window": "16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n", "original_text": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. "}, "hash": "16262490649330623ff4f6a4012e5766e83473007797e9772b02e8e738764881", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14a. ", "mimetype": "text/plain", "start_char_idx": 48527, "end_char_idx": 48532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04dbc094-c7b3-4704-a906-bf8c29c869dd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n", "original_text": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80cecc40-80d6-4814-8b5a-ce1f483e4168", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. ", "original_text": "14a. "}, "hash": "bf561ddc4971b9138b43f54a6f15b1b34cf2c885c840faf88e40167df4d17180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31a421d3-b6d0-46d8-bc0c-2a201b788fc7", "node_type": "1", "metadata": {"window": "We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig. ", "original_text": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. "}, "hash": "1b0a8ba8535825cdb5ee083f2a2cbcb25a5c48a91f53b8353883f7d93e7a56da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. ", "mimetype": "text/plain", "start_char_idx": 48532, "end_char_idx": 48673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31a421d3-b6d0-46d8-bc0c-2a201b788fc7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig. ", "original_text": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04dbc094-c7b3-4704-a906-bf8c29c869dd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16b shows the change in the mean score as we move out on both sides in the y direction.  We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n", "original_text": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob. "}, "hash": "87e714823fbcc864ef6be5ff817f224ebe80aec9e97477354d92d35d48ee84f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13c1fe96-1ca2-4f76-ab9d-e5a0188da1cb", "node_type": "1", "metadata": {"window": "The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15. ", "original_text": "Outside this region, all data points look the same to the standard Isolation Forest. "}, "hash": "fd7b12584c0874179dd72bf87eb6afb5e14a40c872e15a9b8193cd316cab3f14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. ", "mimetype": "text/plain", "start_char_idx": 48673, "end_char_idx": 48821, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13c1fe96-1ca2-4f76-ab9d-e5a0188da1cb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15. ", "original_text": "Outside this region, all data points look the same to the standard Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31a421d3-b6d0-46d8-bc0c-2a201b788fc7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We observe that in this case, the mean scores behave differently.  The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig. ", "original_text": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area. "}, "hash": "8dade5a59dccc1618ef4f65dc6df5b299a6943df2e60e641ff36dc1853d66832", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a089757-1f41-420d-a3c0-dfd3a858cfde", "node_type": "1", "metadata": {"window": "This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center.", "original_text": "This rapid rise is undesirable. "}, "hash": "978c45a00b6dc279ab754b8f886de85c11d7b701f99b4b8c19a027ec02abd7b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Outside this region, all data points look the same to the standard Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 48821, "end_char_idx": 48906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1a089757-1f41-420d-a3c0-dfd3a858cfde", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center.", "original_text": "This rapid rise is undesirable. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13c1fe96-1ca2-4f76-ab9d-e5a0188da1cb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The mean score for the standard Isolation forest reaches a constant value very quickly.  This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15. ", "original_text": "Outside this region, all data points look the same to the standard Isolation Forest. "}, "hash": "1a10df389f3b61a3e646cc26e9a066e708c16c9aac17e12fd5225fc7b0774676", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eec6130c-836c-495e-a7a7-d468960619d5", "node_type": "1", "metadata": {"window": "14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data. ", "original_text": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. "}, "hash": "cea201aa185fec32908f9d627a0b1ac987bd0e4cb70fe60ecb72b3b338fe4f14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This rapid rise is undesirable. ", "mimetype": "text/plain", "start_char_idx": 48906, "end_char_idx": 48938, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eec6130c-836c-495e-a7a7-d468960619d5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data. ", "original_text": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a089757-1f41-420d-a3c0-dfd3a858cfde", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This is consistent with what we observed in the score maps, Fig.  14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center.", "original_text": "This rapid rise is undesirable. "}, "hash": "ba55a9e7be97ba93376ffcfe79cca1dd3347ebc87addcfe59e87e00cafe4b54a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a53b9af-f016-4fb1-859a-bafe3b1b2bb2", "node_type": "1", "metadata": {"window": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n", "original_text": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n"}, "hash": "a6d68b2d3ab25e24da6217db60d505626599fd3add6797277d6c0a4a53051198", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. ", "mimetype": "text/plain", "start_char_idx": 48938, "end_char_idx": 49126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a53b9af-f016-4fb1-859a-bafe3b1b2bb2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n", "original_text": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eec6130c-836c-495e-a7a7-d468960619d5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "14a.  We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data. ", "original_text": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below. "}, "hash": "a01e59f1117aae6d4a9b31cb3016353534619b728bb1e869091063b2373a2eec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b57779c-f1be-4bf8-a6f8-f0f4567eb028", "node_type": "1", "metadata": {"window": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius. ", "original_text": "***\n**Fig. "}, "hash": "002d26f809f0f30d52bd0f4aed43c95add17f96be90d2a3cc990875a837ed503", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n", "mimetype": "text/plain", "start_char_idx": 49126, "end_char_idx": 49319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b57779c-f1be-4bf8-a6f8-f0f4567eb028", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius. ", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a53b9af-f016-4fb1-859a-bafe3b1b2bb2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We saw that the standard Isolation Forest was unable to capture the structure present in this dataset, and treated it as a rectangular blob.  The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n", "original_text": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n"}, "hash": "549c4b9c1b68307fbccfdf26ad98e41be18e77a809ba9329f79acb8d80495d11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40a45d18-d6a3-455d-9edf-6f5ac9636fea", "node_type": "1", "metadata": {"window": "Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n", "original_text": "15. "}, "hash": "abde8171240ad4e6bf21b81db0b7840c88b9f3d425544a472c2d944cdfe714a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 49319, "end_char_idx": 49330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40a45d18-d6a3-455d-9edf-6f5ac9636fea", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n", "original_text": "15. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b57779c-f1be-4bf8-a6f8-f0f4567eb028", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The rapid rise of the anomaly score for this case represents moving through the boundary of this rectangular region into a high anomaly score area.  Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius. ", "original_text": "***\n**Fig. "}, "hash": "179bc9dbc2847e863a1a2b39e02c109b67753764bc41f1ad270f8c78445c30f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30368fce-2518-46ae-b51d-76b66a1388ca", "node_type": "1", "metadata": {"window": "This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius. ", "original_text": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center."}, "hash": "5f652e74c7eaaba866e09edc460f824c00ea6e6124f742638e6e2fe4cec9ae67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15. ", "mimetype": "text/plain", "start_char_idx": 49330, "end_char_idx": 49334, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30368fce-2518-46ae-b51d-76b66a1388ca", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius. ", "original_text": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40a45d18-d6a3-455d-9edf-6f5ac9636fea", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Outside this region, all data points look the same to the standard Isolation Forest.  This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n", "original_text": "15. "}, "hash": "f1d0937bc448848231711704cc16a5f9f88bd0288065a43a3c98306292571f53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62de2f12-1371-44d5-8ac2-136c0fef2013", "node_type": "1", "metadata": {"window": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance. ", "original_text": "**\n(a) Data: A scatter plot of the single blob data. "}, "hash": "9e15febe9fad62fb9c9be4af810ea903c093b50ff5f7f6407a7a38f188b16eb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center.", "mimetype": "text/plain", "start_char_idx": 49334, "end_char_idx": 49437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62de2f12-1371-44d5-8ac2-136c0fef2013", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance. ", "original_text": "**\n(a) Data: A scatter plot of the single blob data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30368fce-2518-46ae-b51d-76b66a1388ca", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This rapid rise is undesirable.  In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius. ", "original_text": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center."}, "hash": "1cdec8b6eac342d74f97588019de72593920dc2180b0259458eb48c2c8b625ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5435c326-1d07-4958-9dc9-406be8a1b986", "node_type": "1", "metadata": {"window": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n", "original_text": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n"}, "hash": "30ecf8ff4ef5a3a3284b74192564e435cd3032a036b281bbae0451a826204ce8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Data: A scatter plot of the single blob data. ", "mimetype": "text/plain", "start_char_idx": 49437, "end_char_idx": 49490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5435c326-1d07-4958-9dc9-406be8a1b986", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n", "original_text": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62de2f12-1371-44d5-8ac2-136c0fef2013", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition to not capturing the structure of the data, these scores are much more sensitive to choosing an anomaly threshold value which results in reducing AUCROC, as we will see below.  In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance. ", "original_text": "**\n(a) Data: A scatter plot of the single blob data. "}, "hash": "82752dd666b86cf79273b476c6cca5742847644b9ca32ce1e61305c7e3cf7558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c9bc1c7-e135-4a60-b009-9c84121aef2b", "node_type": "1", "metadata": {"window": "***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig. ", "original_text": "(b) Score Mean: A line plot of mean anomaly score versus radius. "}, "hash": "1be2f5591a830b4d83d776a641ec78bf5f5839bbe9c025fd6e575894c00a8bd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n", "mimetype": "text/plain", "start_char_idx": 49490, "end_char_idx": 49593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c9bc1c7-e135-4a60-b009-9c84121aef2b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig. ", "original_text": "(b) Score Mean: A line plot of mean anomaly score versus radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5435c326-1d07-4958-9dc9-406be8a1b986", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the other two cases, we observe a similar trend as before where the mean is converging to a steady score value, but much more slowly, making it less sensitive to choosing threshold values.\n\n ***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n", "original_text": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n"}, "hash": "ddf651b0b7cde24aa988004c5e781f2dddbb96799f840a7338454f81b7864175", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1596d404-f3cc-4aa1-b98a-c7acb90d63c9", "node_type": "1", "metadata": {"window": "15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16. ", "original_text": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n"}, "hash": "a1cc3cc0fafafde15b757754b4dfb80de02109c44e49c09e5094924a1d918696", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Score Mean: A line plot of mean anomaly score versus radius. ", "mimetype": "text/plain", "start_char_idx": 49593, "end_char_idx": 49658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1596d404-f3cc-4aa1-b98a-c7acb90d63c9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16. ", "original_text": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c9bc1c7-e135-4a60-b009-9c84121aef2b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig. ", "original_text": "(b) Score Mean: A line plot of mean anomaly score versus radius. "}, "hash": "bbf355de5ec47c29a11f8c298b9edf081220ca6ebff0b251b4c136088ecbe979", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01e4843a-4d7e-40c8-aae6-fd18e1488344", "node_type": "1", "metadata": {"window": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line.", "original_text": "(c) Score Variance: A line plot of score variance versus radius. "}, "hash": "59cd694e79407470db2839aa3a6007d369b65ae45f9025e498ba851226f466b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n", "mimetype": "text/plain", "start_char_idx": 49658, "end_char_idx": 49752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01e4843a-4d7e-40c8-aae6-fd18e1488344", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line.", "original_text": "(c) Score Variance: A line plot of score variance versus radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1596d404-f3cc-4aa1-b98a-c7acb90d63c9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15.  Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16. ", "original_text": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n"}, "hash": "659430fc0941eda0aa3c69bae83d10daf4a6c38b8595c3c1890b3dd120e192b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcddbbd5-8df2-4391-a0d9-76096bc76a0c", "node_type": "1", "metadata": {"window": "**\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data. ", "original_text": "The \"Standard\" line shows a large peak in variance. "}, "hash": "7c13c67c196b6547b78a5aa25b07afa4812ac69906ac5ce17deb46cac6ea628a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Score Variance: A line plot of score variance versus radius. ", "mimetype": "text/plain", "start_char_idx": 49752, "end_char_idx": 49817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fcddbbd5-8df2-4391-a0d9-76096bc76a0c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data. ", "original_text": "The \"Standard\" line shows a large peak in variance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01e4843a-4d7e-40c8-aae6-fd18e1488344", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance of the scores for the datapoints located radially at fixed distances from the center. **\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line.", "original_text": "(c) Score Variance: A line plot of score variance versus radius. "}, "hash": "2b2962393016ff3dd4c30679d093017bdf208bb88e0a4c25330555394bd7ffe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15f69268-b5d1-40f7-9e8b-e7e9af1223ee", "node_type": "1", "metadata": {"window": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n", "original_text": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n"}, "hash": "918a91a6a62ca5995518d40bc6f7bfd67b161415319354f9c509c3772cef0a10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Standard\" line shows a large peak in variance. ", "mimetype": "text/plain", "start_char_idx": 49817, "end_char_idx": 49869, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "15f69268-b5d1-40f7-9e8b-e7e9af1223ee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n", "original_text": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcddbbd5-8df2-4391-a0d9-76096bc76a0c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Data: A scatter plot of the single blob data.  Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data. ", "original_text": "The \"Standard\" line shows a large peak in variance. "}, "hash": "2d6b68dfce42f0a21072f71bbc642ce171e3fb341d3299206f6be2a23b0d25e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "411e6a29-cdf0-4e23-89cb-f48ab0765826", "node_type": "1", "metadata": {"window": "(b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line. ", "original_text": "**Fig. "}, "hash": "aad82fd8719b8aa693430db2be5caeef6a50e05b836edb7f0e3ab93b8f6dcc8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n", "mimetype": "text/plain", "start_char_idx": 49869, "end_char_idx": 49975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "411e6a29-cdf0-4e23-89cb-f48ab0765826", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15f69268-b5d1-40f7-9e8b-e7e9af1223ee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Gray concentric circles are overlaid on the plot, with red circles at 1, 2, and 3 standard deviations.\n (b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n", "original_text": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n"}, "hash": "468e97dc4567066f8d24e8b954cc0b86d56a71c2745f3d6eff8810033ecb4c94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "038e0c33-c576-40fd-aa9e-8ce0bed553e3", "node_type": "1", "metadata": {"window": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n", "original_text": "16. "}, "hash": "0d17fb06d4b414ccd1698312b230539e6b762b961441af264a36b2bf7de833cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 49975, "end_char_idx": 49982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "038e0c33-c576-40fd-aa9e-8ce0bed553e3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n", "original_text": "16. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "411e6a29-cdf0-4e23-89cb-f48ab0765826", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Score Mean: A line plot of mean anomaly score versus radius.  Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line. ", "original_text": "**Fig. "}, "hash": "f33b9362292cd795d22d19f2bf28941a80f4a1e3c92f3587a721a35e18405a84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc53a56b-1016-4b56-8b0e-9f8240c94397", "node_type": "1", "metadata": {"window": "(c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance. ", "original_text": "Mean and variance of the scores for data points at fixed distances from the center line."}, "hash": "d4c14a99eb6b104d025a879ff5e155f1375de27403f4009c0cb3630465099a10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16. ", "mimetype": "text/plain", "start_char_idx": 49982, "end_char_idx": 49986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc53a56b-1016-4b56-8b0e-9f8240c94397", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance. ", "original_text": "Mean and variance of the scores for data points at fixed distances from the center line."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "038e0c33-c576-40fd-aa9e-8ce0bed553e3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Three lines (Standard, Extended, Rotated) are overlaid and follow a similar increasing trend.\n (c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n", "original_text": "16. "}, "hash": "a5ac24d50e729efb4127d5b6437d38b784b90e333d8099c6c0ab6c9d3a6feff5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a0f786d-8a30-4cc5-9653-8dfcd4a1257c", "node_type": "1", "metadata": {"window": "The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n", "original_text": "**\n(a) Data: A scatter plot of the sinusoidal data. "}, "hash": "07c0fd37767a448a3c7611db79e2ccd72cd1a694dea0808d36e68fdf17a4c865", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and variance of the scores for data points at fixed distances from the center line.", "mimetype": "text/plain", "start_char_idx": 49986, "end_char_idx": 50074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a0f786d-8a30-4cc5-9653-8dfcd4a1257c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n", "original_text": "**\n(a) Data: A scatter plot of the sinusoidal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc53a56b-1016-4b56-8b0e-9f8240c94397", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Score Variance: A line plot of score variance versus radius.  The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance. ", "original_text": "Mean and variance of the scores for data points at fixed distances from the center line."}, "hash": "9c1bdb56556e64ba58efd5c14d3484894970b7c3df3d3a0746e3d398549fcf09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de085c9c-5f6e-4a7b-9165-3c0259073dbb", "node_type": "1", "metadata": {"window": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig. ", "original_text": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n"}, "hash": "f61257d80fb592fbf8a11a969aca46a9898525317455abe90c0ec4052365cc81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Data: A scatter plot of the sinusoidal data. ", "mimetype": "text/plain", "start_char_idx": 50074, "end_char_idx": 50126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de085c9c-5f6e-4a7b-9165-3c0259073dbb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig. ", "original_text": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a0f786d-8a30-4cc5-9653-8dfcd4a1257c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line shows a large peak in variance.  The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n", "original_text": "**\n(a) Data: A scatter plot of the sinusoidal data. "}, "hash": "e7e7f0e11df13187b39f4eacf34251a2f7692122023ef72ef494bb02bbfdafd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "718857d8-9fb5-4914-be21-6528dd5b0113", "node_type": "1", "metadata": {"window": "**Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained. ", "original_text": "(b) Score Mean: A line plot of mean score versus distance from the center line. "}, "hash": "c4ec304f726d244928bcad8c277f3730667a0347bae793b36a58d915b10308e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n", "mimetype": "text/plain", "start_char_idx": 50126, "end_char_idx": 50242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "718857d8-9fb5-4914-be21-6528dd5b0113", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained. ", "original_text": "(b) Score Mean: A line plot of mean score versus distance from the center line. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de085c9c-5f6e-4a7b-9165-3c0259073dbb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Extended\" and \"Rotated\" lines show much lower and more stable variance, especially at larger radii.\n\n **Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig. ", "original_text": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n"}, "hash": "1753ce862236068844eaa4f9e249c312cce4cc65799524b79e3d3808980a4792", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e557481f-3c69-45c9-8342-803b1bcdfcc7", "node_type": "1", "metadata": {"window": "16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. ", "original_text": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n"}, "hash": "58b3b0b4c9da628162107feb64a6f72326f2b41fb35e586f631eff745467e68b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Score Mean: A line plot of mean score versus distance from the center line. ", "mimetype": "text/plain", "start_char_idx": 50242, "end_char_idx": 50322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e557481f-3c69-45c9-8342-803b1bcdfcc7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. ", "original_text": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "718857d8-9fb5-4914-be21-6528dd5b0113", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained. ", "original_text": "(b) Score Mean: A line plot of mean score versus distance from the center line. "}, "hash": "f4cdcd6b8a34bcc9ce9524fc3ff0cc5129736571165b573c50c46f1f0a1740ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3d04935-4e21-4e6f-891d-328cf3715ed5", "node_type": "1", "metadata": {"window": "Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. ", "original_text": "(c) Score Variance: A line plot of score variance versus distance. "}, "hash": "192a874902c34acf79c7519ce0ccc34d73a86137689be941c2e8dcae00921b47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n", "mimetype": "text/plain", "start_char_idx": 50322, "end_char_idx": 50437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e3d04935-4e21-4e6f-891d-328cf3715ed5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. ", "original_text": "(c) Score Variance: A line plot of score variance versus distance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e557481f-3c69-45c9-8342-803b1bcdfcc7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16.  Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. ", "original_text": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n"}, "hash": "74f3fe5e8f33000c3da2ea6cadc60f4d3469a7365bfc4b996f1d2d8355d83bb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfcf925a-71bd-4c75-b2ab-baf9ae115ec1", "node_type": "1", "metadata": {"window": "**\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. ", "original_text": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n"}, "hash": "b29bc1eb8e93de13d34888f6054152e858508639cbe45940de7237f6c23a8890", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Score Variance: A line plot of score variance versus distance. ", "mimetype": "text/plain", "start_char_idx": 50437, "end_char_idx": 50504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bfcf925a-71bd-4c75-b2ab-baf9ae115ec1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. ", "original_text": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3d04935-4e21-4e6f-891d-328cf3715ed5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance of the scores for data points at fixed distances from the center line. **\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. ", "original_text": "(c) Score Variance: A line plot of score variance versus distance. "}, "hash": "e61012a2a45f25ccb3ff0f8238df734163691d353e797f2a5be5d38321a5274e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cff53613-1877-46ee-8663-968dedce577b", "node_type": "1", "metadata": {"window": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n", "original_text": "***\n\nFig. "}, "hash": "993a7cf94caa1ec9ff5f7077f1b42484a98c5d1f1555ebd213fe82cb295ecab0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n", "mimetype": "text/plain", "start_char_idx": 50504, "end_char_idx": 50651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cff53613-1877-46ee-8663-968dedce577b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n", "original_text": "***\n\nFig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfcf925a-71bd-4c75-b2ab-baf9ae115ec1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Data: A scatter plot of the sinusoidal data.  Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. ", "original_text": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n"}, "hash": "6cbc531cf02d977fc5b2f48fd5f1867eabc1b5a23d6a684f3951f338f8a32f4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dce7819-c5d1-4aca-bfa2-48fb7ca7689e", "node_type": "1", "metadata": {"window": "(b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data. ", "original_text": "16c shows the variance of the scores obtained. "}, "hash": "1ff81cb0e9d7ee25f469e6f9f1b91e84c7945d0dafb6ee4a7d596119cb5077a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nFig. ", "mimetype": "text/plain", "start_char_idx": 50651, "end_char_idx": 50661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7dce7819-c5d1-4aca-bfa2-48fb7ca7689e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data. ", "original_text": "16c shows the variance of the scores obtained. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cff53613-1877-46ee-8663-968dedce577b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Gray lines parallel to the sine curve are overlaid, with red lines at 1, 2, and 3 standard deviations of the noise.\n (b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n", "original_text": "***\n\nFig. "}, "hash": "582f663b213d20b47114172a322725ab19cd838ba41126ff67dced2a04c2e449", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a39548db-274c-49ee-b7e5-852481434dd5", "node_type": "1", "metadata": {"window": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. ", "original_text": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. "}, "hash": "ba63a6e28283ab4b658597d837698042e474d9a8b9b311fb86069715780259b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16c shows the variance of the scores obtained. ", "mimetype": "text/plain", "start_char_idx": 50661, "end_char_idx": 50708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a39548db-274c-49ee-b7e5-852481434dd5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. ", "original_text": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dce7819-c5d1-4aca-bfa2-48fb7ca7689e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Score Mean: A line plot of mean score versus distance from the center line.  The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data. ", "original_text": "16c shows the variance of the scores obtained. "}, "hash": "0e45b51a215d7e3d79d7ee686945de9070131236241abaf2c26683d3d351cb11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea4aeac5-101c-4e48-bc82-245a3d548247", "node_type": "1", "metadata": {"window": "(c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. ", "original_text": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. "}, "hash": "c5944274148dcc96345508db5d8962ceedb15ad79edf4e315b22e9686ffdb60e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 50708, "end_char_idx": 50843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea4aeac5-101c-4e48-bc82-245a3d548247", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. ", "original_text": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a39548db-274c-49ee-b7e5-852481434dd5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line rises sharply and then plateaus, while the \"Extended\" and \"Rotated\" lines rise more gradually.\n (c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. ", "original_text": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest. "}, "hash": "fe77992d29a7af664722bd5d8b50cb8dd9f9ac6a4f6e5a769ddf472fc444eac6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "617a2878-66b6-4ba0-9ccf-279083cc5c22", "node_type": "1", "metadata": {"window": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above. ", "original_text": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. "}, "hash": "b5fabd0286538be1a4fc9d73b711c98fe9ea25344c338343f80b667de1c33044", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. ", "mimetype": "text/plain", "start_char_idx": 50843, "end_char_idx": 51001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "617a2878-66b6-4ba0-9ccf-279083cc5c22", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above. ", "original_text": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea4aeac5-101c-4e48-bc82-245a3d548247", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Score Variance: A line plot of score variance versus distance.  The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. ", "original_text": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value. "}, "hash": "1640a6192c2a81edd75db4d5c3e5239faab785490c479082576519a92e69a03d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8010a16-a59c-431e-9922-689b2937c297", "node_type": "1", "metadata": {"window": "***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n", "original_text": "The EIF again provides a more robust scoring prediction.\n\n"}, "hash": "a84fef538373a386c4299f82cd7e094f61a75d7a93b1753620cd6df8578161aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. ", "mimetype": "text/plain", "start_char_idx": 51001, "end_char_idx": 51175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8010a16-a59c-431e-9922-689b2937c297", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n", "original_text": "The EIF again provides a more robust scoring prediction.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "617a2878-66b6-4ba0-9ccf-279083cc5c22", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line has a significantly higher variance compared to the much lower and more stable variance of the \"Extended\" and \"Rotated\" lines.\n ***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above. ", "original_text": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before. "}, "hash": "34f61f8e1a46bc039e0b011a1b059aaf3e612d0dc0e0b4f27ea262a4a1604a1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4e10a67-098d-46fc-8af8-ea6ca8cd647f", "node_type": "1", "metadata": {"window": "16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs. ", "original_text": "In addition to the two examples above, we can perform the same analysis on higher dimensional data. "}, "hash": "60943820eede3842a88ce40b3c88ff1eb6d06d7e855c0b805ac934dcbb501ec0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The EIF again provides a more robust scoring prediction.\n\n", "mimetype": "text/plain", "start_char_idx": 51175, "end_char_idx": 51233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4e10a67-098d-46fc-8af8-ea6ca8cd647f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs. ", "original_text": "In addition to the two examples above, we can perform the same analysis on higher dimensional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8010a16-a59c-431e-9922-689b2937c297", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nFig.  16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n", "original_text": "The EIF again provides a more robust scoring prediction.\n\n"}, "hash": "ea227252c6f78be06c49b960e7b487c231acd41d81aa01a00ad9618abeeb0d1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f28c584-9c30-4833-9faf-9eb69254d671", "node_type": "1", "metadata": {"window": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. ", "original_text": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. "}, "hash": "776178726b6740f3f7e5d618097ae8626dce3d1ba9b489b7eda9145b5ab500b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition to the two examples above, we can perform the same analysis on higher dimensional data. ", "mimetype": "text/plain", "start_char_idx": 51233, "end_char_idx": 51333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f28c584-9c30-4833-9faf-9eb69254d671", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. ", "original_text": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4e10a67-098d-46fc-8af8-ea6ca8cd647f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "16c shows the variance of the scores obtained.  We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs. ", "original_text": "In addition to the two examples above, we can perform the same analysis on higher dimensional data. "}, "hash": "1290113c3d5359d25585223e9afb1c23357e16fde60ce2cad9409589510da09f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67429184-5b24-4ec1-b1c6-ebd35f0ab492", "node_type": "1", "metadata": {"window": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. ", "original_text": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. "}, "hash": "56f828bbb0babc31f1c9b13e1eb6ac7023b4c9734e65c0f3bd4ec5ebe60f7baa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. ", "mimetype": "text/plain", "start_char_idx": 51333, "end_char_idx": 51482, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67429184-5b24-4ec1-b1c6-ebd35f0ab492", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. ", "original_text": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f28c584-9c30-4833-9faf-9eb69254d671", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We notice that, as before, the variance is much higher for the standard Isolation Forest, and lower for the extended Isolation Forest.  After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. ", "original_text": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case. "}, "hash": "da875912153549386a8c405336bd6d3db24708a54d2f8623810f5bba2d3e1266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf59320a-ef7e-4486-a142-6d5cd7423c0e", "node_type": "1", "metadata": {"window": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases. ", "original_text": "These are represented by Exn as discussed above. "}, "hash": "017321c317e1b2ce5ad7d87a79d3c0cabdf85abdd169bde77c4c48ede7cb228f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. ", "mimetype": "text/plain", "start_char_idx": 51482, "end_char_idx": 51643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf59320a-ef7e-4486-a142-6d5cd7423c0e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases. ", "original_text": "These are represented by Exn as discussed above. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67429184-5b24-4ec1-b1c6-ebd35f0ab492", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After 3\u03c3 the variance for the standard case drops rapidly to a constant value but higher than that obtained by EIF, as the mean score reaches a steady value.  The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. ", "original_text": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts. "}, "hash": "d4e4e267093913bf199e5bb9f963f7c61dd159f0681f5bc71faab326d53d90d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "697c8509-b7d3-4302-9daa-fcf7ca2b0e3c", "node_type": "1", "metadata": {"window": "The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n", "original_text": "We do not include the rotated case in the following analysis.\n\n"}, "hash": "66f1f511fa47452cb7bbbd488966f59da50a108df680e25ae9cbf7994e2a48ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These are represented by Exn as discussed above. ", "mimetype": "text/plain", "start_char_idx": 51643, "end_char_idx": 51692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "697c8509-b7d3-4302-9daa-fcf7ca2b0e3c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n", "original_text": "We do not include the rotated case in the following analysis.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf59320a-ef7e-4486-a142-6d5cd7423c0e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The constant value here is a clear indication that the standard Isolation Forest is unable to capture any useful information outside the rectangular region discussed before.  The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases. ", "original_text": "These are represented by Exn as discussed above. "}, "hash": "b665ac88bf5f170eefd3de2b3efdcc837bff94e7516d1530bf67255a91c0e332", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ab4d145-fcce-41b5-83c4-cec8eef8c922", "node_type": "1", "metadata": {"window": "In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. ", "original_text": "As before and not surprisingly, we can see from Figs. "}, "hash": "1ea829d7e6df8f5d78dd866a2c9cd93ff394aa1843a560c57ff6dc7e2b33cea0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We do not include the rotated case in the following analysis.\n\n", "mimetype": "text/plain", "start_char_idx": 51692, "end_char_idx": 51755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ab4d145-fcce-41b5-83c4-cec8eef8c922", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. ", "original_text": "As before and not surprisingly, we can see from Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "697c8509-b7d3-4302-9daa-fcf7ca2b0e3c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The EIF again provides a more robust scoring prediction.\n\n In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n", "original_text": "We do not include the rotated case in the following analysis.\n\n"}, "hash": "0a32a6b81a0d6cf2d1c97095cabdb5521cfff9c5ae47c62f6c1a4face25ad29b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6855341-60c9-4189-91c1-2667ffe6930a", "node_type": "1", "metadata": {"window": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. ", "original_text": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. "}, "hash": "b09f39c94e894d58f7a1898d60decd4b7078af2f6fb7bc3186204abdd926a82c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As before and not surprisingly, we can see from Figs. ", "mimetype": "text/plain", "start_char_idx": 51755, "end_char_idx": 51809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6855341-60c9-4189-91c1-2667ffe6930a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. ", "original_text": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ab4d145-fcce-41b5-83c4-cec8eef8c922", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In addition to the two examples above, we can perform the same analysis on higher dimensional data.  We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. ", "original_text": "As before and not surprisingly, we can see from Figs. "}, "hash": "98720202ec23a11d58a82664445f6c6eee1539213f279e8ad208e3486bc34340", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9e1743e-5cd7-41fd-9770-5f397179b382", "node_type": "1", "metadata": {"window": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n", "original_text": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. "}, "hash": "0b8e1a4c6d296f3629ae47c1c9ad34c966e7414d6c0288ab38a797b4b0b26419", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. ", "mimetype": "text/plain", "start_char_idx": 51809, "end_char_idx": 51947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9e1743e-5cd7-41fd-9770-5f397179b382", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n", "original_text": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6855341-60c9-4189-91c1-2667ffe6930a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We choose 3-D and 4-D blobs of normally distributed data in space around the origin with unit variance in all directions, similarly to the 2-D case.  Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. ", "original_text": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case. "}, "hash": "5af96e024ed83323c2d857b1948a6d4651786108e399a235dd70330eeebe6db8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c7cc953-e0d8-4a80-bf8b-599cb3880e67", "node_type": "1", "metadata": {"window": "These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. ", "original_text": "We also observe that for each case, the variance decreases as the extension level increases. "}, "hash": "73714990accfe7cd9e3ef8e464727e672c89d06908f4a141322e2f05ff0cfb23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. ", "mimetype": "text/plain", "start_char_idx": 51947, "end_char_idx": 52108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c7cc953-e0d8-4a80-bf8b-599cb3880e67", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. ", "original_text": "We also observe that for each case, the variance decreases as the extension level increases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9e1743e-5cd7-41fd-9770-5f397179b382", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Recall that as soon as we move beyond the two dimensional case, we have more levels of extension that we can consider with regards to generating the plane cuts.  These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n", "original_text": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance. "}, "hash": "c35fc7b4b37ee59d03ce5cee4124b31c6824c08025aebbf8cf289e3a1d4c86f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4e4e46d-0ee0-440c-8dcb-ef532c342c46", "node_type": "1", "metadata": {"window": "We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s. ", "original_text": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n"}, "hash": "3b5d9c546db7a51431b8f490837e8275dd3ba607bbfc3168bac96d2fa96e6c1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also observe that for each case, the variance decreases as the extension level increases. ", "mimetype": "text/plain", "start_char_idx": 52108, "end_char_idx": 52201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4e4e46d-0ee0-440c-8dcb-ef532c342c46", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s. ", "original_text": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c7cc953-e0d8-4a80-bf8b-599cb3880e67", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "These are represented by Exn as discussed above.  We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. ", "original_text": "We also observe that for each case, the variance decreases as the extension level increases. "}, "hash": "05ad95baa7982e8e0b72c71befbd3e25ec947a866eac410bdb836b6989044428", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02d2ca29-8eb8-4c26-b852-c71008b53661", "node_type": "1", "metadata": {"window": "As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n", "original_text": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. "}, "hash": "01532cb62c026861223818dbb1315931e617fa00c11c4a040189b6307059fc95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n", "mimetype": "text/plain", "start_char_idx": 52201, "end_char_idx": 52474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "02d2ca29-8eb8-4c26-b852-c71008b53661", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n", "original_text": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4e4e46d-0ee0-440c-8dcb-ef532c342c46", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We do not include the rotated case in the following analysis.\n\n As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s. ", "original_text": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n"}, "hash": "20b2c839aaca88b9b3981565a8ab079f7143c5033e60176e58dc8baddfb7f697", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48d9ac05-1ddd-4744-b980-26311794d47c", "node_type": "1", "metadata": {"window": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. ", "original_text": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. "}, "hash": "9f8d02302acbe128ce6c3225b28d6abaa997bd3da8c784902c92338239ace4d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. ", "mimetype": "text/plain", "start_char_idx": 52474, "end_char_idx": 52584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48d9ac05-1ddd-4744-b980-26311794d47c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. ", "original_text": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02d2ca29-8eb8-4c26-b852-c71008b53661", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As before and not surprisingly, we can see from Figs.  17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n", "original_text": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods. "}, "hash": "c3076a59ca876aacc816c6a73d0c01988739d2a63fa589156698d213ddf36b0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3de118f7-e6ab-4811-8117-1472f87bc3f0", "node_type": "1", "metadata": {"window": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n", "original_text": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n"}, "hash": "fb9834c8c86a2e1933e2545fd187b4641f49a29fc28b6e42292d75846f9cf908", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. ", "mimetype": "text/plain", "start_char_idx": 52584, "end_char_idx": 52770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3de118f7-e6ab-4811-8117-1472f87bc3f0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n", "original_text": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48d9ac05-1ddd-4744-b980-26311794d47c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "17 and 18 that the mean score increases with increasing radius very similarly in all cases, which was also observed in the 2-D blob case.  However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. ", "original_text": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper. "}, "hash": "8f42aa0e5a2e7a9df8a113b5dbccfb5db83550f11fffd1539b2b38618dadb816", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f39f3f1-4192-4692-8a87-250e361d78fb", "node_type": "1", "metadata": {"window": "We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets. ", "original_text": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. "}, "hash": "92c328c9e5f8e63788e9b73c24072ea116d01fa9b3a32221661eda6577f2598c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n", "mimetype": "text/plain", "start_char_idx": 52770, "end_char_idx": 52882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f39f3f1-4192-4692-8a87-250e361d78fb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets. ", "original_text": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3de118f7-e6ab-4811-8117-1472f87bc3f0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "However, again we see in the region beyond 3\u03c3, which is where the anomalies lie, the Extended Isolation Forest produces much lower values in the score variance.  We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n", "original_text": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n"}, "hash": "ff77ba0c5d8e6166a0b91c0cf5d1ce4757d29710ff5cde43d47d3dba3e59ecf9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f0659d5-7d00-43b9-a04e-07b29f485a8e", "node_type": "1", "metadata": {"window": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties. ", "original_text": "We then add 200 anomalous points, score the data, and compute AUC\u2019s. "}, "hash": "32a3aa076ff0721fe78bd4ccb73633f58736416a07e9f331761069e1f95d29f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. ", "mimetype": "text/plain", "start_char_idx": 52882, "end_char_idx": 53038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f0659d5-7d00-43b9-a04e-07b29f485a8e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties. ", "original_text": "We then add 200 anomalous points, score the data, and compute AUC\u2019s. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f39f3f1-4192-4692-8a87-250e361d78fb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also observe that for each case, the variance decreases as the extension level increases.  In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets. ", "original_text": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously. "}, "hash": "25da27aa9e8265d960f553484d21c642014f1482ff9a957e04470e564f3d50e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40043f75-9931-41c4-a271-015fb39094fe", "node_type": "1", "metadata": {"window": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n", "original_text": "The results are summarized in Table 1.\n\n"}, "hash": "82241e34ed2dd85b4659971d3dee1acc5f7967a3f58081521b8f3178a6940a10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We then add 200 anomalous points, score the data, and compute AUC\u2019s. ", "mimetype": "text/plain", "start_char_idx": 53038, "end_char_idx": 53107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40043f75-9931-41c4-a271-015fb39094fe", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n", "original_text": "The results are summarized in Table 1.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f0659d5-7d00-43b9-a04e-07b29f485a8e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In other words, the Extended Isolation Forest, i.e., the fully general case (when the planes subdividing the data have no constrains and the normal can point in any direction within all the dimensions of the problem), produces the most reliable and robust anomaly scores.\n\n ### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties. ", "original_text": "We then add 200 anomalous points, score the data, and compute AUC\u2019s. "}, "hash": "0985cd1df967a401a5f896f4cf8d0a29930bde34d6d5b1f080d340f83eb71e27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e361447-30ed-4490-891d-371ea72b899f", "node_type": "1", "metadata": {"window": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest. ", "original_text": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. "}, "hash": "5ce4a1e5ffe81eeda51472152bae7644f307765829426b7d37de062e4fb1af6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results are summarized in Table 1.\n\n", "mimetype": "text/plain", "start_char_idx": 53107, "end_char_idx": 53147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6e361447-30ed-4490-891d-371ea72b899f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest. ", "original_text": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40043f75-9931-41c4-a271-015fb39094fe", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 4.3 AUC Comparison\nIn this section we report AUC values for both ROC and PRC and compare the two methods.  Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n", "original_text": "The results are summarized in Table 1.\n\n"}, "hash": "44a11533dc88ec79c3b562da30fa305d17867c1f0111a3c0c7e5badb3086a97b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33a01e29-d056-4489-9619-40b2d67746d5", "node_type": "1", "metadata": {"window": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. ", "original_text": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n"}, "hash": "0693389da6d2977e9d71724f3b257bfa3e821c0930fa683fc99c1a6af82c7c86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. ", "mimetype": "text/plain", "start_char_idx": 53147, "end_char_idx": 53346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33a01e29-d056-4489-9619-40b2d67746d5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. ", "original_text": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e361447-30ed-4490-891d-371ea72b899f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Initially, we produce these results for the examples provided earlier, since these examples are designed to highlight the specific shortcoming of Isolation Forest studied in this paper.  We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest. ", "original_text": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset. "}, "hash": "c8ebb27c9b62b69ea81652047e1df12b3347ebd4f67c07407d83b04b57763e65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f49262a9-dde4-462e-995e-e3bcad1c8c9e", "node_type": "1", "metadata": {"window": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other. ", "original_text": "We perform the same analysis on some real world benchmark datasets. "}, "hash": "b3f2196c2ca9e3d724b2daf1e65ae705cb1da3ced615efa77e53e2f6229eb48d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n", "mimetype": "text/plain", "start_char_idx": 53346, "end_char_idx": 53459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f49262a9-dde4-462e-995e-e3bcad1c8c9e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other. ", "original_text": "We perform the same analysis on some real world benchmark datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33a01e29-d056-4489-9619-40b2d67746d5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We then report the AUC for ROC and PRC for some real world benchmark datasets of varying sizes and dimensions.\n\n In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. ", "original_text": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n"}, "hash": "d7b08eab5d532cc87258ec16ca11e3e494efd722da799b871c79121eac17d818", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbb6ff15-50c5-48cb-943e-cc5f7621f228", "node_type": "1", "metadata": {"window": "We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n", "original_text": "Table 2 lists the datasets used as well as their properties. "}, "hash": "a4bb537e0fcf9c6ca29d05b5d84f816b50b67b37a0b641a85de698cdea75daea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We perform the same analysis on some real world benchmark datasets. ", "mimetype": "text/plain", "start_char_idx": 53459, "end_char_idx": 53527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbb6ff15-50c5-48cb-943e-cc5f7621f228", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n", "original_text": "Table 2 lists the datasets used as well as their properties. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f49262a9-dde4-462e-995e-e3bcad1c8c9e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In order to obtain the AUC values for the example datasets, we first train both algorithms on 2000 data points in each case distributed as seen previously.  We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other. ", "original_text": "We perform the same analysis on some real world benchmark datasets. "}, "hash": "a53ffd4b9d6af0a61d669adcac3448e2dbb7b8a0500b6ed04511c129ba38fb84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72d1c9e6-aca5-4850-a13b-cb945706b726", "node_type": "1", "metadata": {"window": "The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. ", "original_text": "The results are summarized in Table 3.\n\n"}, "hash": "7c859ae3318465adb58fee98e56620b9484bdfbb811572d06fb78b3704c5851c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2 lists the datasets used as well as their properties. ", "mimetype": "text/plain", "start_char_idx": 53527, "end_char_idx": 53588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72d1c9e6-aca5-4850-a13b-cb945706b726", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. ", "original_text": "The results are summarized in Table 3.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbb6ff15-50c5-48cb-943e-cc5f7621f228", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We then add 200 anomalous points, score the data, and compute AUC\u2019s.  The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n", "original_text": "Table 2 lists the datasets used as well as their properties. "}, "hash": "36eb9a53ee26c1d4a18698ab25432404a8e06c871111bb943dabc1aed47e5d78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f49db92-7c67-4b8d-88b8-f9af0fed509e", "node_type": "1", "metadata": {"window": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). ", "original_text": "We can see the improved AUC values in the case of Extended Isolation Forest. "}, "hash": "567fce916c8fec0ee17a0b55bc324a6e8994137c6f599ac31316b24569641fda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results are summarized in Table 3.\n\n", "mimetype": "text/plain", "start_char_idx": 53588, "end_char_idx": 53628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f49db92-7c67-4b8d-88b8-f9af0fed509e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). ", "original_text": "We can see the improved AUC values in the case of Extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72d1c9e6-aca5-4850-a13b-cb945706b726", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The results are summarized in Table 1.\n\n In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. ", "original_text": "The results are summarized in Table 3.\n\n"}, "hash": "18c3bb14007c6ae947f7bf3d4599096c5c42d2ca7c062e176800243cf494c931", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9154a965-c668-4a22-bdfc-770421d75fd7", "node_type": "1", "metadata": {"window": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs. ", "original_text": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. "}, "hash": "d269d9b9dc558fe26b050bc24b72919779e6e276c8d1deb5b81d959cd538e404", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We can see the improved AUC values in the case of Extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 53628, "end_char_idx": 53705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9154a965-c668-4a22-bdfc-770421d75fd7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs. ", "original_text": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f49db92-7c67-4b8d-88b8-f9af0fed509e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In all cases the improvement is obvious, especially in the case of the double blob, where we have \u201cghost\u201d regions, and the sinusoid, where we have a high variability in the structure of the dataset.  This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). ", "original_text": "We can see the improved AUC values in the case of Extended Isolation Forest. "}, "hash": "c35c7dfc487e30ceed32b4386573cdaebca870173166763513ca1604245de0a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f47f37c-2d56-4e36-b53b-b75a32630f3e", "node_type": "1", "metadata": {"window": "We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before. ", "original_text": "Naturally, the axis-parallel limitation has more effects on some datasets than other. "}, "hash": "a5e668fca798f634cd9b928c39c1388c2b10356bcb20c10934f9d77e0773d723", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. ", "mimetype": "text/plain", "start_char_idx": 53705, "end_char_idx": 53811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f47f37c-2d56-4e36-b53b-b75a32630f3e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before. ", "original_text": "Naturally, the axis-parallel limitation has more effects on some datasets than other. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9154a965-c668-4a22-bdfc-770421d75fd7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This highlights the importance of understanding the problem that arises due to using axis-parallel hyperplanes.\n\n We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs. ", "original_text": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference. "}, "hash": "cf7beb797ae1abcadf452fc412bd9a10ef2ec3f770afb8eb486af8d7a54c4f47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56beed77-a324-443d-9e03-818254144c1f", "node_type": "1", "metadata": {"window": "Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig. ", "original_text": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n"}, "hash": "900eac6e9b1a47bbf821a76d10aa3219afb39fa5ad0ee01fe0539840f90c3dc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Naturally, the axis-parallel limitation has more effects on some datasets than other. ", "mimetype": "text/plain", "start_char_idx": 53811, "end_char_idx": 53897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "56beed77-a324-443d-9e03-818254144c1f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig. ", "original_text": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f47f37c-2d56-4e36-b53b-b75a32630f3e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We perform the same analysis on some real world benchmark datasets.  Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before. ", "original_text": "Naturally, the axis-parallel limitation has more effects on some datasets than other. "}, "hash": "0dec93389b52fb7deb8c4dd33b762b2824d2b01ee611f4f4273c6d29e710b1c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2926fcb1-013f-4e5f-b831-0768b2189ccf", "node_type": "1", "metadata": {"window": "The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n", "original_text": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. "}, "hash": "2cf31683f66dcda7b790bd793bf34db45687774cc8e2c232969dcdda5a94fa20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n", "mimetype": "text/plain", "start_char_idx": 53897, "end_char_idx": 54007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2926fcb1-013f-4e5f-b831-0768b2189ccf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n", "original_text": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56beed77-a324-443d-9e03-818254144c1f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Table 2 lists the datasets used as well as their properties.  The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig. ", "original_text": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n"}, "hash": "3e4a0e968c10fc7a194ac32af89e325db6046bac3c074d2d32ef9d8b1b236aff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4857370-8ca7-4997-be88-b4d0f71f64bc", "node_type": "1", "metadata": {"window": "We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig. ", "original_text": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). "}, "hash": "ac707df3db329343efd5db45bb543a0f1d0ee7923c021c8ddd24ba6089f6dabe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. ", "mimetype": "text/plain", "start_char_idx": 54007, "end_char_idx": 54172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4857370-8ca7-4997-be88-b4d0f71f64bc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig. ", "original_text": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2926fcb1-013f-4e5f-b831-0768b2189ccf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The results are summarized in Table 3.\n\n We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n", "original_text": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case. "}, "hash": "84f948c91cee4d9f09cc5066641ed32cf4d879bf1739ea17a25ce2773ba8b8b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "723497c9-3cb2-4031-bd72-a738782d4270", "node_type": "1", "metadata": {"window": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17. ", "original_text": "We perform this for the 2-D, 3-D, and 4-D blobs. "}, "hash": "eb8b7eaf6c5fef8b93bdf83126e419dbfe7b04ffd10f70652ab30f929e9dc2a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). ", "mimetype": "text/plain", "start_char_idx": 54172, "end_char_idx": 54293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "723497c9-3cb2-4031-bd72-a738782d4270", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17. ", "original_text": "We perform this for the 2-D, 3-D, and 4-D blobs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4857370-8ca7-4997-be88-b4d0f71f64bc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We can see the improved AUC values in the case of Extended Isolation Forest.  Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig. ", "original_text": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees). "}, "hash": "bb190721f35927498dedd41f7245ca9c008a0c94b9e100dc7e7110fd30d146a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d289ef5-56bd-491d-b0a8-af10aca9ddcf", "node_type": "1", "metadata": {"window": "Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob.", "original_text": "In the case of 2-D we also include the method of tree rotations, described before. "}, "hash": "aa04e5993867557cd5e02e7a7062293c0eac9d58520458f06f59ad9232c43171", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We perform this for the 2-D, 3-D, and 4-D blobs. ", "mimetype": "text/plain", "start_char_idx": 54293, "end_char_idx": 54342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d289ef5-56bd-491d-b0a8-af10aca9ddcf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob.", "original_text": "In the case of 2-D we also include the method of tree rotations, described before. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "723497c9-3cb2-4031-bd72-a738782d4270", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Even though in some cases the improvement may seem modest, in others, there is an appreciable difference.  Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17. ", "original_text": "We perform this for the 2-D, 3-D, and 4-D blobs. "}, "hash": "b3727456a4f4a8be18359e3bc2115510f5104418f0e55d960b5a220bfda0b344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2ec0005-4d56-4cb4-8495-f69bc6676c5d", "node_type": "1", "metadata": {"window": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. ", "original_text": "Fig. "}, "hash": "bc9338faac12f56ed9db5aac2f916c908e79871184ad25a493afb71dc573dbbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of 2-D we also include the method of tree rotations, described before. ", "mimetype": "text/plain", "start_char_idx": 54342, "end_char_idx": 54425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2ec0005-4d56-4cb4-8495-f69bc6676c5d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d289ef5-56bd-491d-b0a8-af10aca9ddcf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Naturally, the axis-parallel limitation has more effects on some datasets than other.  Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob.", "original_text": "In the case of 2-D we also include the method of tree rotations, described before. "}, "hash": "454c23b40f12e44d5fdc600b7ee5363d9875b1566b722dc4d52677d7d80a1b63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfb1fd94-5d46-409b-9394-5ee7cca29f26", "node_type": "1", "metadata": {"window": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n", "original_text": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n"}, "hash": "990dede85e46f85f3c42071d750fe88d67147fdc8a7d924d39b97875a0c45c83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 54425, "end_char_idx": 54430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfb1fd94-5d46-409b-9394-5ee7cca29f26", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n", "original_text": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2ec0005-4d56-4cb4-8495-f69bc6676c5d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Nevertheless, we can see that the EIF in general achieves better results than the standard Isolation Forest.\n\n ### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. ", "original_text": "Fig. "}, "hash": "0d88be5adc2e941c060fc88540806a77edf9fe53a8e276413303a25ba0679eb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fb9f464-f234-4bc6-ac28-8270af735a5d", "node_type": "1", "metadata": {"window": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. ", "original_text": "***\n**Fig. "}, "hash": "69d274d448ace2910b26249e52e1d0b3bc5abd7bc557b89b86ada4e271a647ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n", "mimetype": "text/plain", "start_char_idx": 54430, "end_char_idx": 54519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fb9f464-f234-4bc6-ac28-8270af735a5d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. ", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfb1fd94-5d46-409b-9394-5ee7cca29f26", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "### 4.4 Convergence of Anomaly Scores\nIn a final comparison, we would like to find out how efficient the Extended Isolation Forest is compared to the standard case.  For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n", "original_text": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n"}, "hash": "e5d06c10b0b91197ff6b4cf1c652414a74f66372e192b6a5057d1c7fdde981b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "642abe4f-9e6a-41ba-a82e-904a73df83f4", "node_type": "1", "metadata": {"window": "We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance. ", "original_text": "17. "}, "hash": "3d901e4af06b78c17ec6f752a057ee775c929650f890dac3850680ef08d702a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 54519, "end_char_idx": 54530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "642abe4f-9e6a-41ba-a82e-904a73df83f4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance. ", "original_text": "17. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fb9f464-f234-4bc6-ac28-8270af735a5d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For this purpose we consider convergence plots for the anomaly score as a function of forest size (the number of trees).  We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. ", "original_text": "***\n**Fig. "}, "hash": "577589e6b1d3390afe025408f4bdf8933e30f0e342817dd933ae03b1ef9a0d2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "777b550f-a335-440b-8f39-6d4612545f58", "node_type": "1", "metadata": {"window": "In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n", "original_text": "Mean and variance for concentric points around a normally distributed 3-D blob."}, "hash": "ae7eb36369e72a9df246e51d7e18d0a5c85cc76db9627e2b134c07ae478e0e39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17. ", "mimetype": "text/plain", "start_char_idx": 54530, "end_char_idx": 54534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "777b550f-a335-440b-8f39-6d4612545f58", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n", "original_text": "Mean and variance for concentric points around a normally distributed 3-D blob."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "642abe4f-9e6a-41ba-a82e-904a73df83f4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We perform this for the 2-D, 3-D, and 4-D blobs.  In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance. ", "original_text": "17. "}, "hash": "2d55e273b519e0379e84b90858191a4026fe438b9b08a4e36be60d2a7b2177bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86e1b64e-8edd-4cce-9078-185c40b7d38e", "node_type": "1", "metadata": {"window": "Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. ", "original_text": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. "}, "hash": "650366edda242aa271e8cf3dcc067c581adc75937ede91a555bca8d510f9b584", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and variance for concentric points around a normally distributed 3-D blob.", "mimetype": "text/plain", "start_char_idx": 54534, "end_char_idx": 54613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86e1b64e-8edd-4cce-9078-185c40b7d38e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. ", "original_text": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "777b550f-a335-440b-8f39-6d4612545f58", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of 2-D we also include the method of tree rotations, described before.  Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n", "original_text": "Mean and variance for concentric points around a normally distributed 3-D blob."}, "hash": "0c650f29c69ce0c88ed9fa54f281be3cd6a3d9178ae2656bc0078d0c10ce6473", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c628ac1-ece4-4249-8ad0-c04e4ad5bb18", "node_type": "1", "metadata": {"window": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18. ", "original_text": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n"}, "hash": "d30c7dd3096706dba81238de217bf4efd1bf027fabce304261f842fe91257f9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. ", "mimetype": "text/plain", "start_char_idx": 54613, "end_char_idx": 54691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c628ac1-ece4-4249-8ad0-c04e4ad5bb18", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18. ", "original_text": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86e1b64e-8edd-4cce-9078-185c40b7d38e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Fig.  19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. ", "original_text": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius. "}, "hash": "f2536e14422614b9fede727167f5f5b1695c31e276a9e48a757fee7c03663854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed1662ad-3b58-4582-8c6f-cfa54f6bc28a", "node_type": "1", "metadata": {"window": "***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob.", "original_text": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. "}, "hash": "fe45635c20511d4aab7604c97b858a21b874bb5113f9d12caee91b7b16f41d70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n", "mimetype": "text/plain", "start_char_idx": 54691, "end_char_idx": 54765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed1662ad-3b58-4582-8c6f-cfa54f6bc28a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob.", "original_text": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c628ac1-ece4-4249-8ad0-c04e4ad5bb18", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "19 shows the convergence plots for the standard, rotated and Extended Isolation forest.\n\n ***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18. ", "original_text": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n"}, "hash": "ae80da62ec1b34e2791bd1132d1bef2269d4eb5ce23fe74e97fc259fce422e5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "295c8709-2a77-44ec-aa7f-203a21db9d16", "node_type": "1", "metadata": {"window": "17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. ", "original_text": "The \"Standard\" line has the highest variance. "}, "hash": "92ccc7b14477a1d526fdf7ba9824ec76d8e7f46ba911f6de440706a1c1525dbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. ", "mimetype": "text/plain", "start_char_idx": 54765, "end_char_idx": 54848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "295c8709-2a77-44ec-aa7f-203a21db9d16", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. ", "original_text": "The \"Standard\" line has the highest variance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed1662ad-3b58-4582-8c6f-cfa54f6bc28a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**Fig.  17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob.", "original_text": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius. "}, "hash": "3b8fc3b12838b5cdda18b07260a82441878f27f9585c5e771f028dba1d89c543", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e23c1f00-3cb2-4233-8361-b431ec784ba2", "node_type": "1", "metadata": {"window": "Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n", "original_text": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n"}, "hash": "235e13712df0097149caf17ecebe500aa5b96fe63e25c8d237aeb09be50ca68e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Standard\" line has the highest variance. ", "mimetype": "text/plain", "start_char_idx": 54848, "end_char_idx": 54894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e23c1f00-3cb2-4233-8361-b431ec784ba2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n", "original_text": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "295c8709-2a77-44ec-aa7f-203a21db9d16", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "17.  Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. ", "original_text": "The \"Standard\" line has the highest variance. "}, "hash": "54fd2120d9ec44a2545a684ccfb042ddf07aa36c7f911190d926f71adc7336e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c73a0693-7315-48ae-ad8f-274d5b766b4d", "node_type": "1", "metadata": {"window": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. ", "original_text": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. "}, "hash": "b534987c81b14fc47b09ae92d1bcfb46699fe6f2570e5bfb740e4076da4def86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n", "mimetype": "text/plain", "start_char_idx": 54894, "end_char_idx": 55008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c73a0693-7315-48ae-ad8f-274d5b766b4d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. ", "original_text": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e23c1f00-3cb2-4233-8361-b431ec784ba2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance for concentric points around a normally distributed 3-D blob. **\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n", "original_text": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n"}, "hash": "2b5de28afe35ac8b35e6cf617e7ac0ecce3e9cdd71996352e4e8645766785f55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42d52374-a9cc-41a8-aa61-e08e8bc86a60", "node_type": "1", "metadata": {"window": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance. ", "original_text": "18. "}, "hash": "8a62815627ad02c71cb32384db113f8acc4e4972ea2d30120bf1a5a4f3204356", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 55008, "end_char_idx": 55655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42d52374-a9cc-41a8-aa61-e08e8bc86a60", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance. ", "original_text": "18. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c73a0693-7315-48ae-ad8f-274d5b766b4d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) 3-D Blob, mean of the scores: A line plot of mean score versus radius.  Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. ", "original_text": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig. "}, "hash": "c0205db9a8d7a2a4ae0f0b56809a78b95ce4990f4dd3db3e5c11982e13cfee39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7b177f2-ba06-4c20-87c7-42a86d84323e", "node_type": "1", "metadata": {"window": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n", "original_text": "Mean and variance for concentric points around a normally distributed 4-D blob."}, "hash": "061257ae54403489bd167270397a687a215e2363e58f55ce3056676922d67ff7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18. ", "mimetype": "text/plain", "start_char_idx": 55655, "end_char_idx": 55659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7b177f2-ba06-4c20-87c7-42a86d84323e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n", "original_text": "Mean and variance for concentric points around a normally distributed 4-D blob."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42d52374-a9cc-41a8-aa61-e08e8bc86a60", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Three lines (Standard, Ex 1, Extended) follow a similar increasing trend.\n (b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance. ", "original_text": "18. "}, "hash": "192ea2e3929d827fb2dff92e57d19edd337e31f83e04726b3048fb29a1af6d68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "683259d2-dddc-496a-aff8-f4146b3fc793", "node_type": "1", "metadata": {"window": "The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs. ", "original_text": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. "}, "hash": "62bcb112a16ed57af44667c2af7ac11f880ead280c24904e4fe5b35be5ab32a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and variance for concentric points around a normally distributed 4-D blob.", "mimetype": "text/plain", "start_char_idx": 55659, "end_char_idx": 55738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "683259d2-dddc-496a-aff8-f4146b3fc793", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs. ", "original_text": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7b177f2-ba06-4c20-87c7-42a86d84323e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) 3-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n", "original_text": "Mean and variance for concentric points around a normally distributed 4-D blob."}, "hash": "88a26ef97ea7e5e2aedec5be45060e9eea54d0ddbdeefb16778bfa01bae9cb2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3b8ae33-7617-4683-8ef1-ef57287b47de", "node_type": "1", "metadata": {"window": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n", "original_text": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n"}, "hash": "e272ac6b1ba85e97a17a5b60796180279a921aae43bf79dfc1c25a5dded34e24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. ", "mimetype": "text/plain", "start_char_idx": 55738, "end_char_idx": 55816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3b8ae33-7617-4683-8ef1-ef57287b47de", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n", "original_text": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "683259d2-dddc-496a-aff8-f4146b3fc793", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line has the highest variance.  The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs. ", "original_text": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius. "}, "hash": "b706d7cc86e31f52286fea78d30a16cfe9791f569223d507ae18c03924418241", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d995cf1-d142-404d-938d-77dca297b6e8", "node_type": "1", "metadata": {"window": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). ", "original_text": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. "}, "hash": "769623aea556337ae663308d8ba78219dacf00ee920c0719e61f52bd2b10ff15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n", "mimetype": "text/plain", "start_char_idx": 55816, "end_char_idx": 55895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d995cf1-d142-404d-938d-77dca297b6e8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). ", "original_text": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3b8ae33-7617-4683-8ef1-ef57287b47de", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Ex 1\" line has lower variance, and the \"Extended\" line has the lowest variance, especially at larger radii.\n\n **TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n", "original_text": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n"}, "hash": "4910f6a31191b36fd422050136350c2cbcd5ac6affae2a35babbb582dce270ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "702adc41-821b-4693-a9e8-bc5a0720b9bf", "node_type": "1", "metadata": {"window": "18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size. ", "original_text": "The \"Standard\" line has the highest variance. "}, "hash": "e7caa4c77e74cf75a5026bd9bcfe36f1947e29a7a2905960ec01c684364885a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. ", "mimetype": "text/plain", "start_char_idx": 55895, "end_char_idx": 55978, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "702adc41-821b-4693-a9e8-bc5a0720b9bf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size. ", "original_text": "The \"Standard\" line has the highest variance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d995cf1-d142-404d-938d-77dca297b6e8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**TABLE 1: AUC Values for Both ROC and PRC for Example Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Single Blob | 0.919 | 0.999 | 0.800 | 0.999 |\n| Double Blob | 0.869 | 0.999 | 0.303 | 0.997 |\n| Sinusoid | 0.809 | 0.924 | 0.430 | 0.504 |\n\n**TABLE 2: Table of Data Properties**\n\n| | Size | Dimension | % Anomaly |\n| :--- | :--- | :--- | :--- |\n| Cardio | 1831 | 21 | 9.6 |\n| ForestCover | 286048 | 10 | 0.9 |\n| Ionosphere | 351 | 33 | 36 |\n| Mammography | 11183 | 6 | 2.32 |\n| Satellite | 6435 | 36 | 32 |\n\n**Fig.  18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). ", "original_text": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius. "}, "hash": "3e31a4fdc1d46047dc4957dd3d1b587b452c59612ea49afadcf48b87f954d603", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5aa8005-6671-4659-a5e9-11ded76d7230", "node_type": "1", "metadata": {"window": "Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small. ", "original_text": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n"}, "hash": "fd51a79525ac124924b5eb79a80cd771f39f8e1c18f0159f2a1c7c6708d7a1ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Standard\" line has the highest variance. ", "mimetype": "text/plain", "start_char_idx": 55978, "end_char_idx": 56024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f5aa8005-6671-4659-a5e9-11ded76d7230", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small. ", "original_text": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "702adc41-821b-4693-a9e8-bc5a0720b9bf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "18.  Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size. ", "original_text": "The \"Standard\" line has the highest variance. "}, "hash": "820b5eddac9ca23b5d3a2b02163e24aee653f5da53d0e03949eac42a696d9131", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55731e7c-ebc7-4179-bf94-37204ce9c292", "node_type": "1", "metadata": {"window": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. ", "original_text": "***\n\nFigs. "}, "hash": "48adf974719789d12b27b5d25e6ba64f416f991d4d3ef6d840c150eb1a018c3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n", "mimetype": "text/plain", "start_char_idx": 56024, "end_char_idx": 56128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "55731e7c-ebc7-4179-bf94-37204ce9c292", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. ", "original_text": "***\n\nFigs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5aa8005-6671-4659-a5e9-11ded76d7230", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mean and variance for concentric points around a normally distributed 4-D blob. **\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small. ", "original_text": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n"}, "hash": "77aee7638d8d7cb3322739d1d9cb8fe9b3f1046c4a0ac9588b7e14b5a5aa5e96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf1668de-ca20-49ad-a223-70e2b9114e83", "node_type": "1", "metadata": {"window": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n", "original_text": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n"}, "hash": "41bf48c4106e8760ef48b60d75f53ae56844285f02a9860751be2cc89dc94a27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nFigs. ", "mimetype": "text/plain", "start_char_idx": 56128, "end_char_idx": 56139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf1668de-ca20-49ad-a223-70e2b9114e83", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n", "original_text": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55731e7c-ebc7-4179-bf94-37204ce9c292", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) 4-D Blob, mean of the scores: A line plot of mean score versus radius.  Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. ", "original_text": "***\n\nFigs. "}, "hash": "470044102e720c9910178fff8026f4a6f24853ad347637780a45670d87ca7895", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a4a9749-3ab1-419d-9742-def4b03fd4e9", "node_type": "1", "metadata": {"window": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. ", "original_text": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). "}, "hash": "083386bef89354a592874538ef24f8919e601676e7f7b90576c13ed89e5613a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n", "mimetype": "text/plain", "start_char_idx": 56139, "end_char_idx": 56261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a4a9749-3ab1-419d-9742-def4b03fd4e9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. ", "original_text": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf1668de-ca20-49ad-a223-70e2b9114e83", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Four lines (Standard, Ex 1, Ex 2, Extended) follow a similar increasing trend.\n (b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n", "original_text": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n"}, "hash": "e6fd4b829ed4deea1d3dff221e9f07a2832bcab8bb760c05015a0b888d944487", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5203fbca-f998-4877-93a4-8e2da9bae9cd", "node_type": "1", "metadata": {"window": "The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. ", "original_text": "As we can see, in all the cases the score settles very quickly as a function of the forest size. "}, "hash": "1a6f9ef115ad6029e2fba8a0e58d087bff76e101cb4e120fda65092e165589e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). ", "mimetype": "text/plain", "start_char_idx": 56261, "end_char_idx": 56440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5203fbca-f998-4877-93a4-8e2da9bae9cd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. ", "original_text": "As we can see, in all the cases the score settles very quickly as a function of the forest size. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a4a9749-3ab1-419d-9742-def4b03fd4e9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) 4-D Blob, variance of the scores: A line plot of score variance versus radius.  The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. ", "original_text": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres). "}, "hash": "ea904609e72e4d355a2744f162494e050a3adda93d1eccd4970920ba81195d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b97200f-a096-47f9-a982-dca0dba4ce0c", "node_type": "1", "metadata": {"window": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n", "original_text": "There might be a slight difference between the nominal and anomalous points but the differences are very small. "}, "hash": "19eb7217bc188a593fac9a3fd04a5f824a59df13e157764835e0a166cd0cb1e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we can see, in all the cases the score settles very quickly as a function of the forest size. ", "mimetype": "text/plain", "start_char_idx": 56440, "end_char_idx": 56537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b97200f-a096-47f9-a982-dca0dba4ce0c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n", "original_text": "There might be a slight difference between the nominal and anomalous points but the differences are very small. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5203fbca-f998-4877-93a4-8e2da9bae9cd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The \"Standard\" line has the highest variance.  The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. ", "original_text": "As we can see, in all the cases the score settles very quickly as a function of the forest size. "}, "hash": "2b9b32cf1e2cd84c3e57bb8dc886e6569749b8bad373b6785be97562ce89c8c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40b9d52e-2de0-49e7-a8a5-106607e5671c", "node_type": "1", "metadata": {"window": "***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. ", "original_text": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. "}, "hash": "6ce2c9d726b385bb034240976c9f21b11b27fad22b67e9058802be332642de87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There might be a slight difference between the nominal and anomalous points but the differences are very small. ", "mimetype": "text/plain", "start_char_idx": 56537, "end_char_idx": 56649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40b9d52e-2de0-49e7-a8a5-106607e5671c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. ", "original_text": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b97200f-a096-47f9-a982-dca0dba4ce0c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The variance decreases progressively for \"Ex 1\", \"Ex 2\", and \"Extended\", which has the lowest variance.\n ***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n", "original_text": "There might be a slight difference between the nominal and anomalous points but the differences are very small. "}, "hash": "16873d252a2ac9f0dbb87668fe55212e24db378858bfa5cfbeb78ed79a1d20e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e436ccb8-e5ae-441f-afe4-a3b85a502db7", "node_type": "1", "metadata": {"window": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. ", "original_text": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n"}, "hash": "38da75a97bcadf10e56a774b0553f422517a51884641c13aec97a63e34d73e2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. ", "mimetype": "text/plain", "start_char_idx": 56649, "end_char_idx": 56801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e436ccb8-e5ae-441f-afe4-a3b85a502db7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. ", "original_text": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40b9d52e-2de0-49e7-a8a5-106607e5671c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nFigs.  20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. ", "original_text": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels. "}, "hash": "8d7c900a08ce03352e741687e0c034da9333cf7d6aec48ee6d3db99a197f93c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6c29510-7251-478b-a9d7-fa981d39c8c9", "node_type": "1", "metadata": {"window": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself. ", "original_text": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. "}, "hash": "66a614d80dcd9eb49aba3f1b82c41a2d0cc637e1c41a41bea658989812450093", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n", "mimetype": "text/plain", "start_char_idx": 56801, "end_char_idx": 57080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6c29510-7251-478b-a9d7-fa981d39c8c9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself. ", "original_text": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e436ccb8-e5ae-441f-afe4-a3b85a502db7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "20 and 21 show the same plots for the 3-D blob and 4-D blobs respectively, except for the Rotated Isolation Forest case.\n\n For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. ", "original_text": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n"}, "hash": "0a35b2ea3185e9ebbf52dad7efa1636cdfe78bcdb8fbf958f6e3e9d4c905f5a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a39378e9-d21f-4b86-bc96-83430d084a97", "node_type": "1", "metadata": {"window": "As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features. ", "original_text": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. "}, "hash": "d34ab64cf753396e75365b1a1dbd57d3e048f784d26c56ffd80d186add880cf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 57080, "end_char_idx": 57240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a39378e9-d21f-4b86-bc96-83430d084a97", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features. ", "original_text": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6c29510-7251-478b-a9d7-fa981d39c8c9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For the convergence plots, the error bars show the variance among the scores of 500 data points tested along constant level sets as before (concentric shells of 2 and 3-spheres).  As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself. ", "original_text": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest. "}, "hash": "0b63d3252a79a93bc9e133cf4ac2484762185353fc228dd48b6bdb8e3b5ad0cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61ab46a2-a8db-435a-9f5e-2b75a1cb6f90", "node_type": "1", "metadata": {"window": "There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. ", "original_text": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n"}, "hash": "560b40e13df0be94a3e1ceba1a10db7b37474d8139473f3d64d74dd2864b4205", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. ", "mimetype": "text/plain", "start_char_idx": 57240, "end_char_idx": 57371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61ab46a2-a8db-435a-9f5e-2b75a1cb6f90", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. ", "original_text": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a39378e9-d21f-4b86-bc96-83430d084a97", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "As we can see, in all the cases the score settles very quickly as a function of the forest size.  There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features. ", "original_text": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section. "}, "hash": "f8056186bfdc65951cd00e63d579f88c41629c07eb117301c416ba7b3eef2ed6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "752732f3-c4f2-4fef-a13f-f18b207eb53e", "node_type": "1", "metadata": {"window": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n", "original_text": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. "}, "hash": "3aa5327f0ba0e126cad85978ac241cd0e801e79c4259b3ef935aff8da3a5b49a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n", "mimetype": "text/plain", "start_char_idx": 57371, "end_char_idx": 57475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "752732f3-c4f2-4fef-a13f-f18b207eb53e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n", "original_text": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61ab46a2-a8db-435a-9f5e-2b75a1cb6f90", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "There might be a slight difference between the nominal and anomalous points but the differences are very small.  Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. ", "original_text": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n"}, "hash": "6f4ea6292542dab845eabb0242b08232b4ee756db6e9b3573cf1d494014a0b3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c847c677-35c1-4e72-849b-ee3c85aee9db", "node_type": "1", "metadata": {"window": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches. ", "original_text": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. "}, "hash": "750172423527f96aaabede4a002db507f6742450d595637d88be84ee18b25fb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. ", "mimetype": "text/plain", "start_char_idx": 57475, "end_char_idx": 57589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c847c677-35c1-4e72-849b-ee3c85aee9db", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches. ", "original_text": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "752732f3-c4f2-4fef-a13f-f18b207eb53e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Moreover we find no appreciable difference between the standard Isolation Forest and the Extended Isolation Forest, including all the extension levels.  In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n", "original_text": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9]. "}, "hash": "4a5a929497807aae2e495d6dcdc25510f2148b058f855e9f451c5000586055d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e81f6b2d-badd-40d5-b616-1884b6bb7d81", "node_type": "1", "metadata": {"window": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. ", "original_text": "We showed these artifacts can be traced to the branching procedure in the algorithm itself. "}, "hash": "58c68d281a5fd65313e634bc06564d6cecd64c8ace91b82ddf3bdf0ae1b47607", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. ", "mimetype": "text/plain", "start_char_idx": 57589, "end_char_idx": 57724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e81f6b2d-badd-40d5-b616-1884b6bb7d81", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. ", "original_text": "We showed these artifacts can be traced to the branching procedure in the algorithm itself. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c847c677-35c1-4e72-849b-ee3c85aee9db", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In the case of standard Isolation Forest, the variances are much higher, consistent with the results found in the previous subsection, but in terms of rates of convergence, there is not much difference which indicates that the number of trees on each method should be the same.\n\n We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches. ", "original_text": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts. "}, "hash": "727ddcad2073e6be4a0a064c2cc1e661d0a5346b084a348b24c992aea8ef05ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43ef058d-eecc-4953-8e0f-ee7de9482658", "node_type": "1", "metadata": {"window": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. ", "original_text": "The branching procedure followed slicing data along random values of randomly selected features. "}, "hash": "3030a3078e7fe6b3b85e251755a5a136b2453ba8c3cb4c941a5d2c5ba830076b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We showed these artifacts can be traced to the branching procedure in the algorithm itself. ", "mimetype": "text/plain", "start_char_idx": 57724, "end_char_idx": 57816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "43ef058d-eecc-4953-8e0f-ee7de9482658", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. ", "original_text": "The branching procedure followed slicing data along random values of randomly selected features. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e81f6b2d-badd-40d5-b616-1884b6bb7d81", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We have seen that the Extended Isolation Forest is able to improve on the robustness of the scores generated compared to the standard case of Isolation Forest.  It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. ", "original_text": "We showed these artifacts can be traced to the branching procedure in the algorithm itself. "}, "hash": "c197b1951dd13c566f9a9a1686464a2938afadb0506c7e06008b5c1dc0b98d01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4bf2a39-5f81-49b0-8ba8-5cb3b4c1c136", "node_type": "1", "metadata": {"window": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. ", "original_text": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. "}, "hash": "33e97feaccfb17984855272f077232509a434c91aece50397c18f9e1dc0af53e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The branching procedure followed slicing data along random values of randomly selected features. ", "mimetype": "text/plain", "start_char_idx": 57816, "end_char_idx": 57913, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4bf2a39-5f81-49b0-8ba8-5cb3b4c1c136", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. ", "original_text": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43ef058d-eecc-4953-8e0f-ee7de9482658", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "It accomplishes this without sacrificing computational efficiency as evidenced by the convergence plots presented in this section.  The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. ", "original_text": "The branching procedure followed slicing data along random values of randomly selected features. "}, "hash": "b9dbc39f87c6fbf57c90d77fc2486e72a16bf521c806c8fec89523a71fea6c50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97c3699d-c685-4bcf-8c9e-919133ea3aee", "node_type": "1", "metadata": {"window": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. ", "original_text": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n"}, "hash": "417d2d81673914bb306ba8d1f51fced5f4c22ab3d08b2266109d0b9ecfcd23fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. ", "mimetype": "text/plain", "start_char_idx": 57913, "end_char_idx": 58071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97c3699d-c685-4bcf-8c9e-919133ea3aee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. ", "original_text": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4bf2a39-5f81-49b0-8ba8-5cb3b4c1c136", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The fully extended case seems to produce the best results when higher dimensional data are considered.\n\n ## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. ", "original_text": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame. "}, "hash": "78dfd32a1fa5e1435176ce4dfb25fa64822f2eb3d30fadc45587a6de27576256", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c515831-5e05-4d34-b464-2ac44903572c", "node_type": "1", "metadata": {"window": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n", "original_text": "In order to remedy the situation we proposed two different approaches. "}, "hash": "a9feafb8d038f83b89a0c9577a96b980a4dbf48f06e4580633c6ca557c106b39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n", "mimetype": "text/plain", "start_char_idx": 58071, "end_char_idx": 58323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c515831-5e05-4d34-b464-2ac44903572c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n", "original_text": "In order to remedy the situation we proposed two different approaches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97c3699d-c685-4bcf-8c9e-919133ea3aee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## 5 CONCLUSIONS\nWe have presented an extension to the anomaly detection algorithm known as Isolation Forest [9].  The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. ", "original_text": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n"}, "hash": "9c17502b4051683ac45ecea9f209bd12d1724b7608a8d0d0acec2981479842cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "072925e5-e486-4f52-8791-cdb685076691", "node_type": "1", "metadata": {"window": "We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples. ", "original_text": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. "}, "hash": "76817091a73e2429d4d807092c995dbe262280e189c803a9f530b36ae379a861", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to remedy the situation we proposed two different approaches. ", "mimetype": "text/plain", "start_char_idx": 58323, "end_char_idx": 58394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "072925e5-e486-4f52-8791-cdb685076691", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples. ", "original_text": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c515831-5e05-4d34-b464-2ac44903572c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The motivation for the study arose from the observation that score maps in two dimensional datasets demonstrated unexpected artifacts.  We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n", "original_text": "In order to remedy the situation we proposed two different approaches. "}, "hash": "228249894e26f1e6a3792ef8ca86c4258d182d3a8b62c5738f92dc4396c86c17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de496bf3-505f-461e-848e-2ecbc6c6c44a", "node_type": "1", "metadata": {"window": "The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. ", "original_text": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. "}, "hash": "27b83fb3068a8ab3d5ce70ed1ebc81972b94585e8fb6c8f1a38625749d5258ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. ", "mimetype": "text/plain", "start_char_idx": 58394, "end_char_idx": 58512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de496bf3-505f-461e-848e-2ecbc6c6c44a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. ", "original_text": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "072925e5-e486-4f52-8791-cdb685076691", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We showed these artifacts can be traced to the branching procedure in the algorithm itself.  The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples. ", "original_text": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree. "}, "hash": "b22e7edc63ef6c91b56d6ae44b76b482f1f46162114e2f61c6b4628290741c3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "646cce37-1692-4c30-aa1d-07e9177681dc", "node_type": "1", "metadata": {"window": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. ", "original_text": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. "}, "hash": "e43549b00a69509fd03bbc0d177706ca38f20a9921635528efedb205349e7ea9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. ", "mimetype": "text/plain", "start_char_idx": 58512, "end_char_idx": 58679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "646cce37-1692-4c30-aa1d-07e9177681dc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. ", "original_text": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de496bf3-505f-461e-848e-2ecbc6c6c44a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The branching procedure followed slicing data along random values of randomly selected features.  This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. ", "original_text": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame. "}, "hash": "88cc4a3b1777a7da44d075d25ee0ea47110c9988dca009da5d328641079e3daf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb794a9b-0a8c-41f0-bbb5-42c28b47564c", "node_type": "1", "metadata": {"window": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n", "original_text": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. "}, "hash": "766738565e7902acd746b9df5fc34ac1a31073534d8a778059735a315b4d1904", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 58679, "end_char_idx": 58793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb794a9b-0a8c-41f0-bbb5-42c28b47564c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n", "original_text": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "646cce37-1692-4c30-aa1d-07e9177681dc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This introduced a bias in terms of the number of branching operations performed based on the location of the data point with respect to the coordinate frame.  Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. ", "original_text": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest. "}, "hash": "14eef56d514e6cc95a5f01c450cbdc63b8415522a99cbff59721fbff04951459", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1b26dd1-8bc7-4fb1-8ec9-be15c2cab10f", "node_type": "1", "metadata": {"window": "In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. ", "original_text": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n"}, "hash": "e30ff712edd12d16097abe08f20314c2004da16717f18955c597e7aa6e23f00c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 58793, "end_char_idx": 59008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b1b26dd1-8bc7-4fb1-8ec9-be15c2cab10f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. ", "original_text": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb794a9b-0a8c-41f0-bbb5-42c28b47564c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Since the length of tree branches are used directly in computing the anomaly scores, we saw regions in the domain that showed inconsistent anomaly scores, introducing artificial zones of higher/lower scores which are not present in the original data.\n\n In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n", "original_text": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest. "}, "hash": "14104c882c6fbaaa0be9e2b1dc3a11345532d18b7f408b014ac5ad1522b604d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "838ef68c-f10b-4922-a180-14de84398f73", "node_type": "1", "metadata": {"window": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19. ", "original_text": "We showed score maps for various examples. "}, "hash": "7a9fa39df729e7361bd61dd75393cc52034df022252a1077df1e363ae0b6c40d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n", "mimetype": "text/plain", "start_char_idx": 59008, "end_char_idx": 59158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "838ef68c-f10b-4922-a180-14de84398f73", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19. ", "original_text": "We showed score maps for various examples. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1b26dd1-8bc7-4fb1-8ec9-be15c2cab10f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "In order to remedy the situation we proposed two different approaches.  First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. ", "original_text": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n"}, "hash": "2c89d8c463a4b8c5f207bc7d069ec6df1580e4c72d0c7b36df31dc3949c10ec6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9315740-a901-420a-b07d-147be5d3fda9", "node_type": "1", "metadata": {"window": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text.", "original_text": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. "}, "hash": "2d0129fb40d4a9c771d91842f7cef6da6b93c37cba3d8e0d6e83801c110db150", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We showed score maps for various examples. ", "mimetype": "text/plain", "start_char_idx": 59158, "end_char_idx": 59201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9315740-a901-420a-b07d-147be5d3fda9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text.", "original_text": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "838ef68c-f10b-4922-a180-14de84398f73", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "First we showed the score maps can be improved if the data undergoes a rotation before the construction of each tree.  The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19. ", "original_text": "We showed score maps for various examples. "}, "hash": "d0b5fe8e63e4bf0456c39f25e6d9152232d3188a1c12a925f1dcaa757fa1db0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19990522-8ddb-4f9e-bda8-0a99cfb434f1", "node_type": "1", "metadata": {"window": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs. ", "original_text": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. "}, "hash": "8f173e7917d28fd38e32515fa1341dee066ebf0b0360abb837db2660a63e94bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. ", "mimetype": "text/plain", "start_char_idx": 59201, "end_char_idx": 59497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19990522-8ddb-4f9e-bda8-0a99cfb434f1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs. ", "original_text": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9315740-a901-420a-b07d-147be5d3fda9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The second approach, Extended Isolation Forest, allows the branching hyperplanes to take on any slope as opposed to hyperplanes only parallel to the coordinate frame.  This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text.", "original_text": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased. "}, "hash": "0b893f029d2c3910c5d839e90249f681c447fa05a8a65943d114d9f174955769", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e51f5cd-fb5b-4a34-a3ee-597a63a4d64e", "node_type": "1", "metadata": {"window": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees. ", "original_text": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n"}, "hash": "ba622356c2ac917de289913eff1e7c947e8759ba1549b40cdfe39721b653bff1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. ", "mimetype": "text/plain", "start_char_idx": 59497, "end_char_idx": 59621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e51f5cd-fb5b-4a34-a3ee-597a63a4d64e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees. ", "original_text": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19990522-8ddb-4f9e-bda8-0a99cfb434f1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "This extension in the algorithm completely resolves the bias introduced in the case of standard Isolation Forest.  We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs. ", "original_text": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data. "}, "hash": "4978acdaca416a81fdc174343255c6222d892a67864542653ad0a602f12bf481", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e0872bd-19b2-4088-9ecb-50522caac4b6", "node_type": "1", "metadata": {"window": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n", "original_text": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. "}, "hash": "33aa732b9c03ea2e080c6681df4763a87edc4bb8770900b7d439ae802d03ae4f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n", "mimetype": "text/plain", "start_char_idx": 59621, "end_char_idx": 59720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e0872bd-19b2-4088-9ecb-50522caac4b6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n", "original_text": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e51f5cd-fb5b-4a34-a3ee-597a63a4d64e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We show the algorithm is readily extended to high dimensional datasets, while it possesses Ex = N \u2212 1 levels of extensions for an N dimensional dataset, with Ex = 0 being identical to the standard Isolation Forest.  All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees. ", "original_text": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n"}, "hash": "df76ccb0e27ddaec2fea56618321d05a4b3cdf4ec706e4db16ef8150f8f33f41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6af66fa0-59bb-48c4-b3ee-4deeeba69c77", "node_type": "1", "metadata": {"window": "We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n", "original_text": "19. "}, "hash": "c2a9decd2972d277d1d666946ff926bd1d35996769ca4ec2087acf2284f89f28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 59720, "end_char_idx": 60198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6af66fa0-59bb-48c4-b3ee-4deeeba69c77", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n", "original_text": "19. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e0872bd-19b2-4088-9ecb-50522caac4b6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "All of these extension levels show better performance than the standard case, and we suggest the EIF should be preferred over the rotation approach.\n\n We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n", "original_text": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig. "}, "hash": "3d2a269642ffa4fb0ceafacc2e5ff57b139c70a5609d6915713235c6417e1561", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "412214a4-5460-4393-8781-9e0b19ae3601", "node_type": "1", "metadata": {"window": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n", "original_text": "Convergence plots for the 2-D Blob case and the three methods described in the text."}, "hash": "762565b6cea9957d69663a37e390514d21b4a2545bd8cd781432b5da1115a1a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19. ", "mimetype": "text/plain", "start_char_idx": 60198, "end_char_idx": 60202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "412214a4-5460-4393-8781-9e0b19ae3601", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n", "original_text": "Convergence plots for the 2-D Blob case and the three methods described in the text."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6af66fa0-59bb-48c4-b3ee-4deeeba69c77", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We showed score maps for various examples.  We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n", "original_text": "19. "}, "hash": "594a1e75066edcbe10965fce96bfd8c6c5bb7130c1be0484068453c40cf4fc50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4eb21487-f320-42a3-ab40-8791334d0ca8", "node_type": "1", "metadata": {"window": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig. ", "original_text": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs. "}, "hash": "2f79a7cc20c1263703defa019eacc9478f2da26e7a51139123c93c634190bf4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Convergence plots for the 2-D Blob case and the three methods described in the text.", "mimetype": "text/plain", "start_char_idx": 60202, "end_char_idx": 60286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4eb21487-f320-42a3-ab40-8791334d0ca8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig. ", "original_text": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "412214a4-5460-4393-8781-9e0b19ae3601", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We also showed that for the region of anomaly, the standard Isolation Forest algorithm produces high variance in the scores along constant level sets of anomaly scores, while the Extended Isolation Forest resulted in remarkably smaller variances which decreased as the extension level increased.  We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n", "original_text": "Convergence plots for the 2-D Blob case and the three methods described in the text."}, "hash": "349736ba15e94e535c6d905345d9e366cbdd50ceb8ec38b3f25425b3e536e4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f54b49f4-f517-4f84-9e35-801db5180ed0", "node_type": "1", "metadata": {"window": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20. ", "original_text": "number of trees. "}, "hash": "0f8c22139adf0dcbc98102696906d351ebbbf17487590aba1069cb21f51d41af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs. ", "mimetype": "text/plain", "start_char_idx": 60286, "end_char_idx": 60348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f54b49f4-f517-4f84-9e35-801db5180ed0", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20. ", "original_text": "number of trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4eb21487-f320-42a3-ab40-8791334d0ca8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We presented AUC for ROC and PRC for all the examples considered in the paper as well as various real world benchmark data.  The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig. ", "original_text": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs. "}, "hash": "0059c41b915b34ee7f7c93204ee0d8a348487c3c9ee59588410565ef6a262e28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98454eb0-a858-4bec-a201-3510c5a6eea4", "node_type": "1", "metadata": {"window": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases.", "original_text": "The score converges quickly, but the error bars (representing variance) are large.\n"}, "hash": "b59e6580d8477a48279f8267376d6acaf87d15e237caf42b0ad2b167e6644589", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "number of trees. ", "mimetype": "text/plain", "start_char_idx": 60348, "end_char_idx": 60365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98454eb0-a858-4bec-a201-3510c5a6eea4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases.", "original_text": "The score converges quickly, but the error bars (representing variance) are large.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f54b49f4-f517-4f84-9e35-801db5180ed0", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The EIF performed consistently better than the standard Isolation Forest in all cases considered.\n\n ***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20. ", "original_text": "number of trees. "}, "hash": "b7bf60ccba881bfe6093962a100cae6fa28034f51ffcfed55805ed727748f48f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0471f8f0-bc04-4de3-9d82-42cb016ce4f8", "node_type": "1", "metadata": {"window": "19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "original_text": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n"}, "hash": "0c521d33f52a1922381c4765033fa4a7b5ee95939621af38b68ce9ebcec050e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The score converges quickly, but the error bars (representing variance) are large.\n", "mimetype": "text/plain", "start_char_idx": 60365, "end_char_idx": 60448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0471f8f0-bc04-4de3-9d82-42cb016ce4f8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "original_text": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98454eb0-a858-4bec-a201-3510c5a6eea4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n**TABLE 3: AUC Values for Both ROC and PRC for Benchmark Datasets Using Standard Isolation Forest and Extended Isolation Forest**\n\n| Data | AUC ROC | AUC PRC |\n| :--- | :--- | :--- | :--- | :--- |\n| | iForest | EIF | iForest | EIF |\n| Cardio | 0.888 | 0.915 | 0.466 | 0.483 |\n| ForestCover | 0.809 | 0.924 | 0.430 | 0.504 |\n| Ionosphere | 0.85 | 0.913 | 0.877 | 0.893 |\n| Mammography | 0.859 | 0.862 | 0.4198 | 0.4271 |\n| Satellite | 0.714 | 0.778 | 0.783 | 0.808 |\n\n**Fig.  19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases.", "original_text": "The score converges quickly, but the error bars (representing variance) are large.\n"}, "hash": "a3144329d4010a248891b14d3e01b6920186f5aa657040612586c9820ba6a287", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4d70fef-6f46-4d8f-b531-55ca3d57a9f6", "node_type": "1", "metadata": {"window": "Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n", "original_text": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n"}, "hash": "85e927b041d799c29d5351570403afb607357c456e4cf016acf2796135b15a01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n", "mimetype": "text/plain", "start_char_idx": 60448, "end_char_idx": 60558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4d70fef-6f46-4d8f-b531-55ca3d57a9f6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n", "original_text": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0471f8f0-bc04-4de3-9d82-42cb016ce4f8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "19.  Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "original_text": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n"}, "hash": "81f50a73e18bbbcf1e1c0a572c86469c8712db5174cbce89e0b7c29f1856e52b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8afae5b-8292-4162-bc9c-76770b780cef", "node_type": "1", "metadata": {"window": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig. ", "original_text": "**Fig. "}, "hash": "13614eb3cdff43cd6495d9fd287fe39b027d05fc3796c17c46658a0226c6cca0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n", "mimetype": "text/plain", "start_char_idx": 60558, "end_char_idx": 60696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8afae5b-8292-4162-bc9c-76770b780cef", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4d70fef-6f46-4d8f-b531-55ca3d57a9f6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Convergence plots for the 2-D Blob case and the three methods described in the text. **\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n", "original_text": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n"}, "hash": "5fe35423aac08ba8862216963570c898d57e1e868567adac58d6bd30e7d9d78a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4506ab65-f140-4ec7-ad3b-9d596a0cc6c2", "node_type": "1", "metadata": {"window": "number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21. ", "original_text": "20. "}, "hash": "d4f9eba6c2475dee217625a10bdcae5d8cc22cc637d4918fd18498c1f594c261", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 60696, "end_char_idx": 60703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4506ab65-f140-4ec7-ad3b-9d596a0cc6c2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21. ", "original_text": "20. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8afae5b-8292-4162-bc9c-76770b780cef", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard Isolation Forest: A plot of anomaly score vs.  number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig. ", "original_text": "**Fig. "}, "hash": "5b0a681d49e13c238187204aa0d9a6e34826501c048206f7ba3b62f848771661", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa6296c2-d472-44dd-aa35-8145cf03e93c", "node_type": "1", "metadata": {"window": "The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases.", "original_text": "Convergence plots for the 3-D blob for the standard and fully extended cases."}, "hash": "4d4aac5d92e6c1b45c11add085b64053cd1b47a6c4397e5feec733fc1f490237", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "20. ", "mimetype": "text/plain", "start_char_idx": 60703, "end_char_idx": 60707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fa6296c2-d472-44dd-aa35-8145cf03e93c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases.", "original_text": "Convergence plots for the 3-D blob for the standard and fully extended cases."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4506ab65-f140-4ec7-ad3b-9d596a0cc6c2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "number of trees.  The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21. ", "original_text": "20. "}, "hash": "845a22789ade092486cf507ffb4d75385e473a9ff1da4da0002e573ee4415256", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f42a21a-8b1f-4f21-a108-12b15048bc1c", "node_type": "1", "metadata": {"window": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "original_text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n"}, "hash": "3313ae9dbc5af566d3e24cd352b7e78a9d4a8e750ee8a80d6b3dfbdbeb8fcabb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Convergence plots for the 3-D blob for the standard and fully extended cases.", "mimetype": "text/plain", "start_char_idx": 60707, "end_char_idx": 60784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f42a21a-8b1f-4f21-a108-12b15048bc1c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "original_text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa6296c2-d472-44dd-aa35-8145cf03e93c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "The score converges quickly, but the error bars (representing variance) are large.\n (b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases.", "original_text": "Convergence plots for the 3-D blob for the standard and fully extended cases."}, "hash": "ff25fd8767667ad3a2a49610ff38f323ef2d08a55af37a6ca1410d6f5891770e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ed35b8a-3502-4bd0-9307-f6927a3e0340", "node_type": "1", "metadata": {"window": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n", "original_text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n"}, "hash": "7169968e93d6188e71246771808af2bfe03d709edb2ea8916ccf607f021b3bdb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "mimetype": "text/plain", "start_char_idx": 60784, "end_char_idx": 60887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ed35b8a-3502-4bd0-9307-f6927a3e0340", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n", "original_text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f42a21a-8b1f-4f21-a108-12b15048bc1c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Rotated Isolation Forest: A similar convergence plot, but with smaller error bars than the standard case.\n (c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "original_text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n"}, "hash": "b83da3b92c073838bf32092b7fcef11d400b68ee85a1b22f2fbf80c777facee1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d185c5a-74c8-44b7-a232-392f10631f31", "node_type": "1", "metadata": {"window": "**Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. ", "original_text": "**Fig. "}, "hash": "c7fb04f4578b08bb2d6d87cec88dc70e2d0b80a91e3021dea9ebdffb4669aaa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n", "mimetype": "text/plain", "start_char_idx": 60887, "end_char_idx": 61014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6d185c5a-74c8-44b7-a232-392f10631f31", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ed35b8a-3502-4bd0-9307-f6927a3e0340", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(c) Extended Isolation Forest: A similar convergence plot, with the smallest error bars of the three, indicating the most stable scores.\n\n **Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n", "original_text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n"}, "hash": "66f362676576fd007fe3d291c13c041277858887a8c65b15943ae50aa1298d10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "189ba653-65d9-48fe-8396-d41afeca2d06", "node_type": "1", "metadata": {"window": "20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency. ", "original_text": "21. "}, "hash": "6f3128dc8368e4457a5c84dcd5a717120537dded09edaa856b8f2bc7d9e8d3ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 61014, "end_char_idx": 61021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "189ba653-65d9-48fe-8396-d41afeca2d06", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency. ", "original_text": "21. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d185c5a-74c8-44b7-a232-392f10631f31", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. ", "original_text": "**Fig. "}, "hash": "b6442586db100fdbe29ebc0f23a758a5cb0b79bbcecd90213daddebd20c438f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "290910e5-c702-453d-9d8e-4113592c3924", "node_type": "1", "metadata": {"window": "Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n", "original_text": "Convergence plots for the 4-D blob for the standard and fully extended cases."}, "hash": "fd11dce88011b590f09fb542b73ce40dbf76b0b18783358df388f9f8728b1c96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21. ", "mimetype": "text/plain", "start_char_idx": 61021, "end_char_idx": 61025, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "290910e5-c702-453d-9d8e-4113592c3924", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n", "original_text": "Convergence plots for the 4-D blob for the standard and fully extended cases."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "189ba653-65d9-48fe-8396-d41afeca2d06", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "20.  Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency. ", "original_text": "21. "}, "hash": "884a9840e79c1b23e75da4e0ccfa99a9b47b17a3b3d64f961e060dc7d69b0951", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ed98930-fa76-434c-ae65-4a292e65da36", "node_type": "1", "metadata": {"window": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. ", "original_text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n"}, "hash": "26251d24e7a148d2361603327a3ab35af5f0bf0ffb7861664dfa6b831c2e0a16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Convergence plots for the 4-D blob for the standard and fully extended cases.", "mimetype": "text/plain", "start_char_idx": 61025, "end_char_idx": 61102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ed98930-fa76-434c-ae65-4a292e65da36", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. ", "original_text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "290910e5-c702-453d-9d8e-4113592c3924", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Convergence plots for the 3-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n", "original_text": "Convergence plots for the 4-D blob for the standard and fully extended cases."}, "hash": "5551eb1ce7b26281062e7d34c3edb5ca141ea09f4d43b754b886eb788a491961", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25d7e47f-754a-4bf2-ab19-128a86ecd518", "node_type": "1", "metadata": {"window": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n", "original_text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n"}, "hash": "549fd1417b523629792f7a972ba28cf0fefa17ad62898e3630ca0b6262a0ad71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n", "mimetype": "text/plain", "start_char_idx": 61102, "end_char_idx": 61205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "25d7e47f-754a-4bf2-ab19-128a86ecd518", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n", "original_text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ed98930-fa76-434c-ae65-4a292e65da36", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. ", "original_text": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n"}, "hash": "2861e7eca1cdbc87527df17ca1cd24d266c0605624eef0dd519757ea906a9c29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a94e460-f5b2-4c8d-9b86-06df28eeda31", "node_type": "1", "metadata": {"window": "**Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. ", "original_text": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. "}, "hash": "b5504218c256645dc05f29cf38fd6711f69adbb47d2be13f380d91c5a37ffab2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n", "mimetype": "text/plain", "start_char_idx": 61205, "end_char_idx": 61331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a94e460-f5b2-4c8d-9b86-06df28eeda31", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. ", "original_text": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25d7e47f-754a-4bf2-ab19-128a86ecd518", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n\n **Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n", "original_text": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n"}, "hash": "189a59a93b1a96889059ca6795cf84f0c888c6b752c563373a7c7f489c785bfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "790c6cb7-1fb0-494c-8732-cf4ce239826e", "node_type": "1", "metadata": {"window": "21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int. ", "original_text": "We additionally saw these results can be achieved without sacrificing computational efficiency. "}, "hash": "6eb0430bbb698532ef44efb6e14fd2339eaa7d0cbb62fbd49857d89904b2abfd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. ", "mimetype": "text/plain", "start_char_idx": 61331, "end_char_idx": 61486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "790c6cb7-1fb0-494c-8732-cf4ce239826e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int. ", "original_text": "We additionally saw these results can be achieved without sacrificing computational efficiency. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a94e460-f5b2-4c8d-9b86-06df28eeda31", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Fig.  21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. ", "original_text": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset. "}, "hash": "5d07f79f2d6b8932406f0be0e9f1f5b523f5a11bd7855d325036d90dc8b2efb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b591ab97-1358-4792-819b-451989d5799e", "node_type": "1", "metadata": {"window": "Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf. ", "original_text": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n"}, "hash": "f2d666e2df33ce3fb947d002897e32a27fae219444112637e5b1ee2e363686c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We additionally saw these results can be achieved without sacrificing computational efficiency. ", "mimetype": "text/plain", "start_char_idx": 61486, "end_char_idx": 61582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b591ab97-1358-4792-819b-451989d5799e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf. ", "original_text": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "790c6cb7-1fb0-494c-8732-cf4ce239826e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "21.  Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int. ", "original_text": "We additionally saw these results can be achieved without sacrificing computational efficiency. "}, "hash": "7617aef089ffe3acb75052d9e22a3f348d77b2077d4a1f45e9dcacc9399cc9e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06c55c31-2ee6-4bed-a327-564259052720", "node_type": "1", "metadata": {"window": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp. ", "original_text": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. "}, "hash": "f043df4b882cf4b835fae5f7aa2b088a7974d41693174265524593643f6b6d29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n", "mimetype": "text/plain", "start_char_idx": 61582, "end_char_idx": 61765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06c55c31-2ee6-4bed-a327-564259052720", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp. ", "original_text": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b591ab97-1358-4792-819b-451989d5799e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Convergence plots for the 4-D blob for the standard and fully extended cases. **\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf. ", "original_text": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n"}, "hash": "16dca46b06e94f2a3c03378962f75ba4b3ddd0d84fe3d003b0978a739a18fed5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "205dbe50-a208-40d3-b4ae-6a23037d3aad", "node_type": "1", "metadata": {"window": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n", "original_text": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n"}, "hash": "596ec6164f93686c3558a134b7c6ede3cda14c7d9251a0af8a57995a80443d5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. ", "mimetype": "text/plain", "start_char_idx": 61765, "end_char_idx": 61904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "205dbe50-a208-40d3-b4ae-6a23037d3aad", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n", "original_text": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06c55c31-2ee6-4bed-a327-564259052720", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**\n(a) Standard Isolation Forest: A convergence plot showing a stable mean score but large error bars.\n (b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp. ", "original_text": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation. "}, "hash": "fd8ed530c4342db701029fb88003ec4b142939eeeb37c81f02c9cd2106eb8ed1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbdf012d-9f3a-47ab-bc5c-09d528d21be7", "node_type": "1", "metadata": {"window": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. ", "original_text": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. "}, "hash": "2c411f959251c669fe133f232e2a573c9048d52a68074833a44b41a520ca5857", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n", "mimetype": "text/plain", "start_char_idx": 61904, "end_char_idx": 61992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbdf012d-9f3a-47ab-bc5c-09d528d21be7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. ", "original_text": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "205dbe50-a208-40d3-b4ae-6a23037d3aad", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "(b) Extended Isolation forest: A convergence plot showing a similar stable mean score but with noticeably smaller error bars.\n ***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n", "original_text": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n"}, "hash": "7c6e594ef40e95990277ae544dbb91b9ee47161b980ce20c01a100faf36ab496", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbb57b86-7a44-42d4-a30b-149e778cdc2d", "node_type": "1", "metadata": {"window": "We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell.", "original_text": "IEEE Int. "}, "hash": "4c6c491cd104f5ea84525a0c4a8059703f44dbedc2d63d274a0c7d5bd8459b5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 61992, "end_char_idx": 62169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbb57b86-7a44-42d4-a30b-149e778cdc2d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell.", "original_text": "IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbdf012d-9f3a-47ab-bc5c-09d528d21be7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "***\n\nThe results of this extension are more reliable and robust anomaly scores, and in some cases more accurate detection of structure of a given dataset.  We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. ", "original_text": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc. "}, "hash": "711876c9d35a60cd58dda95f5e23f6a02102a2c3dbeb73d377a67e2d5a86084b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "745cc08f-1ae9-4c12-b130-118a43a7db6f", "node_type": "1", "metadata": {"window": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol. ", "original_text": "Conf. "}, "hash": "8f5f02b56d517e5603455121e897ab3456793ea19b9a5c1aecc2293e22f170aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 62169, "end_char_idx": 62179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "745cc08f-1ae9-4c12-b130-118a43a7db6f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbb57b86-7a44-42d4-a30b-149e778cdc2d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We additionally saw these results can be achieved without sacrificing computational efficiency.  We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell.", "original_text": "IEEE Int. "}, "hash": "a4e6e391fa8b742d87528495ca41c37564557eb74de7e8e566b4589780be9dd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0acbcab2-7224-449d-8539-870edb541114", "node_type": "1", "metadata": {"window": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no. ", "original_text": "Data Mining Workshop*, 2014, pp. "}, "hash": "fb73257685cfca5ef75d0f0840fbc8e9559d686830d453edf7973250b71759c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 62179, "end_char_idx": 62185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0acbcab2-7224-449d-8539-870edb541114", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no. ", "original_text": "Data Mining Workshop*, 2014, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "745cc08f-1ae9-4c12-b130-118a43a7db6f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "We have made a Python implementation of the Extended Isolation Forest algorithm publicly available\u00b2, accompanied by example Jupyter notebooks to reproduce the figures in this paper.\n\n ## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol. ", "original_text": "Conf. "}, "hash": "e492765804315046c4e62830da5fac1437ca5a2c2ad726c4c391baa0daa9570f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9930e26d-ce8e-4f11-a5bd-f31ef5121e19", "node_type": "1", "metadata": {"window": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp. ", "original_text": "698-705.\n"}, "hash": "cf14f0f90500643a2de354e9c8709d03bb3fecbae7c80da76493ba691bf5e822", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining Workshop*, 2014, pp. ", "mimetype": "text/plain", "start_char_idx": 62185, "end_char_idx": 62218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9930e26d-ce8e-4f11-a5bd-f31ef5121e19", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp. ", "original_text": "698-705.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0acbcab2-7224-449d-8539-870edb541114", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## ACKNOWLEDGMENTS\nThe authors would like to thank Seng Keat Yeoh for his useful comments and discussion in regards to the implementation.  MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no. ", "original_text": "Data Mining Workshop*, 2014, pp. "}, "hash": "f700dbb4c824f20e18bf7aaa66c5b9162d9c64322575f96c2142f02cbf1b18fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06c9dbf0-4dfa-414f-94ec-c0a31b7b5511", "node_type": "1", "metadata": {"window": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n", "original_text": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. "}, "hash": "bb02a4f9ece3cbd60c55146db060bc8dea839e7a2e02083e56cef4aaa3b28604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "698-705.\n", "mimetype": "text/plain", "start_char_idx": 62218, "end_char_idx": 62227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06c9dbf0-4dfa-414f-94ec-c0a31b7b5511", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n", "original_text": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9930e26d-ce8e-4f11-a5bd-f31ef5121e19", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "MCK\u2019s work has been supported by Grant projects NSF AST 07-15036 and NSF AST 08-13543.\n\n ## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp. ", "original_text": "698-705.\n"}, "hash": "cde1a119627eb54c3a13bdf6d890831d567efd63416d36de29639efe655b704e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2bd233c-632e-423e-a290-ee19bbb521c7", "node_type": "1", "metadata": {"window": "IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. ", "original_text": "Intell."}, "hash": "46eddb00b82f3b1f3bdccf8a11fea7a9623783da78d759f288c288f7c6120f10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. ", "mimetype": "text/plain", "start_char_idx": 62227, "end_char_idx": 62390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f2bd233c-632e-423e-a290-ee19bbb521c7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. ", "original_text": "Intell."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06c9dbf0-4dfa-414f-94ec-c0a31b7b5511", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "## REFERENCES\n[1] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, and J. R. Wells, \"Efficient anomaly detection by isolation using nearest neighbour ensemble,\" in *Proc.  IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n", "original_text": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput. "}, "hash": "39abe2e0cecadd0483f0a4d0f088dc6c30d8f903f93c02285bc7a2a547f38f94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff858f31-1f70-4057-88e5-4d2a82318a0e", "node_type": "1", "metadata": {"window": "Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol. ", "original_text": "*, vol. "}, "hash": "82964929eed9d1c08385b4d9cdd78959f0b52c3e450056787e980188342f7155", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Intell.", "mimetype": "text/plain", "start_char_idx": 62390, "end_char_idx": 62397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff858f31-1f70-4057-88e5-4d2a82318a0e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol. ", "original_text": "*, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2bd233c-632e-423e-a290-ee19bbb521c7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. ", "original_text": "Intell."}, "hash": "0cdb1134d741f74abf6c1095dd6e15ce2683fe11550f42f4ce7b254c1e298f53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "face6e49-33b9-4c73-b8c6-4f56570724d3", "node_type": "1", "metadata": {"window": "Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no. ", "original_text": "34, no. "}, "hash": "388f04658d35be082165020980bc94c9b074c1fa67ad9f342d1a5fbf4088dd48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, vol. ", "mimetype": "text/plain", "start_char_idx": 62397, "end_char_idx": 62405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "face6e49-33b9-4c73-b8c6-4f56570724d3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no. ", "original_text": "34, no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff858f31-1f70-4057-88e5-4d2a82318a0e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol. ", "original_text": "*, vol. "}, "hash": "5320beb743fd5ece60314b983c59d8357a9e0457a9e98c9eba2b8dc4db52058e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4f55032-59fc-4572-a2af-b6177222d7a5", "node_type": "1", "metadata": {"window": "698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art. ", "original_text": "4, pp. "}, "hash": "9c3cb1530458eb098687add8a62fd2b7119d43de15c66c296af4324c68d3759a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "34, no. ", "mimetype": "text/plain", "start_char_idx": 62405, "end_char_idx": 62413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4f55032-59fc-4572-a2af-b6177222d7a5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art. ", "original_text": "4, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "face6e49-33b9-4c73-b8c6-4f56570724d3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining Workshop*, 2014, pp.  698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no. ", "original_text": "34, no. "}, "hash": "49f5db6938b5c8ad2fac0595cf108261234137a1198231d1b65e5b5f76560fab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30f1327c-7c86-4bd3-bbeb-d05b67f104eb", "node_type": "1", "metadata": {"window": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no. ", "original_text": "968\u2013998, 2018.\n"}, "hash": "abe8a0d09d1433841da0feb51cb9911b59530167aa01cafd709bcc8f08b0b08a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4, pp. ", "mimetype": "text/plain", "start_char_idx": 62413, "end_char_idx": 62420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30f1327c-7c86-4bd3-bbeb-d05b67f104eb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no. ", "original_text": "968\u2013998, 2018.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4f55032-59fc-4572-a2af-b6177222d7a5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "698-705.\n [2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art. ", "original_text": "4, pp. "}, "hash": "aa1310d549b6984abc20614ef15becb39055746f1d432b79d4b89fd150340abe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b5c7598-b43f-43af-a003-546df60e5794", "node_type": "1", "metadata": {"window": "Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n", "original_text": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. "}, "hash": "7a102707322224ee3527be63567a863e46779d50b41539542c999a0ff29b8c52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "968\u2013998, 2018.\n", "mimetype": "text/plain", "start_char_idx": 62420, "end_char_idx": 62435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b5c7598-b43f-43af-a003-546df60e5794", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n", "original_text": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30f1327c-7c86-4bd3-bbeb-d05b67f104eb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[2] T. R. Bandaragoda, K. M. Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells, \"Isolation-based anomaly detection using nearest-neighbor ensembles,\" *Comput.  Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no. ", "original_text": "968\u2013998, 2018.\n"}, "hash": "29045502e11de04739670e1a298c4ae3cceef2a04eef9b1157559e393396064f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7861ef15-91d1-4f3d-a4f6-1b21cdc227d2", "node_type": "1", "metadata": {"window": "*, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. ", "original_text": "Surveys*, vol. "}, "hash": "d2d09bdfecad4409a6553c32eb3fb543ea26a8f331ceb59acf5b5c98dab454c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. ", "mimetype": "text/plain", "start_char_idx": 62435, "end_char_idx": 62523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7861ef15-91d1-4f3d-a4f6-1b21cdc227d2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. ", "original_text": "Surveys*, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b5c7598-b43f-43af-a003-546df60e5794", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Intell. *, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n", "original_text": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput. "}, "hash": "7fde9dde94e43f83800311d7752fbd453947a6157d176d3a46100c2ecfc0c630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19586f4f-caff-4ee3-a4ad-90f29a14d5d4", "node_type": "1", "metadata": {"window": "34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int. ", "original_text": "41, no. "}, "hash": "0c94e03eca4ce7e27adfd29b928ca9487d169b41a0880b855d249a0c3bb1232a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Surveys*, vol. ", "mimetype": "text/plain", "start_char_idx": 62523, "end_char_idx": 62538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19586f4f-caff-4ee3-a4ad-90f29a14d5d4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int. ", "original_text": "41, no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7861ef15-91d1-4f3d-a4f6-1b21cdc227d2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. ", "original_text": "Surveys*, vol. "}, "hash": "e8dd6629fd3dc4e20c47b3c6e5e1d1602af73f6a46f99a6e34e1dcd11c2ec732", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88ff8b72-6f5b-425f-961a-995d93a46727", "node_type": "1", "metadata": {"window": "4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf. ", "original_text": "3, 2009, Art. "}, "hash": "64aaa6c28fccbdbe011a5d7568440bb0f7c6df52c9a79665fded58b79f48466e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "41, no. ", "mimetype": "text/plain", "start_char_idx": 62538, "end_char_idx": 62546, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88ff8b72-6f5b-425f-961a-995d93a46727", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf. ", "original_text": "3, 2009, Art. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19586f4f-caff-4ee3-a4ad-90f29a14d5d4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "34, no.  4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int. ", "original_text": "41, no. "}, "hash": "d2bc2b04e705ce414c2341a5b54cfe0aa6cd5b261ef6437df5a2f3aadcbd93d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90d7fd79-f12a-4c41-be91-a03c0fdff340", "node_type": "1", "metadata": {"window": "968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol. ", "original_text": "no. "}, "hash": "08939b7a2b41e8dc6fc89a390ebbfd906f3b9f3677ad0da918996e562d7f4e2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3, 2009, Art. ", "mimetype": "text/plain", "start_char_idx": 62546, "end_char_idx": 62560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90d7fd79-f12a-4c41-be91-a03c0fdff340", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol. ", "original_text": "no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88ff8b72-6f5b-425f-961a-995d93a46727", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "4, pp.  968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf. ", "original_text": "3, 2009, Art. "}, "hash": "3f615098a4031ea245e0669073e23d2e175d31b1f3c66704f308a69cfceb1548", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c2f0801-04cc-404f-b19e-484e0c69f33d", "node_type": "1", "metadata": {"window": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell. ", "original_text": "15.\n"}, "hash": "bf9130bb03be5ebbaa863a9b392dc9268a1482630317e3d8ae19216f301c461c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "no. ", "mimetype": "text/plain", "start_char_idx": 62560, "end_char_idx": 62564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c2f0801-04cc-404f-b19e-484e0c69f33d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell. ", "original_text": "15.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90d7fd79-f12a-4c41-be91-a03c0fdff340", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "968\u2013998, 2018.\n [3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol. ", "original_text": "no. "}, "hash": "fbe0185b7b3160acef36e145802ab173143558bd732f62bf9174954e3f78da7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f11209d-d68c-4b50-88f2-26c52c99f3e5", "node_type": "1", "metadata": {"window": "Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst.", "original_text": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. "}, "hash": "ba332f34274531376f29c67cd95e461f9cc5a0f798d7f505ed8902d6e82b0e5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15.\n", "mimetype": "text/plain", "start_char_idx": 62564, "end_char_idx": 62568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0f11209d-d68c-4b50-88f2-26c52c99f3e5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst.", "original_text": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c2f0801-04cc-404f-b19e-484e0c69f33d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[3] V. Chandola, A. Banerjee, and V. Kumar, \"Anomaly detection: A survey,\" *ACM Comput.  Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell. ", "original_text": "15.\n"}, "hash": "0e65199ddb5608ccf4191cbfe81c7bf5ab5493ad4c64e811cee88948ebf7b092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe779bb2-4b64-435d-a472-8ec3a3f32a9b", "node_type": "1", "metadata": {"window": "41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar. ", "original_text": "IEEE Int. "}, "hash": "6fcef188412ab4a0946d6a7d1794e86214dda05342a26042bf4d557579775880", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. ", "mimetype": "text/plain", "start_char_idx": 62568, "end_char_idx": 62703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe779bb2-4b64-435d-a472-8ec3a3f32a9b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar. ", "original_text": "IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f11209d-d68c-4b50-88f2-26c52c99f3e5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Surveys*, vol.  41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst.", "original_text": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc. "}, "hash": "c0462b366ba4162436b3a965a98c5c4de1eaa5fd3e6359d0b1acc3dd02a0489c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ef1f3ff-2957-48bd-a562-f4141d3baa6b", "node_type": "1", "metadata": {"window": "3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp. ", "original_text": "Conf. "}, "hash": "f479c940364488ea7dab6db7996dd51d280a671a5f70ac333909d94db871aa18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 62703, "end_char_idx": 62713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ef1f3ff-2957-48bd-a562-f4141d3baa6b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe779bb2-4b64-435d-a472-8ec3a3f32a9b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "41, no.  3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar. ", "original_text": "IEEE Int. "}, "hash": "0392cb113d1b40850d7171165cc3e0824fb454c90561cc029b3b5b1f468e8fef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fd6e731-de80-4394-8845-ad9d87d6c4c5", "node_type": "1", "metadata": {"window": "no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n", "original_text": "Cyber Technol. "}, "hash": "beba53ba0e72c10212bd5b61d8b153266d3078145a1b2e5d80921021444e1905", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 62713, "end_char_idx": 62719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fd6e731-de80-4394-8845-ad9d87d6c4c5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n", "original_text": "Cyber Technol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ef1f3ff-2957-48bd-a562-f4141d3baa6b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3, 2009, Art.  no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp. ", "original_text": "Conf. "}, "hash": "467bfb26795ca08fa51b6af301cef1d2794453139193ebfed0d7bc6a5cb9b55d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65fb9593-712d-46a4-a4a7-eb7db958491a", "node_type": "1", "metadata": {"window": "15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. ", "original_text": "Automation Control Intell. "}, "hash": "3b18f1f264201eb3d568d304da8132f38e64201f3f951b847217804ccc94606a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cyber Technol. ", "mimetype": "text/plain", "start_char_idx": 62719, "end_char_idx": 62734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65fb9593-712d-46a4-a4a7-eb7db958491a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. ", "original_text": "Automation Control Intell. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fd6e731-de80-4394-8845-ad9d87d6c4c5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "no.  15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n", "original_text": "Cyber Technol. "}, "hash": "c0d6ffc96c0f452e5f43646e9544fb65dfef5589140117cc2e313d9cf384f94e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c4b4009-d56e-4cdc-afd5-c4d1b4f9e6b9", "node_type": "1", "metadata": {"window": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int. ", "original_text": "Syst."}, "hash": "aaf30d52b1e25002005b8435b141a1d18d0c1a0194696c81f2cde1253dfba3a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Automation Control Intell. ", "mimetype": "text/plain", "start_char_idx": 62734, "end_char_idx": 62761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c4b4009-d56e-4cdc-afd5-c4d1b4f9e6b9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int. ", "original_text": "Syst."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65fb9593-712d-46a4-a4a7-eb7db958491a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "15.\n [4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. ", "original_text": "Automation Control Intell. "}, "hash": "9f23d6ed725c27d983ee718cf2f5539cbc1ae8144f66385e200bd623b8bb2b6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be3f7d41-39fd-4aef-b5ae-ad1282a4fbed", "node_type": "1", "metadata": {"window": "IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf. ", "original_text": "*, Mar. "}, "hash": "45fbea5cc717f8de2ee152e0abb683a1ed1c29570a7f045824afaa85440afe3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Syst.", "mimetype": "text/plain", "start_char_idx": 62761, "end_char_idx": 62766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be3f7d41-39fd-4aef-b5ae-ad1282a4fbed", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf. ", "original_text": "*, Mar. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c4b4009-d56e-4cdc-afd5-c4d1b4f9e6b9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[4] G. Chen, Y. L. Cai, and J. Shi, \"Ordinal isolation: An efficient and effective intelligent outlier detection algorithm,\u201d in *Proc.  IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int. ", "original_text": "Syst."}, "hash": "4716e736d5b895d2b6242c733d77dac9b43d6f135de2f412a5d658c1a7d7c93d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "638d7019-30ce-4243-be49-a4d94a7ef928", "node_type": "1", "metadata": {"window": "Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec. ", "original_text": "2011, pp. "}, "hash": "b55c3963fb640f0d4d9517ddc7dcad3be5834339168e4427d743b9057d9d124b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, Mar. ", "mimetype": "text/plain", "start_char_idx": 62766, "end_char_idx": 62774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "638d7019-30ce-4243-be49-a4d94a7ef928", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec. ", "original_text": "2011, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be3f7d41-39fd-4aef-b5ae-ad1282a4fbed", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf. ", "original_text": "*, Mar. "}, "hash": "2ca80778405d917abdbc2721813750e218a21f83a6133271939992e8f0a56714", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0b0bd09-96af-4ea5-b69a-eb771734282b", "node_type": "1", "metadata": {"window": "Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp. ", "original_text": "21-26.\n"}, "hash": "9402864c6843d29e76bb75bdfaac5ca689d5a6bb86a68752b4c320001d2b255f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2011, pp. ", "mimetype": "text/plain", "start_char_idx": 62774, "end_char_idx": 62784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0b0bd09-96af-4ea5-b69a-eb771734282b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp. ", "original_text": "21-26.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "638d7019-30ce-4243-be49-a4d94a7ef928", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec. ", "original_text": "2011, pp. "}, "hash": "18486f232a837205a656e85141f71de26d54f7a6c44c8114c391341b541b6050", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "121079bf-cbd8-4f6b-9b66-14885b21e0f4", "node_type": "1", "metadata": {"window": "Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n", "original_text": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. "}, "hash": "6bcbfcf7ede5b50cf87d29b74ef9bfa56e56e081a13be8f82e0951cba8d6c17f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21-26.\n", "mimetype": "text/plain", "start_char_idx": 62784, "end_char_idx": 62791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "121079bf-cbd8-4f6b-9b66-14885b21e0f4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n", "original_text": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0b0bd09-96af-4ea5-b69a-eb771734282b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Cyber Technol.  Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp. ", "original_text": "21-26.\n"}, "hash": "8306857427c1c452860ebc9b7d7540a06e16a6a6ff586bd71f7bfd11af7f8bbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5d9618b-78ff-4b42-94aa-583b7be494cb", "node_type": "1", "metadata": {"window": "Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. ", "original_text": "IEEE 16th Int. "}, "hash": "24e76a1f570a4666318896942b4db5be729e3dce333cd73f6f3b5fae326226d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 62791, "end_char_idx": 62927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5d9618b-78ff-4b42-94aa-583b7be494cb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. ", "original_text": "IEEE 16th Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "121079bf-cbd8-4f6b-9b66-14885b21e0f4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Automation Control Intell.  Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n", "original_text": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc. "}, "hash": "211b7a41b0155b4795dc9fdc4f909ecf13164a60a9083a8f49116898c18867aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5000a8d9-a1ff-4f0d-86f5-b0a9416897c8", "node_type": "1", "metadata": {"window": "*, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n", "original_text": "Conf. "}, "hash": "da80878bc32e29a2a86e532693d9fc084a6dc236e749047532f38ceb03ca9795", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE 16th Int. ", "mimetype": "text/plain", "start_char_idx": 62927, "end_char_idx": 62942, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5000a8d9-a1ff-4f0d-86f5-b0a9416897c8", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5d9618b-78ff-4b42-94aa-583b7be494cb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Syst. *, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. ", "original_text": "IEEE 16th Int. "}, "hash": "a96651f47956120daca0852654337003341d6ce2bcb28982768d1cb4fa1bf362", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f279cf25-5cc6-4385-97c3-8a3ff1e1c0f3", "node_type": "1", "metadata": {"window": "2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. ", "original_text": "Data Mining*, Dec. "}, "hash": "5f2b5606e2724786b3ea82ef860e6ca66c61c01de11b095329e309e744bb3b10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 62942, "end_char_idx": 62948, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f279cf25-5cc6-4385-97c3-8a3ff1e1c0f3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. ", "original_text": "Data Mining*, Dec. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5000a8d9-a1ff-4f0d-86f5-b0a9416897c8", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, Mar.  2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n", "original_text": "Conf. "}, "hash": "b77c2faa4db5e5a9dec24602390d9934d80a887718a87848703b83726e275709", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "110e956c-b84c-4a9e-835d-c2068c65eaee", "node_type": "1", "metadata": {"window": "21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci. ", "original_text": "2016, pp. "}, "hash": "09b09d63215559dd042448815dc468600f7fe1cdbaa7b41fc96678a4985912f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining*, Dec. ", "mimetype": "text/plain", "start_char_idx": 62948, "end_char_idx": 62967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "110e956c-b84c-4a9e-835d-c2068c65eaee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci. ", "original_text": "2016, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f279cf25-5cc6-4385-97c3-8a3ff1e1c0f3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2011, pp.  21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. ", "original_text": "Data Mining*, Dec. "}, "hash": "250b9dfb605787f12b521d9e34e3228ed5bb33f4bc427d519f58ce6c850ed705", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66d6bbc7-76bb-4a5c-bd9e-6f8b5d8a4192", "node_type": "1", "metadata": {"window": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput.", "original_text": "853-858.\n"}, "hash": "b34425c33ce1f6069ce28bb1b5736d6533e782be74ac77ba7b80da52e6201d9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2016, pp. ", "mimetype": "text/plain", "start_char_idx": 62967, "end_char_idx": 62977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "66d6bbc7-76bb-4a5c-bd9e-6f8b5d8a4192", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput.", "original_text": "853-858.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "110e956c-b84c-4a9e-835d-c2068c65eaee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "21-26.\n [5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci. ", "original_text": "2016, pp. "}, "hash": "6305f9c162a69826cc5672a897b52379bc2cbd1a5bfc5d648b1437bd92bf3afd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7716a60-a1cf-45a4-954d-6d6a09b46e5b", "node_type": "1", "metadata": {"window": "IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art. ", "original_text": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. "}, "hash": "b050a6299518f8c3acf2584899cb6ad0c01d296d06d92eacd8c68180d431c80e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "853-858.\n", "mimetype": "text/plain", "start_char_idx": 62977, "end_char_idx": 62986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7716a60-a1cf-45a4-954d-6d6a09b46e5b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art. ", "original_text": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66d6bbc7-76bb-4a5c-bd9e-6f8b5d8a4192", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[5] S. Das, W. K. Wong, T. Dietterich, A. Fern, and A. Emmott, \"Incorporating expert feedback into active anomaly discovery,\" in *Proc.  IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput.", "original_text": "853-858.\n"}, "hash": "28a7aec556231d1d6ffcb17b5de806f98a63103b89bb8e25a81a458654febb7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18462b6c-c4ac-4b78-b0d7-5dff7c9fd761", "node_type": "1", "metadata": {"window": "Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no. ", "original_text": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n"}, "hash": "27b53ad24795cf42b6b032403fbdf894840f97637e429560f797530750d3f581", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. ", "mimetype": "text/plain", "start_char_idx": 62986, "end_char_idx": 63046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18462b6c-c4ac-4b78-b0d7-5dff7c9fd761", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no. ", "original_text": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7716a60-a1cf-45a4-954d-6d6a09b46e5b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE 16th Int.  Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art. ", "original_text": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K. "}, "hash": "4fb475fa5a7c52c6b25bd8bd87783334d623fd2a012bb55cefaa186e5e3e7bab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ef0a8c5-ebcc-46cd-ba9b-9aeb54e71208", "node_type": "1", "metadata": {"window": "Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n", "original_text": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. "}, "hash": "e96a04c238d114ad62f6dc53c439174fc0ff8683230a02c936cc62090565bda5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n", "mimetype": "text/plain", "start_char_idx": 63046, "end_char_idx": 63110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ef0a8c5-ebcc-46cd-ba9b-9aeb54e71208", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n", "original_text": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18462b6c-c4ac-4b78-b0d7-5dff7c9fd761", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no. ", "original_text": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n"}, "hash": "4202f2e28e219afe6f255a4da59be5566f549e9939f60e4b2985b743511fc732", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a51a5bb3-dc54-4b1d-8ffe-5c08bd7796a5", "node_type": "1", "metadata": {"window": "2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal.", "original_text": "9th Workshop Sci. "}, "hash": "9630816f796ff5c977b21eb0d50ca8e4ee8d88d90d2db8a000f417ed09a14ff7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 63110, "end_char_idx": 63244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a51a5bb3-dc54-4b1d-8ffe-5c08bd7796a5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal.", "original_text": "9th Workshop Sci. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ef0a8c5-ebcc-46cd-ba9b-9aeb54e71208", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining*, Dec.  2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n", "original_text": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc. "}, "hash": "83c34a20669afa3fa3a0beef223d0fa8028eca29c0febfa11bfdbfb3ce8adf98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "537c2d6e-2e58-4852-bdaa-6c5236215188", "node_type": "1", "metadata": {"window": "853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol. ", "original_text": "Cloud Comput."}, "hash": "679a6f1af83d16127c39d6d573c4c4b4dbbde465eff7994babe2450977c17ea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9th Workshop Sci. ", "mimetype": "text/plain", "start_char_idx": 63244, "end_char_idx": 63262, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "537c2d6e-2e58-4852-bdaa-6c5236215188", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol. ", "original_text": "Cloud Comput."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a51a5bb3-dc54-4b1d-8ffe-5c08bd7796a5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2016, pp.  853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal.", "original_text": "9th Workshop Sci. "}, "hash": "6d0e7ce540579e42202020dd8fbe4081b39548e18f5badf4585c4cd45a94e6f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6e0bebb-4f9d-44b7-be1d-cdc6c95800cd", "node_type": "1", "metadata": {"window": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no. ", "original_text": "*, 2018, Art. "}, "hash": "92f32ef3689e6cded6adc7aca994340d5cfa3f7d637e25bbd81a937b024405c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cloud Comput.", "mimetype": "text/plain", "start_char_idx": 63262, "end_char_idx": 63275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6e0bebb-4f9d-44b7-be1d-cdc6c95800cd", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no. ", "original_text": "*, 2018, Art. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "537c2d6e-2e58-4852-bdaa-6c5236215188", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "853-858.\n [6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol. ", "original_text": "Cloud Comput."}, "hash": "0e45b037fc58a3823203141d593fd2599f287f5637c8ca68f92cae3f9ccb47f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77904917-8a2d-4d3c-912e-89bf3d285c83", "node_type": "1", "metadata": {"window": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp. ", "original_text": "no. "}, "hash": "35dc90aefcacf8676abe3a9da8b4be87d901c599532a242e054190cd535b9d6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, 2018, Art. ", "mimetype": "text/plain", "start_char_idx": 63275, "end_char_idx": 63289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77904917-8a2d-4d3c-912e-89bf3d285c83", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp. ", "original_text": "no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6e0bebb-4f9d-44b7-be1d-cdc6c95800cd", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[6] A. Emmott, S. Das, T. G. Dietterich, A. Fern, and W.-K.  Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no. ", "original_text": "*, 2018, Art. "}, "hash": "a7b2744151ee0b99cd543ba4c21f1bfce1f744c86a38ffdd71825d88766d911f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2511d5c0-6efd-47ce-8897-cda9e141a577", "node_type": "1", "metadata": {"window": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n", "original_text": "3.\n"}, "hash": "2e8294e1e12070fc378f489dc968a571ed3c65dc31e0fb51a8211a07ae908053", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "no. ", "mimetype": "text/plain", "start_char_idx": 63289, "end_char_idx": 63293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2511d5c0-6efd-47ce-8897-cda9e141a577", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n", "original_text": "3.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77904917-8a2d-4d3c-912e-89bf3d285c83", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Wong, \"A meta-analysis of the anomaly detection problem,\" 2015.\n [7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp. ", "original_text": "no. "}, "hash": "971b9a2027aa6015f142cb674876a4240b4888c772a4aee7424d2cafa6477e56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c067b02-f718-49a9-893d-0d7b928396c7", "node_type": "1", "metadata": {"window": "9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal."}, "hash": "70a128ad82c0914b14d59cf97865391bcca264c5af81102a616a0564a8eeb981", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.\n", "mimetype": "text/plain", "start_char_idx": 63293, "end_char_idx": 63296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c067b02-f718-49a9-893d-0d7b928396c7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2511d5c0-6efd-47ce-8897-cda9e141a577", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[7] S. Hariri and M. C. Kind, \"Batch and online anomaly detection for scientific applications in a Kubernetes environment,\" in *Proc.  9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n", "original_text": "3.\n"}, "hash": "29ba8658c700b6dc6da69ad592d5e1bf5115007f8593a8347ef46c3077c46663", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "910f62ae-67f6-4a10-bc62-298660b7d76d", "node_type": "1", "metadata": {"window": "Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc. ", "original_text": "*, vol. "}, "hash": "2839d23331a8612be4f4270d937453c930827092e5370b71a2c516d9946123f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal.", "mimetype": "text/plain", "start_char_idx": 63296, "end_char_idx": 63427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "910f62ae-67f6-4a10-bc62-298660b7d76d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc. ", "original_text": "*, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c067b02-f718-49a9-893d-0d7b928396c7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "9th Workshop Sci.  Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal."}, "hash": "9d216f053417cfd197b398a538767fb369a1d7a78744c4dd55f7eee3f51c00ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "641c779a-b991-4c1c-b53a-e5a7560b0d79", "node_type": "1", "metadata": {"window": "*, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int. ", "original_text": "101, no. "}, "hash": "546af0888c361b85541149f80fe7252716ea6a2568642c734d648da1ddb1043f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, vol. ", "mimetype": "text/plain", "start_char_idx": 63427, "end_char_idx": 63435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "641c779a-b991-4c1c-b53a-e5a7560b0d79", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int. ", "original_text": "101, no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "910f62ae-67f6-4a10-bc62-298660b7d76d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Cloud Comput. *, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc. ", "original_text": "*, vol. "}, "hash": "040df2c4281e3c33560199fb6e5e85dc99c32030e08b89c4cee9cef417c99282", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd98f3bd-3714-4cc8-a2c1-6021b16fe39d", "node_type": "1", "metadata": {"window": "no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf. ", "original_text": "10, pp. "}, "hash": "aa2360b273137b4e066527e1f0e634bc211bbb0d44c30b8e4cee954baac3a509", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "101, no. ", "mimetype": "text/plain", "start_char_idx": 63435, "end_char_idx": 63444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd98f3bd-3714-4cc8-a2c1-6021b16fe39d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf. ", "original_text": "10, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "641c779a-b991-4c1c-b53a-e5a7560b0d79", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, 2018, Art.  no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int. ", "original_text": "101, no. "}, "hash": "5c39a0aaea22470f5db30bd0df214b51f312e0ca59b12dc822bc716f9ccf85dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a8f69e-e2e2-43c7-a25f-c8e98809de5b", "node_type": "1", "metadata": {"window": "3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp. ", "original_text": "2297-2304, 2010.\n"}, "hash": "e91c26f67ffad15861401d6e38bfde0f1b17617ca08825b4bf2c62ba6e44743e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10, pp. ", "mimetype": "text/plain", "start_char_idx": 63444, "end_char_idx": 63452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04a8f69e-e2e2-43c7-a25f-c8e98809de5b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp. ", "original_text": "2297-2304, 2010.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd98f3bd-3714-4cc8-a2c1-6021b16fe39d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "no.  3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf. ", "original_text": "10, pp. "}, "hash": "73528036da612f8f54ab8ac3319e26d406cd80dfac80fb8f31b597470bd0a39f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5321e4f8-fdea-49ae-9388-51d4aaf26eca", "node_type": "1", "metadata": {"window": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n", "original_text": "[9] F. T. Liu, K. M. Ting, and Z.-H. "}, "hash": "22aeac168cf77fd9ce63580f30b86bc3846fa0319561e297d70344cf82979a64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2297-2304, 2010.\n", "mimetype": "text/plain", "start_char_idx": 63452, "end_char_idx": 63469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5321e4f8-fdea-49ae-9388-51d4aaf26eca", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n", "original_text": "[9] F. T. Liu, K. M. Ting, and Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a8f69e-e2e2-43c7-a25f-c8e98809de5b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3.\n [8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp. ", "original_text": "2297-2304, 2010.\n"}, "hash": "b84b88406bad406d1806b690262a6bbd3c2abca7b4f80c40ea8648b828d2a489", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb153e1b-6ff3-4f87-86a3-f7e1d6587192", "node_type": "1", "metadata": {"window": "*, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "Zhou, \u201cIsolation forest,\" in *Proc. "}, "hash": "86b28f16bb14d41f581b5a58c5d7f9b165f973f2b08305d9dee54497be4a48cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] F. T. Liu, K. M. Ting, and Z.-H. ", "mimetype": "text/plain", "start_char_idx": 63469, "end_char_idx": 63506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb153e1b-6ff3-4f87-86a3-f7e1d6587192", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "Zhou, \u201cIsolation forest,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5321e4f8-fdea-49ae-9388-51d4aaf26eca", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[8] R. Harman and V. Lacko, \"On decompositional algorithms for uniform sampling from n-spheres and n-balls,\" *J. Multivariate Anal. *, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n", "original_text": "[9] F. T. Liu, K. M. Ting, and Z.-H. "}, "hash": "8c1fa511212664346d08d2c51ecffcbbb4dd2b02c48d827e120acdcf10497376", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2c17719-9a8e-4179-a758-574456d0ed26", "node_type": "1", "metadata": {"window": "101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. ", "original_text": "8th IEEE Int. "}, "hash": "b53fa453f761c7e7b44abe622f09dae652d8b397e6560facef172c67e468fbc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, \u201cIsolation forest,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 63506, "end_char_idx": 63542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2c17719-9a8e-4179-a758-574456d0ed26", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. ", "original_text": "8th IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb153e1b-6ff3-4f87-86a3-f7e1d6587192", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "Zhou, \u201cIsolation forest,\" in *Proc. "}, "hash": "eebda571087f478ed66fbf81733a05ae807ec142f11599beaa4ad0fe20e6b216", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d2000fc-e107-4e10-b5a8-841b4ab96543", "node_type": "1", "metadata": {"window": "10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur. ", "original_text": "Conf. "}, "hash": "94697e38ba65019fee76aede61fede58aaa41fe3b6b6b6e80438e9d054fda413", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8th IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 63542, "end_char_idx": 63556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d2000fc-e107-4e10-b5a8-841b4ab96543", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c17719-9a8e-4179-a758-574456d0ed26", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "101, no.  10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. ", "original_text": "8th IEEE Int. "}, "hash": "f434a4efe5b9aaffa92929cfcd011af9393924ded238e513093a916d76d44512", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad687cdf-f396-4b47-8d25-8ee70f2ad543", "node_type": "1", "metadata": {"window": "2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf. ", "original_text": "Data Mining*, 2008, pp. "}, "hash": "af27195d8ec0ebdc1d16bb8a00f6b42b579a5dcfea5f4399191eb33968b80a01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 63556, "end_char_idx": 63562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad687cdf-f396-4b47-8d25-8ee70f2ad543", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf. ", "original_text": "Data Mining*, 2008, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d2000fc-e107-4e10-b5a8-841b4ab96543", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "10, pp.  2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur. ", "original_text": "Conf. "}, "hash": "eb52f56b9299833bcc52a4677788fdeab44a80dcd46ec03e9223906f9094d946", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1acc6416-e467-4820-a563-c2446e0ebe82", "node_type": "1", "metadata": {"window": "[9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach. ", "original_text": "413-422.\n"}, "hash": "9544d0fe793762ba8a8bf9d3e6de6775c791177a3ca85063dcc78cca915a7467", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining*, 2008, pp. ", "mimetype": "text/plain", "start_char_idx": 63562, "end_char_idx": 63586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1acc6416-e467-4820-a563-c2446e0ebe82", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach. ", "original_text": "413-422.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad687cdf-f396-4b47-8d25-8ee70f2ad543", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2297-2304, 2010.\n [9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf. ", "original_text": "Data Mining*, 2008, pp. "}, "hash": "5e97d096321a07a5d116aeab82e25f846769dfb22c064d723ad05f8dbaed7859", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7135df7-02f3-4438-943c-3e42953eb531", "node_type": "1", "metadata": {"window": "Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn. ", "original_text": "[10] F. T. Liu, K. M. Ting, and Z.-H. "}, "hash": "2674bde5319387ff6be17932bec1606a4da56b4f05ec1f379c59bb1dfc3ac2c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "413-422.\n", "mimetype": "text/plain", "start_char_idx": 63586, "end_char_idx": 63595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7135df7-02f3-4438-943c-3e42953eb531", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn. ", "original_text": "[10] F. T. Liu, K. M. Ting, and Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1acc6416-e467-4820-a563-c2446e0ebe82", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[9] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach. ", "original_text": "413-422.\n"}, "hash": "718dfb3ead02f6e31986fe7f5b1bac1523bb28a7abe47f0c998fceb41fcf66e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55ff28f7-abc5-4c78-ada0-40d10d9159f7", "node_type": "1", "metadata": {"window": "8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl. ", "original_text": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. "}, "hash": "a235d26daf97672478b0fd6a98a52fac46671625f6d3ea0abf7cf17a9a3d643c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] F. T. Liu, K. M. Ting, and Z.-H. ", "mimetype": "text/plain", "start_char_idx": 63595, "end_char_idx": 63633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "55ff28f7-abc5-4c78-ada0-40d10d9159f7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl. ", "original_text": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7135df7-02f3-4438-943c-3e42953eb531", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Zhou, \u201cIsolation forest,\" in *Proc.  8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn. ", "original_text": "[10] F. T. Liu, K. M. Ting, and Z.-H. "}, "hash": "5a2d3397df1caa1b4b527e8d68e432a78c655e3c98bf329512685a2fd5875993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9ee6968-71b8-4b4f-b225-5af94c64de32", "node_type": "1", "metadata": {"window": "Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp. ", "original_text": "Joint Eur. "}, "hash": "19774399a342fe321c92235bbf71cd53a8764775e3e23c7715ea6dbed78470f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 63633, "end_char_idx": 63701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9ee6968-71b8-4b4f-b225-5af94c64de32", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp. ", "original_text": "Joint Eur. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55ff28f7-abc5-4c78-ada0-40d10d9159f7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "8th IEEE Int.  Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl. ", "original_text": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc. "}, "hash": "9897ced60ad714c7f1b8f66a239fa603b1bf7826fbb06c62d4ad99289d324e6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ec524b1-9362-41a8-a8a6-f8fafb98230e", "node_type": "1", "metadata": {"window": "Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n", "original_text": "Conf. "}, "hash": "c3d83d45140f66a4ba235a5fbdcce1e8ba13a5f3e247c5eeabd5cc103fc2fdf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Joint Eur. ", "mimetype": "text/plain", "start_char_idx": 63701, "end_char_idx": 63712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ec524b1-9362-41a8-a8a6-f8fafb98230e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9ee6968-71b8-4b4f-b225-5af94c64de32", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp. ", "original_text": "Joint Eur. "}, "hash": "8e46293e9bb71bd953b0852d65ad8780f5587deb6c3d8902175dfbce250d51cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2246b5b1-342c-4961-9fd5-355b9300ac9a", "node_type": "1", "metadata": {"window": "413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "Mach. "}, "hash": "47d835869458fa90b36960fd8c7dabc5b52701f81d56285eed5d47dbf91f1e5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 63712, "end_char_idx": 63718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2246b5b1-342c-4961-9fd5-355b9300ac9a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "Mach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ec524b1-9362-41a8-a8a6-f8fafb98230e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining*, 2008, pp.  413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n", "original_text": "Conf. "}, "hash": "d752508dfeafe3508e7e8e98aaf582c7b176aca6063b3753f3c0bf8d77d9362f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "857de876-ee8d-4941-b139-c277efee164e", "node_type": "1", "metadata": {"window": "[10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. ", "original_text": "Learn. "}, "hash": "e32daf2a62f907aef49fff2e189837158dd7ceeb5a3151bd6866d1096b9c1f40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mach. ", "mimetype": "text/plain", "start_char_idx": 63718, "end_char_idx": 63724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "857de876-ee8d-4941-b139-c277efee164e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. ", "original_text": "Learn. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2246b5b1-342c-4961-9fd5-355b9300ac9a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "413-422.\n [10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H. ", "original_text": "Mach. "}, "hash": "f9ab908bfa6f989d9e7b5a2840b81c9a36644254a27cf70856932789e9e859f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ad75704-d3c0-4a22-bd4f-a7ba1e17e9ba", "node_type": "1", "metadata": {"window": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl. ", "original_text": "Knowl. "}, "hash": "2453243bbafac36a18f201ad7494823706542baf2cda7f7e364e893fff11edbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn. ", "mimetype": "text/plain", "start_char_idx": 63724, "end_char_idx": 63731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ad75704-d3c0-4a22-bd4f-a7ba1e17e9ba", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl. ", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "857de876-ee8d-4941-b139-c277efee164e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[10] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. ", "original_text": "Learn. "}, "hash": "c1dd1fa8ed8c336eb340ad37ff620d58b99eb8ebe44c4b66a678ff06bbd10a8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "096b9ab0-db9c-4792-8aca-13b5dfa6531c", "node_type": "1", "metadata": {"window": "Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol. ", "original_text": "Discovery Databases*, 2010, pp. "}, "hash": "df7b204d90dd6fc4d58e7775f3463b3755d4c0e2de5ac7ab6f3eb054894d7d52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 63731, "end_char_idx": 63738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "096b9ab0-db9c-4792-8aca-13b5dfa6531c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol. ", "original_text": "Discovery Databases*, 2010, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ad75704-d3c0-4a22-bd4f-a7ba1e17e9ba", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Zhou, \"On detecting clustered anomalies using SCiForest,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl. ", "original_text": "Knowl. "}, "hash": "49994734332dbb47ca1ecfecb294dac86162cb5d565c3d2e553ea89a671ed8f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a028aa24-fe23-4aba-b69b-c77cae26987f", "node_type": "1", "metadata": {"window": "Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no. ", "original_text": "274\u2013290.\n"}, "hash": "65e634ea54f35ccc1d963810b0162410204bce812a1136b9035681005f685675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discovery Databases*, 2010, pp. ", "mimetype": "text/plain", "start_char_idx": 63738, "end_char_idx": 63770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a028aa24-fe23-4aba-b69b-c77cae26987f", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no. ", "original_text": "274\u2013290.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "096b9ab0-db9c-4792-8aca-13b5dfa6531c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol. ", "original_text": "Discovery Databases*, 2010, pp. "}, "hash": "9bab63c835dbc01d9e399f453f5951c667dc7c4a5c5caf510a80b912d0cfba0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41334f52-d41a-4041-867e-3622915f6c33", "node_type": "1", "metadata": {"window": "Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp. ", "original_text": "[11] F. T. Liu, K. M. Ting, and Z.-H. "}, "hash": "b556cdaeac3bd56399baea0ebd441f29c80712753759bde118fc0c82c9d3923b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "274\u2013290.\n", "mimetype": "text/plain", "start_char_idx": 63770, "end_char_idx": 63779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41334f52-d41a-4041-867e-3622915f6c33", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp. ", "original_text": "[11] F. T. Liu, K. M. Ting, and Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a028aa24-fe23-4aba-b69b-c77cae26987f", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no. ", "original_text": "274\u2013290.\n"}, "hash": "1cbc8b38663b2da210d72b578d91e8f56e1ee18bcaa92412890ff8b6af903c0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4966a8e-4a8b-46d6-a1f0-61d29bfb61bb", "node_type": "1", "metadata": {"window": "Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar. ", "original_text": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. "}, "hash": "bad51d40cba55b091a5ed8d15fe4d7a63a8469ffc9368bfe4cd01966d6fb2904", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] F. T. Liu, K. M. Ting, and Z.-H. ", "mimetype": "text/plain", "start_char_idx": 63779, "end_char_idx": 63817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4966a8e-4a8b-46d6-a1f0-61d29bfb61bb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar. ", "original_text": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41334f52-d41a-4041-867e-3622915f6c33", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mach.  Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp. ", "original_text": "[11] F. T. Liu, K. M. Ting, and Z.-H. "}, "hash": "6928e04943084545020c8cfd35c0c6ef83741bab50b972e44a7280adbd9111e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3552a8d5-ace2-4585-8c81-b78a44837566", "node_type": "1", "metadata": {"window": "Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n", "original_text": "Knowl. "}, "hash": "36d0210af06b4a74c0a5cdb221e0415826fe94ee173b18924c7a0097358aef85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. ", "mimetype": "text/plain", "start_char_idx": 63817, "end_char_idx": 63872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3552a8d5-ace2-4585-8c81-b78a44837566", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4966a8e-4a8b-46d6-a1f0-61d29bfb61bb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Learn.  Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar. ", "original_text": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans. "}, "hash": "f9cdcb25bac3078db4167014ca826ffa4c526c34d08cc983d31c45d6caacdb45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c75301a-1d68-4524-a54d-14712e7df7cf", "node_type": "1", "metadata": {"window": "Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. ", "original_text": "Discovery Data*, vol. "}, "hash": "ae43055d53fc30b3a5ad86fec1c67c13f2afd149f4b3aeca1b10c8088970f644", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 63872, "end_char_idx": 63879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c75301a-1d68-4524-a54d-14712e7df7cf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. ", "original_text": "Discovery Data*, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3552a8d5-ace2-4585-8c81-b78a44837566", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Knowl.  Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n", "original_text": "Knowl. "}, "hash": "831637d5adae8806a25dd6e4f84ba6213befb05d5db6eb97aa9d81a107926b9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ee59411-d695-48e5-9d57-a36229eadf0b", "node_type": "1", "metadata": {"window": "274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur. ", "original_text": "6, no. "}, "hash": "8ee43f77724d668db145fbecda40a3530944ddbc52e347cb7e2c2f49626376b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discovery Data*, vol. ", "mimetype": "text/plain", "start_char_idx": 63879, "end_char_idx": 63901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ee59411-d695-48e5-9d57-a36229eadf0b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur. ", "original_text": "6, no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c75301a-1d68-4524-a54d-14712e7df7cf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discovery Databases*, 2010, pp.  274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. ", "original_text": "Discovery Data*, vol. "}, "hash": "3bff20f1d7c1844cca5fbb12909f5a36d056f205745ef9960f36172ec8bd496d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6c91b5f-8600-4d34-b7e6-0a6ec49374c9", "node_type": "1", "metadata": {"window": "[11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf. ", "original_text": "1, pp. "}, "hash": "7e09def606abd84d294cec275e13870046cb7fd59498c433588e55d577118466", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6, no. ", "mimetype": "text/plain", "start_char_idx": 63901, "end_char_idx": 63908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6c91b5f-8600-4d34-b7e6-0a6ec49374c9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf. ", "original_text": "1, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ee59411-d695-48e5-9d57-a36229eadf0b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "274\u2013290.\n [11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur. ", "original_text": "6, no. "}, "hash": "c46a0bcb9c374b7782c8fab30c8b2ade096cb920507bcb4e88c4620a9acb115b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5861efe0-015d-4adb-91dd-606b6d09371b", "node_type": "1", "metadata": {"window": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach. ", "original_text": "3:1-3:39, Mar. "}, "hash": "1438ad4542e874e334c78e0f68b553b658cc5988dc5db281c725f553546c2e07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, pp. ", "mimetype": "text/plain", "start_char_idx": 63908, "end_char_idx": 63915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5861efe0-015d-4adb-91dd-606b6d09371b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach. ", "original_text": "3:1-3:39, Mar. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6c91b5f-8600-4d34-b7e6-0a6ec49374c9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[11] F. T. Liu, K. M. Ting, and Z.-H.  Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf. ", "original_text": "1, pp. "}, "hash": "d8ca132a47cd7f1ae94c94b3490a4ae9b14ac45c3b261b35916ba0eab3286435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b09f24ac-cd3f-4941-9cd2-4d7a2398c96c", "node_type": "1", "metadata": {"window": "Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn. ", "original_text": "2012.\n"}, "hash": "667227e5fa2874782ea425bb19d03901fd6a25316183dd253b3e7ca9e8e450de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3:1-3:39, Mar. ", "mimetype": "text/plain", "start_char_idx": 63915, "end_char_idx": 63930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b09f24ac-cd3f-4941-9cd2-4d7a2398c96c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn. ", "original_text": "2012.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5861efe0-015d-4adb-91dd-606b6d09371b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Zhou, \u201cIsolation-based anomaly detection,\" *ACM Trans.  Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach. ", "original_text": "3:1-3:39, Mar. "}, "hash": "f241011779aaf4ef7900306dc368882306dd9af2d026f933979db0d4fc38289c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f5abd6-0e73-4ad9-a625-46cd0fd28f9d", "node_type": "1", "metadata": {"window": "Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl. ", "original_text": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. "}, "hash": "4038ca18bffb7e37910eda4eb1bb4ecbb86588e19649f15142c852d5d5ca3357", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2012.\n", "mimetype": "text/plain", "start_char_idx": 63930, "end_char_idx": 63936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84f5abd6-0e73-4ad9-a625-46cd0fd28f9d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl. ", "original_text": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b09f24ac-cd3f-4941-9cd2-4d7a2398c96c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Knowl.  Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn. ", "original_text": "2012.\n"}, "hash": "385d7ef2aa4824b55ca14b5206c95039e629d1a4538cee7bec59fbc01dc84fd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6be127d5-27c7-4267-8dd7-8f0048bd33a4", "node_type": "1", "metadata": {"window": "6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp. ", "original_text": "Joint Eur. "}, "hash": "e68b7dfe5bf876b819096e42d0481e47010a0ddf3fea96dfeb92155684edf41f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 63936, "end_char_idx": 64055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6be127d5-27c7-4267-8dd7-8f0048bd33a4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp. ", "original_text": "Joint Eur. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84f5abd6-0e73-4ad9-a625-46cd0fd28f9d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discovery Data*, vol.  6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl. ", "original_text": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc. "}, "hash": "c1a567834e183bc1bfe0339cb5f5375d2c4793937cd5bce7147d586fb5b868cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a071aa5-5aec-4a7b-805a-aab050b39658", "node_type": "1", "metadata": {"window": "1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n", "original_text": "Conf. "}, "hash": "7d607fa7440a700dbef431fb870da70204b35be7ccb87341787373ab1efedf11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Joint Eur. ", "mimetype": "text/plain", "start_char_idx": 64055, "end_char_idx": 64066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2a071aa5-5aec-4a7b-805a-aab050b39658", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6be127d5-27c7-4267-8dd7-8f0048bd33a4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "6, no.  1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp. ", "original_text": "Joint Eur. "}, "hash": "46856e51e758c67f41454f3d232c00695999ae24dd5af8930e94c7efe1f8fdb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2411e702-74f7-404a-bb85-d680817ad118", "node_type": "1", "metadata": {"window": "3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. ", "original_text": "Mach. "}, "hash": "dbecc4c5daee63425e66bd21ac1d7b8c2a0e8ca9c6467d3db48e2006c80c8a97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 64066, "end_char_idx": 64072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2411e702-74f7-404a-bb85-d680817ad118", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. ", "original_text": "Mach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a071aa5-5aec-4a7b-805a-aab050b39658", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1, pp.  3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n", "original_text": "Conf. "}, "hash": "3d2351a56073f28b462c540f8785c20103211d6a13f9b349796559ffa85f7d6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c512c2d0-df94-4d62-b6a8-6a779a45d399", "node_type": "1", "metadata": {"window": "2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int. ", "original_text": "Learn. "}, "hash": "a6830d4c9a2832f02c8586aa39bdfdd295f1625beae23dabc1488eb42cde2b36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mach. ", "mimetype": "text/plain", "start_char_idx": 64072, "end_char_idx": 64078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c512c2d0-df94-4d62-b6a8-6a779a45d399", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int. ", "original_text": "Learn. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2411e702-74f7-404a-bb85-d680817ad118", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "3:1-3:39, Mar.  2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. ", "original_text": "Mach. "}, "hash": "671fe4465367c2840d4b08b835a7d70a625c834db103d09f616adc8364a341f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a60e05f3-11da-4b99-9897-2c090bd5a2be", "node_type": "1", "metadata": {"window": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf. ", "original_text": "Knowl. "}, "hash": "9942f9e8421bc9d0d39dbf83f982a646459d87a089aa893baf2e2f24d58118a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn. ", "mimetype": "text/plain", "start_char_idx": 64078, "end_char_idx": 64085, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a60e05f3-11da-4b99-9897-2c090bd5a2be", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf. ", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c512c2d0-df94-4d62-b6a8-6a779a45d399", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2012.\n [12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int. ", "original_text": "Learn. "}, "hash": "a09c79b7658393ec79bb71c33dd4ce3f9f8699b8bb651cff83390f916ce5f204", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8be761f-5793-4edc-ba1c-7cb5595414b5", "node_type": "1", "metadata": {"window": "Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec. ", "original_text": "Discovery Databases*, 2011, pp. "}, "hash": "d711033106bf2d457f7e300650ada6ed8e23037c2d15b5b825fd581592cfc8d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 64085, "end_char_idx": 64092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8be761f-5793-4edc-ba1c-7cb5595414b5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec. ", "original_text": "Discovery Databases*, 2011, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a60e05f3-11da-4b99-9897-2c090bd5a2be", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[12] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht, \"On oblique random forests,\" in *Proc.  Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf. ", "original_text": "Knowl. "}, "hash": "a4754e3e42fe0ee0e1fc7073bda7cf577b0bbc26d9f30ab44976f89a0c155a79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7218540-34c7-481a-90b3-89c30a1fdc7d", "node_type": "1", "metadata": {"window": "Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp. ", "original_text": "453-469.\n"}, "hash": "8d562c149e05e24204050770b06c8efd60e5ec63b33276604ed910a2717ec5c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discovery Databases*, 2011, pp. ", "mimetype": "text/plain", "start_char_idx": 64092, "end_char_idx": 64124, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7218540-34c7-481a-90b3-89c30a1fdc7d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp. ", "original_text": "453-469.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8be761f-5793-4edc-ba1c-7cb5595414b5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Joint Eur.  Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec. ", "original_text": "Discovery Databases*, 2011, pp. "}, "hash": "e40231af1d0fcbe16ff12b43b552c5cdb7869ac054456fe963bd5cf670623503", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0b48603-86c4-4203-a7b4-ab675129285e", "node_type": "1", "metadata": {"window": "Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n", "original_text": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. "}, "hash": "ff239661653c8dabb1318df49eef653aa8167dd5440096e45af70a116757a223", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "453-469.\n", "mimetype": "text/plain", "start_char_idx": 64124, "end_char_idx": 64133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0b48603-86c4-4203-a7b4-ab675129285e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n", "original_text": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7218540-34c7-481a-90b3-89c30a1fdc7d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp. ", "original_text": "453-469.\n"}, "hash": "6e64226c4d97e03533ed8f7dc8f99d0b590de64320e210a675a4220d964e72a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67854a78-f95f-440e-bdf4-1ef0901335c4", "node_type": "1", "metadata": {"window": "Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. ", "original_text": "IEEE Int. "}, "hash": "07ba5c457406504fa4c953b3b70f80e038d71bfb60e05ba328ecb348830d9994", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. ", "mimetype": "text/plain", "start_char_idx": 64133, "end_char_idx": 64241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67854a78-f95f-440e-bdf4-1ef0901335c4", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. ", "original_text": "IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0b48603-86c4-4203-a7b4-ab675129285e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Mach.  Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n", "original_text": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc. "}, "hash": "869bbb6ff309db1ac16f1fc67a4ef5ccde9dac2f2210ad07371d1634667b6af0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80431791-3e59-4487-a390-f88a62a51ec3", "node_type": "1", "metadata": {"window": "Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn.", "original_text": "Conf. "}, "hash": "08f2e2371685891a746cab13cde45ea73a48c4d81e369072bc9eb1400c58b39d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 64241, "end_char_idx": 64251, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80431791-3e59-4487-a390-f88a62a51ec3", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn.", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67854a78-f95f-440e-bdf4-1ef0901335c4", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Learn.  Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. ", "original_text": "IEEE Int. "}, "hash": "915231bee6f90e9f5a4efe5d3fc2fd609077505a2b314809b3f9f4f04ccb5075", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "242519f3-9374-44df-98a5-f8ea5c201eee", "node_type": "1", "metadata": {"window": "Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol. ", "original_text": "Data Mining*, Dec. "}, "hash": "2ae84cd7d7f7c68510b5c94300383cb25a532f792905f6bc5444c584dd8bebeb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 64251, "end_char_idx": 64257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "242519f3-9374-44df-98a5-f8ea5c201eee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol. ", "original_text": "Data Mining*, Dec. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80431791-3e59-4487-a390-f88a62a51ec3", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Knowl.  Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn.", "original_text": "Conf. "}, "hash": "0d90bee6205526e580e16062070171a95167feeba70940c225a867c086f20884", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4042f254-762d-4aa9-9030-7063ae3b9edb", "node_type": "1", "metadata": {"window": "453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no. ", "original_text": "2010, pp. "}, "hash": "628f42fad3c7d760704ad84a7add57da0e02b6e85e34faa532a287a63713230a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining*, Dec. ", "mimetype": "text/plain", "start_char_idx": 64257, "end_char_idx": 64276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4042f254-762d-4aa9-9030-7063ae3b9edb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no. ", "original_text": "2010, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "242519f3-9374-44df-98a5-f8ea5c201eee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Discovery Databases*, 2011, pp.  453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol. ", "original_text": "Data Mining*, Dec. "}, "hash": "3851440f6c879fb34f426fec9aa9caafd5c3b99615f22a82f8688a1a39c84d69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e424faae-cde1-49fe-ab3c-d614ea4accfb", "node_type": "1", "metadata": {"window": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp. ", "original_text": "953-958.\n"}, "hash": "6a0b459b79ecb129358fd32d0f5cec0473590f9929d7f11704cb8de7e6fba565", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2010, pp. ", "mimetype": "text/plain", "start_char_idx": 64276, "end_char_idx": 64286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e424faae-cde1-49fe-ab3c-d614ea4accfb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp. ", "original_text": "953-958.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4042f254-762d-4aa9-9030-7063ae3b9edb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "453-469.\n [13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no. ", "original_text": "2010, pp. "}, "hash": "ff538fc8a2a0ba9f0098c8b1bab5d9cc9c586271b1088e2869702f6c469f84c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f4d6fb2-26c3-410f-a26d-252350b4d7a6", "node_type": "1", "metadata": {"window": "IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n", "original_text": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. "}, "hash": "d40f3da24d012ca197e2ffc5e0cbf7943a194b80f499e57412821d79b10aaa5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "953-958.\n", "mimetype": "text/plain", "start_char_idx": 64286, "end_char_idx": 64295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f4d6fb2-26c3-410f-a26d-252350b4d7a6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n", "original_text": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e424faae-cde1-49fe-ab3c-d614ea4accfb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[13] K. Noto, C. Brodley, and D. Slonim, \"Anomaly detection using an ensemble of feature models,\u201d in *Proc.  IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp. ", "original_text": "953-958.\n"}, "hash": "0d570da67d4703df976ab6c0e6141df30e558a70b3c8f74a02614ba7ebaad42e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf6fb45-89f8-4019-9491-943fb9b2af6b", "node_type": "1", "metadata": {"window": "Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc.", "original_text": "Learn."}, "hash": "b52f7f48ba8e48b00c160075a275b9732d46c731b60f4647690084ff03b62faf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. ", "mimetype": "text/plain", "start_char_idx": 64295, "end_char_idx": 64368, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdf6fb45-89f8-4019-9491-943fb9b2af6b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc.", "original_text": "Learn."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f4d6fb2-26c3-410f-a26d-252350b4d7a6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n", "original_text": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach. "}, "hash": "558cc98a135cb7ec139f28f1ec533096d874cb37f508baa9d1d31eef6d3ee30c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8edf14a1-2398-4c6b-ab7b-55313e934b3e", "node_type": "1", "metadata": {"window": "Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol. ", "original_text": "*, vol. "}, "hash": "e5444769ec2861a418fb441bdc2731d207ecb52a49c1364660ebca4fceaf2cf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learn.", "mimetype": "text/plain", "start_char_idx": 64368, "end_char_idx": 64374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8edf14a1-2398-4c6b-ab7b-55313e934b3e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol. ", "original_text": "*, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdf6fb45-89f8-4019-9491-943fb9b2af6b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf.  Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc.", "original_text": "Learn."}, "hash": "000808b61e58a040e76d81859898b18d9fd88b6b46cd2208af167e95ca0658a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17b4a859-daab-4936-9011-951b29b0e6f2", "node_type": "1", "metadata": {"window": "2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no. ", "original_text": "102, no. "}, "hash": "c917355e7de0c3f317a13424f379173ad38579ac385a4c78b8c792b621ca9c1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, vol. ", "mimetype": "text/plain", "start_char_idx": 64374, "end_char_idx": 64382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17b4a859-daab-4936-9011-951b29b0e6f2", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no. ", "original_text": "102, no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8edf14a1-2398-4c6b-ab7b-55313e934b3e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Data Mining*, Dec.  2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol. ", "original_text": "*, vol. "}, "hash": "245550194d67e3119398c32b65dcd1873e1949c0f76fc2aa86591d969f57b3d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79fbf0cb-1d1b-4007-93b2-1d0abafb9d01", "node_type": "1", "metadata": {"window": "953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp. ", "original_text": "2, pp. "}, "hash": "d6b48b07859ac300a06e8ae8f60c0c27f3c146ec23dcfd44dd6f0a5545217070", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "102, no. ", "mimetype": "text/plain", "start_char_idx": 64382, "end_char_idx": 64391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "79fbf0cb-1d1b-4007-93b2-1d0abafb9d01", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp. ", "original_text": "2, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b4a859-daab-4936-9011-951b29b0e6f2", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2010, pp.  953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no. ", "original_text": "102, no. "}, "hash": "b19f1492207194076e75b1b6544907c05a4761e1d958e768916ed963e3edecc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e4539e0-a9b3-4f86-9307-72dd5afd53af", "node_type": "1", "metadata": {"window": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n", "original_text": "275-304, 2016.\n"}, "hash": "a803288d9de8dd06a3dad1166a107ef6cc03816a981b32c7b4e4c9421e79d67f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2, pp. ", "mimetype": "text/plain", "start_char_idx": 64391, "end_char_idx": 64398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e4539e0-a9b3-4f86-9307-72dd5afd53af", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n", "original_text": "275-304, 2016.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79fbf0cb-1d1b-4007-93b2-1d0abafb9d01", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "953-958.\n [14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp. ", "original_text": "2, pp. "}, "hash": "a90b0fffda67ec8d058794b08fdda5be67fcfd5c84ac040d74b9d111ebcaa700", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba21ecaa-19d4-46fc-8e1e-e34fa59ad5c9", "node_type": "1", "metadata": {"window": "Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. ", "original_text": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc."}, "hash": "abe04dc5331dbb25b344c95ab877c53816227bcc1ad1b9c956095501e7b29302", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "275-304, 2016.\n", "mimetype": "text/plain", "start_char_idx": 64398, "end_char_idx": 64413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba21ecaa-19d4-46fc-8e1e-e34fa59ad5c9", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. ", "original_text": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e4539e0-a9b3-4f86-9307-72dd5afd53af", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[14] T. Pevn\u1ef3, \"Loda: Lightweight on-line detector of anomalies,\" *Mach.  Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n", "original_text": "275-304, 2016.\n"}, "hash": "ea561d5647425e550fcfa9f6a2b17ba62a13351a1d237a0adec6672d5ebceabb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c88b46b5-1cbe-446b-ac98-203063dfb563", "node_type": "1", "metadata": {"window": "*, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu. ", "original_text": "*, vol. "}, "hash": "a3e3d286e492c572fdd86042176448966dc850230494e8007b482a783c577b28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc.", "mimetype": "text/plain", "start_char_idx": 64413, "end_char_idx": 64532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c88b46b5-1cbe-446b-ac98-203063dfb563", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu. ", "original_text": "*, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba21ecaa-19d4-46fc-8e1e-e34fa59ad5c9", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Learn. *, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. ", "original_text": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc."}, "hash": "9c416ad6199fd9e7055dc902f7029c309598c6a980beff1ed7893cdbdd0b71ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8c64eb7-d53a-4209-afed-fd917f07b0bf", "node_type": "1", "metadata": {"window": "102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond. ", "original_text": "91, no. "}, "hash": "cc40606483f377827e7441c8cd3ed7e64a11576325537dbf9c7b0b8b74624a63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, vol. ", "mimetype": "text/plain", "start_char_idx": 64532, "end_char_idx": 64540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8c64eb7-d53a-4209-afed-fd917f07b0bf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond. ", "original_text": "91, no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c88b46b5-1cbe-446b-ac98-203063dfb563", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu. ", "original_text": "*, vol. "}, "hash": "c6539a189a242e12bed85e75a9c7fe84eba6877c442f2a3b5060de7bad757998", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1eb5d9f1-5c8a-4848-8584-3aed71ba382d", "node_type": "1", "metadata": {"window": "2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf. ", "original_text": "435, pp. "}, "hash": "5ff02da55680b896fd9958174bf07028d612bd7f1ac63c67d385f22c8b4e0fec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "91, no. ", "mimetype": "text/plain", "start_char_idx": 64540, "end_char_idx": 64548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1eb5d9f1-5c8a-4848-8584-3aed71ba382d", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf. ", "original_text": "435, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8c64eb7-d53a-4209-afed-fd917f07b0bf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "102, no.  2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond. ", "original_text": "91, no. "}, "hash": "7fb0e9108eab010abefeb07b9a6290aa436abf0c3e3b85fa5719d0e8ce01d795", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e6eb14c-fc54-4e6e-b20b-835259255b2e", "node_type": "1", "metadata": {"window": "275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf.", "original_text": "1047-1061, 1996.\n"}, "hash": "a9ec71f5748f615e164c5cbba460f33dba9a2f3f0b1392ec9c9458627a088393", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "435, pp. ", "mimetype": "text/plain", "start_char_idx": 64548, "end_char_idx": 64557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e6eb14c-fc54-4e6e-b20b-835259255b2e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf.", "original_text": "1047-1061, 1996.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1eb5d9f1-5c8a-4848-8584-3aed71ba382d", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "2, pp.  275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf. ", "original_text": "435, pp. "}, "hash": "29b504abd383e4471fe0430dd208e449893b55a945c67973e1bdff444da000c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72fca0d8-4964-46cf-8db2-408326cf278b", "node_type": "1", "metadata": {"window": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp. ", "original_text": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. "}, "hash": "12e56f32b3e3eefffbb0eba156a0886d532852c3776ecdbdba0df259a66753d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1047-1061, 1996.\n", "mimetype": "text/plain", "start_char_idx": 64557, "end_char_idx": 64574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72fca0d8-4964-46cf-8db2-408326cf278b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp. ", "original_text": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e6eb14c-fc54-4e6e-b20b-835259255b2e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "275-304, 2016.\n [15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf.", "original_text": "1047-1061, 1996.\n"}, "hash": "0a2b37fd788e150b8647b0ccd5abcd40bdcf9b121a8280cf5c3cb00d8f9f38ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38937ec1-efb6-44f9-8393-05a8fecddaaf", "node_type": "1", "metadata": {"window": "*, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n", "original_text": "28th Annu. "}, "hash": "8905c8b52b448095ac026a27bd65b2c5d1661d4ceb8075666800f6525703005b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. ", "mimetype": "text/plain", "start_char_idx": 64574, "end_char_idx": 64716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38937ec1-efb6-44f9-8393-05a8fecddaaf", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n", "original_text": "28th Annu. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72fca0d8-4964-46cf-8db2-408326cf278b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[15] D. M. Rocke and D. L. Woodruff, \"Identification of outliers in multivariate data,\" *J. American Statistical Assoc. *, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp. ", "original_text": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc. "}, "hash": "54f0446d5b216c928d9da68d3513436e7298f9a2e7b65d898460db81658214cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "735767f6-0035-4820-89e5-21f3c7c1e26e", "node_type": "1", "metadata": {"window": "91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. ", "original_text": "SEMI Advanced Semicond. "}, "hash": "6d1b0070bfec7bf8bbf30c763ab00d55ddf8241c99da2c9441806409f16090c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "28th Annu. ", "mimetype": "text/plain", "start_char_idx": 64716, "end_char_idx": 64727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "735767f6-0035-4820-89e5-21f3c7c1e26e", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. ", "original_text": "SEMI Advanced Semicond. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38937ec1-efb6-44f9-8393-05a8fecddaaf", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, vol.  91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n", "original_text": "28th Annu. "}, "hash": "af6fd39611725a66cbd03a52e2feb48fdd22225c30b61bc6bcdd119f194800dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e27d314-f258-4b3c-b9a1-095330fa50a1", "node_type": "1", "metadata": {"window": "435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. ", "original_text": "Manuf. "}, "hash": "de1492ed9a3ebb78fa5ab6e464b117b24181bc993f893138d7bd592a6e60cdc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SEMI Advanced Semicond. ", "mimetype": "text/plain", "start_char_idx": 64727, "end_char_idx": 64751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e27d314-f258-4b3c-b9a1-095330fa50a1", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. ", "original_text": "Manuf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "735767f6-0035-4820-89e5-21f3c7c1e26e", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "91, no.  435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. ", "original_text": "SEMI Advanced Semicond. "}, "hash": "677c1f4ad49be14bb76bb301ade1a8dc21c3bdbc3230fbb4fbb23be0f3c6033b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b42a880f-d220-4c02-b392-0cd52a6ce1b5", "node_type": "1", "metadata": {"window": "1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. ", "original_text": "Conf."}, "hash": "cbed94d12ad149503bfbad653dc4aa0444892a28a29aa7019dc0ec7b7be1c510", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Manuf. ", "mimetype": "text/plain", "start_char_idx": 64751, "end_char_idx": 64758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b42a880f-d220-4c02-b392-0cd52a6ce1b5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. ", "original_text": "Conf."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e27d314-f258-4b3c-b9a1-095330fa50a1", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "435, pp.  1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. ", "original_text": "Manuf. "}, "hash": "0943f18326931d2a8e8baa249fcc1cb3370e95dd759ef1bcaeeac30a608d32ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf83213-a90e-4785-9d7c-2ec17629e436", "node_type": "1", "metadata": {"window": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. ", "original_text": "*, May 2017, pp. "}, "hash": "28617012736bb46eec173a2b275076ef1fe76019a33a428e0cac88e821c99794", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf.", "mimetype": "text/plain", "start_char_idx": 64758, "end_char_idx": 64763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdf83213-a90e-4785-9d7c-2ec17629e436", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. ", "original_text": "*, May 2017, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b42a880f-d220-4c02-b392-0cd52a6ce1b5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "1047-1061, 1996.\n [16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. ", "original_text": "Conf."}, "hash": "01331362326a3417f227cbbd9a1c50d16d150cb41dc77d27466c3788053bf54d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64def00f-772a-4af5-bcc0-cabfe834eddb", "node_type": "1", "metadata": {"window": "28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n", "original_text": "89\u201394.\n\n"}, "hash": "287b07157883df336907f1b9077abf6155d872820d61f1566ef8195c69afc24b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*, May 2017, pp. ", "mimetype": "text/plain", "start_char_idx": 64763, "end_char_idx": 64780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64def00f-772a-4af5-bcc0-cabfe834eddb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n", "original_text": "89\u201394.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdf83213-a90e-4785-9d7c-2ec17629e436", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "[16] G. A. Susto, A. Beghi, and S. McLoone, \"Anomaly detection through on-line isolation forest: An application to plasma etching,\" in *Proc.  28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. ", "original_text": "*, May 2017, pp. "}, "hash": "ea7066cb6ca8c3471c7782dd0ec07ecf2cc39c189887003ca9420ae9bf4db1ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7944e31f-b48f-48e8-be95-b694c4ebe7ba", "node_type": "1", "metadata": {"window": "SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. ", "original_text": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. "}, "hash": "0eb7e7c301fbd94e8e8d3199e9921da89e480938805bd73f6e43c13fa1dba47a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "89\u201394.\n\n", "mimetype": "text/plain", "start_char_idx": 64780, "end_char_idx": 64788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7944e31f-b48f-48e8-be95-b694c4ebe7ba", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. ", "original_text": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64def00f-772a-4af5-bcc0-cabfe834eddb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "28th Annu.  SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n", "original_text": "89\u201394.\n\n"}, "hash": "c4110f15cc6a2eae0b5bd09f6e6ca9e9300962f62f19acc8e58fe94ae6e4ca6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85358056-945e-4985-9101-94e72881c7a6", "node_type": "1", "metadata": {"window": "Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. ", "original_text": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. "}, "hash": "685df6009e6f9670a871ee39c3d65704efeee02ef3201e54357544fd5ffff6e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. ", "mimetype": "text/plain", "start_char_idx": 64788, "end_char_idx": 64940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85358056-945e-4985-9101-94e72881c7a6", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. ", "original_text": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7944e31f-b48f-48e8-be95-b694c4ebe7ba", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "SEMI Advanced Semicond.  Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. ", "original_text": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University. "}, "hash": "73aa675995864b1662fe70bc0f7ee0dae601c29b7a8839a674183d0f8f2d84c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2a01c37-cb09-4cdf-8460-cde1be403a34", "node_type": "1", "metadata": {"window": "Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. ", "original_text": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. "}, "hash": "7c545141858d1e2f4f8cf13acab35e1c68a0bb6036ead6bba52a1046b638ad17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. ", "mimetype": "text/plain", "start_char_idx": 64940, "end_char_idx": 65034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2a01c37-cb09-4cdf-8460-cde1be403a34", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. ", "original_text": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85358056-945e-4985-9101-94e72881c7a6", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Manuf.  Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. ", "original_text": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years. "}, "hash": "7a001de11eece40fe33ac918f3d3ae85f7cb35571acbad951c37893a019f2aac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5465349f-a180-47b6-a990-25eff0088c0b", "node_type": "1", "metadata": {"window": "*, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. ", "original_text": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. "}, "hash": "115d028db90438f2b4860ce28ad3ed8a3710a663b1345af75e7cb5a3a1e3215b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. ", "mimetype": "text/plain", "start_char_idx": 65034, "end_char_idx": 65318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5465349f-a180-47b6-a990-25eff0088c0b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. ", "original_text": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2a01c37-cb09-4cdf-8460-cde1be403a34", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Conf. *, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. ", "original_text": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind. "}, "hash": "a561c684a4d06ea36ab2777536467dc3cf946d08b0926aedb47dd1d1dd068b2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4fc24c2-0780-4d7a-be8e-50172d39d5b7", "node_type": "1", "metadata": {"window": "89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team. ", "original_text": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n"}, "hash": "cf91cd188439478979b5d61c8d0fb867106705d16f9755ba1b81f55442ef98d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. ", "mimetype": "text/plain", "start_char_idx": 65318, "end_char_idx": 65441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4fc24c2-0780-4d7a-be8e-50172d39d5b7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team. ", "original_text": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5465349f-a180-47b6-a990-25eff0088c0b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "*, May 2017, pp.  89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. ", "original_text": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems. "}, "hash": "aa6ca268c8c0c1fa8125e3e78bc420a20e58edbd6df54a6769f36966e1e96c71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ae4c50c-1d55-4dfd-8dbf-edcfac682502", "node_type": "1", "metadata": {"window": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. ", "original_text": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. "}, "hash": "5e0fdcbdbd81bbd4e9db4bef2e11b02e04a10aeaa3c8065072d89895f5b6056f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n", "mimetype": "text/plain", "start_char_idx": 65441, "end_char_idx": 65501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ae4c50c-1d55-4dfd-8dbf-edcfac682502", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. ", "original_text": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4fc24c2-0780-4d7a-be8e-50172d39d5b7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "89\u201394.\n\n ---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team. ", "original_text": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n"}, "hash": "e111f076a75afd4df1bc0754b6b80693ab92c451053df0160ab1265e29dffa87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a589826-2538-4986-977c-c34027056cfc", "node_type": "1", "metadata": {"window": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. ", "original_text": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. "}, "hash": "10de12b4b033e7abbf87aa0d26cf68c00e69948439304f3909d4710ec8301234", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. ", "mimetype": "text/plain", "start_char_idx": 65501, "end_char_idx": 65750, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a589826-2538-4986-977c-c34027056cfc", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. ", "original_text": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae4c50c-1d55-4dfd-8dbf-edcfac682502", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "---\n\u00b2 https://github.com/sahandha/eif\n\n**Sahand Hariri** received the bachelor's and master's degrees in mechanical engineering from Temple University.  He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. ", "original_text": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy. "}, "hash": "ac6202ace7880e92ce3c7b198b32d8268ea26136f197ff128382a4fcb89ab6f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc79753e-d0e3-4cc1-a8ce-893e22c6a7fa", "node_type": "1", "metadata": {"window": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. ", "original_text": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. "}, "hash": "67a30013c40a757daba75f395a58ac0f4a9fe8e486ab9164d31da61fbfbedf6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. ", "mimetype": "text/plain", "start_char_idx": 65750, "end_char_idx": 65922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc79753e-d0e3-4cc1-a8ce-893e22c6a7fa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. ", "original_text": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a589826-2538-4986-977c-c34027056cfc", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He then joined Wolfram Alpha LLC, where he was a research developer for two and a half years.  After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. ", "original_text": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC. "}, "hash": "c5180e3032a928245e1e49c29255266d3f8cc97d2b52b3998dbe5f94ad2c9e2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d123c811-930b-45a1-b03d-93363b778bfa", "node_type": "1", "metadata": {"window": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n", "original_text": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. "}, "hash": "685e4f6706e8743a3559e6efeeca74b6522b908eb89460f484c9e2d11890d03f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. ", "mimetype": "text/plain", "start_char_idx": 65922, "end_char_idx": 66073, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d123c811-930b-45a1-b03d-93363b778bfa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n", "original_text": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc79753e-d0e3-4cc1-a8ce-893e22c6a7fa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "After that, while a graduate student at the University of Illinois in theoretical and applied mechanics, He worked at the National Center for Supercomputing Applications through the summers of 2016, 2017, and 2018 where he met and started his collaboration with Matias Carrasco Kind.  His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. ", "original_text": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project. "}, "hash": "a7f6853a769ccbc354fd3f6a6576c2aed39729937d5748746040cb8c6bf542f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5178648b-cc18-4f7b-9055-732387dc9812", "node_type": "1", "metadata": {"window": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. ", "original_text": "He is also a member of the LSST Data Management team. "}, "hash": "c663fb18cecf55863c0f6144a21abc8383c375a9e0362ca834fe2d367ebcf0fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. ", "mimetype": "text/plain", "start_char_idx": 66073, "end_char_idx": 66200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5178648b-cc18-4f7b-9055-732387dc9812", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. ", "original_text": "He is also a member of the LSST Data Management team. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d123c811-930b-45a1-b03d-93363b778bfa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "His interests include applications of computational methods in sciences, more specifically, dynamical and complex systems.  Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n", "original_text": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure. "}, "hash": "4cf03a92975cd4807de7ee42a029ee8fe44d150bbd5cb72ffa5ff263f803f521", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b661684c-9d4e-4e3f-beb4-7a315372ceb5", "node_type": "1", "metadata": {"window": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. ", "original_text": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. "}, "hash": "d684a915cd692a540025e8e2a76c64a4e3f7349c4d2bb5aaee1efff1bc6ab79c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is also a member of the LSST Data Management team. ", "mimetype": "text/plain", "start_char_idx": 66200, "end_char_idx": 66254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b661684c-9d4e-4e3f-beb4-7a315372ceb5", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. ", "original_text": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5178648b-cc18-4f7b-9055-732387dc9812", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Currently, he works at PSI Metals GmbH in Berlin, Germany.\n\n **Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. ", "original_text": "He is also a member of the LSST Data Management team. "}, "hash": "a2ea2d518008e0091814881d6a0abbf70a185a63b5ed26e4a427878148a47ada", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ed1a886-c222-4485-85b4-ca776cd12efb", "node_type": "1", "metadata": {"window": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. ", "original_text": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. "}, "hash": "fd482b6cc7d7028996306609d74e9d7f21f97aea2ccb5671c4e52625085ab6d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. ", "mimetype": "text/plain", "start_char_idx": 66254, "end_char_idx": 66486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ed1a886-c222-4485-85b4-ca776cd12efb", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. ", "original_text": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b661684c-9d4e-4e3f-beb4-7a315372ceb5", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Matias Carrasco Kind** received the PhD degree in astronomy with the computational science and engineering CSE graduate option at University of Illinois at Urbana-Champaign (UIUC) which focused in machine learning techniques applied to Astronomy.  He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. ", "original_text": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general. "}, "hash": "86a8801ec92f862758d539f4d90cc0523787fb50b6b8a881922ca0055df35e3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a5c7c56-b318-4150-84b8-9b8f4c901217", "node_type": "1", "metadata": {"window": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. ", "original_text": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. "}, "hash": "eaa842c4f19c19c0c5efba3eb252606cc318fcf4569f5e5b6dab5a0ed74d94cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. ", "mimetype": "text/plain", "start_char_idx": 66486, "end_char_idx": 66705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a5c7c56-b318-4150-84b8-9b8f4c901217", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. ", "original_text": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ed1a886-c222-4485-85b4-ca776cd12efb", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is currently a senior research and Data Scientist at the National Center for Supercomputing Applications (NCSA) and assistant research professor in astronomy with UIUC.  He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. ", "original_text": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation. "}, "hash": "3b9764ae8f5a8b043294505acc2293005fd3a06d0fb3f3925cd4384f920c9d10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cf1658a-8c9d-49d6-a381-7392ca17efee", "node_type": "1", "metadata": {"window": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois. ", "original_text": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n"}, "hash": "e1acc23ac21eec512687fe0d26a97a998699d2783fd28d986d4c898eca128781", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. ", "mimetype": "text/plain", "start_char_idx": 66705, "end_char_idx": 66814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6cf1658a-8c9d-49d6-a381-7392ca17efee", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois. ", "original_text": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a5c7c56-b318-4150-84b8-9b8f4c901217", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is also a Trusted CI fellow for the NSF Cybersecurity Center of Excellence and the Data Release Scientist for the Dark Energy Survey (DES) Project.  He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. ", "original_text": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors. "}, "hash": "667b79e3144e1cbd5f79f2eea0d9dd5aa04183c745e90f37004c3c88a2f691ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6a5c59b-ede1-4280-a47a-f074d35a3a70", "node_type": "1", "metadata": {"window": "He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. ", "original_text": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. "}, "hash": "d5efc81f64b81048e275098f3c225a4b3ab37bcc8e1f3953ce2848e6b7107d0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n", "mimetype": "text/plain", "start_char_idx": 66814, "end_char_idx": 67043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b6a5c59b-ede1-4280-a47a-f074d35a3a70", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. ", "original_text": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cf1658a-8c9d-49d6-a381-7392ca17efee", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is an expert in scientific cloud computing and scientific platform and currently leads the DES Data Release Infrastructure.  He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois. ", "original_text": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n"}, "hash": "0ee5406f291658feda4fc001704949f831bfdb475a7b9da8a594c840aade7b4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca2680dc-cfe3-4a5d-90f5-df5aca80c52c", "node_type": "1", "metadata": {"window": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. ", "original_text": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. "}, "hash": "d052c1078d916b1aabf64ddd9ca7e542cf814a685f5b513eb602cf161829fed8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. ", "mimetype": "text/plain", "start_char_idx": 67043, "end_char_idx": 67238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca2680dc-cfe3-4a5d-90f5-df5aca80c52c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. ", "original_text": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6a5c59b-ede1-4280-a47a-f074d35a3a70", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is also a member of the LSST Data Management team.  He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. ", "original_text": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey. "}, "hash": "f41f8e327438396390ecffd3039fdd5421a883cd5334c466f55d907fd39aa4bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "002dc7db-5e02-4377-8b86-fd269f2b820a", "node_type": "1", "metadata": {"window": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. ", "original_text": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. "}, "hash": "9df6d4f3bfa353891872479b0a352758143c3f41148c33b2fc8e2364eb331d3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. ", "mimetype": "text/plain", "start_char_idx": 67238, "end_char_idx": 67372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "002dc7db-5e02-4377-8b86-fd269f2b820a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. ", "original_text": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca2680dc-cfe3-4a5d-90f5-df5aca80c52c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is interested in data-intensive science as well as artificial intelligence, data visualization, image processing, web applications, scientific platforms, software engineering and architecture, and cyberinfrastructure in general.  His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. ", "original_text": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois. "}, "hash": "afc0fa07793dfe8b42806c95a6cfe93a122050f7c60251d43ebea2a1c49181bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8b6cfb2-7dcd-4db8-81e1-5cac44475db7", "node_type": "1", "metadata": {"window": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. ", "original_text": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. "}, "hash": "de344950d5d00ebaf11183b2dcacb5fe949bfb0751104a1870179cf9e08bc01d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. ", "mimetype": "text/plain", "start_char_idx": 67372, "end_char_idx": 67531, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8b6cfb2-7dcd-4db8-81e1-5cac44475db7", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. ", "original_text": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "002dc7db-5e02-4377-8b86-fd269f2b820a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "His interests in astrophysics are in the area of cosmology, extragalactic astronomy, machine and deep learning, especially in large scale structures, galaxy formation and evolution, and photometric redshift estimation.  Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. ", "original_text": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics. "}, "hash": "007efeba423531a930f3336e511d294d3035ea63840c7ced2579fe44fff42685", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d70efa0-b827-4da2-9834-ec828478b012", "node_type": "1", "metadata": {"window": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n", "original_text": "He is also the data science expert in residence at the Research Park at the University of Illinois. "}, "hash": "6be808d108b79a29b37cfa21fd11451ebd81e816ee4809370729036195d005ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. ", "mimetype": "text/plain", "start_char_idx": 67531, "end_char_idx": 67862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d70efa0-b827-4da2-9834-ec828478b012", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n", "original_text": "He is also the data science expert in residence at the Research Park at the University of Illinois. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8b6cfb2-7dcd-4db8-81e1-5cac44475db7", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "Before that, he received the degree in astronomy at Universidad Catolica de Chile (PUC) with highest honors.  For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. ", "original_text": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications. "}, "hash": "943690e56bda68cc187c04e6f73cfb409b6825f3f00178d66abab1b5cd7b890b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84df85ce-4005-4f35-bbf1-a1ebbaa214fa", "node_type": "1", "metadata": {"window": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. "}, "hash": "104bbab4dbb9cd7166d1e31cfa25ef46c4a2a998d835ceba842a6cd33527dc1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He is also the data science expert in residence at the Research Park at the University of Illinois. ", "mimetype": "text/plain", "start_char_idx": 67862, "end_char_idx": 67962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84df85ce-4005-4f35-bbf1-a1ebbaa214fa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d70efa0-b827-4da2-9834-ec828478b012", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "For his undergraduate, he was at the Max-Planck Institute for Astrophysics (MPA) in Germany where he also spent some time doing PhD studies before working as an adjunct professor at the Universidad Andres Bello (UNAB) in Chile.\n\n **Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n", "original_text": "He is also the data science expert in residence at the Research Park at the University of Illinois. "}, "hash": "11c3f59464f861b70bc5fbd4ea6936e66e641aa559521617706346c0f8cd74df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c13eeda-a4ed-4b9d-8651-a623a1d7a43a", "node_type": "1", "metadata": {"window": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. "}, "hash": "362c141c8ee4d737410f2d10fe6dbb3365b073a4bccb7b40af584208414c4f64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. ", "mimetype": "text/plain", "start_char_idx": 67962, "end_char_idx": 68148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c13eeda-a4ed-4b9d-8651-a623a1d7a43a", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84df85ce-4005-4f35-bbf1-a1ebbaa214fa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "**Robert J. Brunner** received the PhD degree in astrophysics from Johns Hopkins University, working under Alex Szalay on the development of the science archive for the Sloan Digital Sky Survey.  He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise. "}, "hash": "4d93c636a1f8e684cf89a9c581ee165a6dd3e7c5f8cd0ff1a15b07ae783fdfb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5879ef4e-af85-468f-8e6d-14a5ea0d2faa", "node_type": "1", "metadata": {"window": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. "}, "hash": "cce4f5635c4feeb3aaecaabcdcbb06328e541542f8442bea09748b493cc75a4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. ", "mimetype": "text/plain", "start_char_idx": 68148, "end_char_idx": 68293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5879ef4e-af85-468f-8e6d-14a5ea0d2faa", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c13eeda-a4ed-4b9d-8651-a623a1d7a43a", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is the associate dean for innovation and chief disruption officer with the Gies College of Business at the University of Illinois.  He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies. "}, "hash": "b08c6626d5c3e3bf8ebff8430c2de13f2250e7e8b0bc0510c0d48b84285b5978", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c46313dc-0e8c-4b12-bc72-bca6c941de8c", "node_type": "1", "metadata": {"window": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. "}, "hash": "48a9409cf2c9424387fbf6dfee3d0d1bc03c5e144d425297539af72154c7190e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. ", "mimetype": "text/plain", "start_char_idx": 68293, "end_char_idx": 68453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c46313dc-0e8c-4b12-bc72-bca6c941de8c", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5879ef4e-af85-468f-8e6d-14a5ea0d2faa", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is a professor of accountancy, an Arthur Anderson fellow, and the director of the University of Illinois Deloitte Foundation Center for Business Analytics.  He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques. "}, "hash": "63d4a4f2e7127abec57534e9b3ef8aeab775eb470fba0e963bbf921f6634e5b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e958195-7b97-419b-898d-01eb0bd4199b", "node_type": "1", "metadata": {"window": "He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n"}, "hash": "88ccc05604da1c50c858739476f12382233cc444d019fb0da69720b6346f3391", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. ", "mimetype": "text/plain", "start_char_idx": 68453, "end_char_idx": 68635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e958195-7b97-419b-898d-01eb0bd4199b", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c46313dc-0e8c-4b12-bc72-bca6c941de8c", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He holds affiliate appointments in the Astronomy, Computer Science, Electrical and Computer Engineering, Informatics, Physics, the School of Information Sciences, and Statistics Departments; at the Beckman Institute, in the Computational Science and Engineering program; and at the National Center for Supercomputing Applications.  He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe. "}, "hash": "63d69a0db20162351de2874f5ee3f03090cc83516734adebdec6b9fc3de3b5ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60a42a5e-d66d-44bb-9950-6f9c9ea8c737", "node_type": "1", "metadata": {"window": "His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "\u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl."}, "hash": "9ecf1504b85edb4de7cd3caea2f65488f03e7ebdead265883622f7606317bbf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n", "mimetype": "text/plain", "start_char_idx": 68635, "end_char_idx": 68840, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60a42a5e-d66d-44bb-9950-6f9c9ea8c737", "embedding": null, "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "\u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "cade406d-a80e-4a84-beeb-fdbde0643687", "node_type": "4", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf"}, "hash": "5930c693621e83f5002dcbd58f7cf2aff8b1250e63f2faa195c4920b44841f00", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e958195-7b97-419b-898d-01eb0bd4199b", "node_type": "1", "metadata": {"title": "Extended Isolation Forest", "authors": "Hariri et al.", "year": 2021, "file_path": "ad-papers-pdf/extended_isolation_forest.pdf", "window": "He is also the data science expert in residence at the Research Park at the University of Illinois.  His primary research interests include application of statistical and machine learning to a variety of real-world problems, and in making these efforts easier, faster, and more precise.  This work spans fundamental algorithm design to more effectively incorporate uncertainty to optimization using novel computational technologies.  More generally, he helps lead efforts to promote data science across campus and to encourage effective data management, analysis, and visualization techniques.  His PhD thesis helped develop the statistical approach to quantifying galaxy evolution, where large data are used to place constraints on the original and evolution of the Universe.  He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n \u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "original_text": "He subsequently spent five years as a postdoctoral scholar at the California Institute of Technology working under S. George Djorgovsi and Tom Prince as the project scientist for the Digital Sky project.\n\n"}, "hash": "31b89e874cd7b2139468a27fcaf473c7a5540a0fc7b26c4738d66f5e092edd51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u25b7 For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.", "mimetype": "text/plain", "start_char_idx": 68840, "end_char_idx": 68959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]