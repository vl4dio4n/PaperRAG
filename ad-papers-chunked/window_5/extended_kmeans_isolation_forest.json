[{"id_": "bfeb25db-a7ed-4ba5-a432-d76eae5e3e27", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail. ", "original_text": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc1af42c-e6f6-4877-8a46-3d7790b464ae", "node_type": "1", "metadata": {"window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n", "original_text": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions. "}, "hash": "39a6631e106dfd435a43a4f731f0de53eef92dbea6767610e5edfe766ac5aab5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc1af42c-e6f6-4877-8a46-3d7790b464ae", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n", "original_text": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfeb25db-a7ed-4ba5-a432-d76eae5e3e27", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail. ", "original_text": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields. "}, "hash": "13d70ea01f18a76fdc923db51a173c80ce821543d73f85375ff53031ad92f1c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8304608-47f4-4e1b-bdd5-5added8abca2", "node_type": "1", "metadata": {"window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. ", "original_text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF). "}, "hash": "d09b2956a4656972c57a80c97df1363c711a7a46ae7c31c76e9210af9f8c347b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions. ", "mimetype": "text/plain", "start_char_idx": 333, "end_char_idx": 505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8304608-47f4-4e1b-bdd5-5added8abca2", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. ", "original_text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc1af42c-e6f6-4877-8a46-3d7790b464ae", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n", "original_text": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions. "}, "hash": "8731c4f0f72f46c194cad58d56d2be878040c5b73987da439af34f146d8c3b97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a4aa507-0145-4201-9db9-9751289893b3", "node_type": "1", "metadata": {"window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n", "original_text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering. "}, "hash": "8e43d3297f60868a1230d64c630b18e3cd9185640f1af6cb1f62fcaf38a3c4fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF). ", "mimetype": "text/plain", "start_char_idx": 505, "end_char_idx": 730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a4aa507-0145-4201-9db9-9751289893b3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n", "original_text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8304608-47f4-4e1b-bdd5-5added8abca2", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. ", "original_text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF). "}, "hash": "a6021f72bbb10077496ae9a6184bd46d77f904fcb3cae4412180b64c227af06c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ffbc8cb-7d3b-4aec-8da9-6ebd20fef57c", "node_type": "1", "metadata": {"window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. ", "original_text": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics. "}, "hash": "67cb70f533beef6efd7f599f4ba496bc427e4765cc36cb5eef155c5581ea6b46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering. ", "mimetype": "text/plain", "start_char_idx": 730, "end_char_idx": 1067, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ffbc8cb-7d3b-4aec-8da9-6ebd20fef57c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. ", "original_text": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a4aa507-0145-4201-9db9-9751289893b3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n", "original_text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering. "}, "hash": "ad388c5a02c7f98721ef6876f7df67fabde34dad5f91ebc48576e23729cc879b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42b66d1c-1ee1-4ee7-bf51-206d2fe7c972", "node_type": "1", "metadata": {"window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" ", "original_text": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail. "}, "hash": "8cf0b98bb55475ea311dc2651e9b8c2551d79f5646b80305155010b3dd00e321", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics. ", "mimetype": "text/plain", "start_char_idx": 1067, "end_char_idx": 1172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42b66d1c-1ee1-4ee7-bf51-206d2fe7c972", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" ", "original_text": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ffbc8cb-7d3b-4aec-8da9-6ebd20fef57c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. ", "original_text": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics. "}, "hash": "dbd4d67064e9798f19a0ab7e93089368619d5ed92c4a4b209b46fd7b6794ae54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62d733ac-c3a7-4be4-9bd1-9c925c94e601", "node_type": "1", "metadata": {"window": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. ", "original_text": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n"}, "hash": "e0e5cc15671f61fc71e92028c823085f3becbe36bfb08b3ed8d5149c91da1510", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail. ", "mimetype": "text/plain", "start_char_idx": 1172, "end_char_idx": 1377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62d733ac-c3a7-4be4-9bd1-9c925c94e601", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. ", "original_text": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42b66d1c-1ee1-4ee7-bf51-206d2fe7c972", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "# Extended K-Means Isolation Forest: A Hybrid Approach Combining Random Projections and Clustering for Anomaly Detection\nVlad-Ioan B\u00eersan\nComputer Science Department, University of Bucharest, Romania\n\n## Abstract\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields.  While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" ", "original_text": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail. "}, "hash": "3bc5231326db55f2f3303deee9a22e153eacd8a951dc1a315c7c02b8a56a91b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "775672b7-91ba-4319-878e-728881107437", "node_type": "1", "metadata": {"window": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. ", "original_text": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. "}, "hash": "9ee415b85fd2a07b6e4fb68b472fa16ebbd4ed9a735470c7e650ce668d00c9c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n", "mimetype": "text/plain", "start_char_idx": 1377, "end_char_idx": 1555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "775672b7-91ba-4319-878e-728881107437", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. ", "original_text": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62d733ac-c3a7-4be4-9bd1-9c925c94e601", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "While isolation-based methods like Isolation Forest (IF) are highly effective, their axis-parallel partitioning strategy can struggle with complex geometric distributions.  In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. ", "original_text": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n"}, "hash": "0d76375b1f3acb0274ee9ffa6c19adb83724bb45495fb9b41a03d37899d131e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04c0c844-1856-47e4-9810-8810493f1a1e", "node_type": "1", "metadata": {"window": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency. ", "original_text": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n"}, "hash": "bf3f42428a8bfa0c07d2aec04592c31ec5fbaee3148c283ba8420e4ab41b70c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. ", "mimetype": "text/plain", "start_char_idx": 1555, "end_char_idx": 1791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04c0c844-1856-47e4-9810-8810493f1a1e", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency. ", "original_text": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775672b7-91ba-4319-878e-728881107437", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across existing isolation forest variants, including Standard IF, Extended IF (EIF), Generalized IF (GIF), and K-Means IF (K-Means IF).  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. ", "original_text": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection. "}, "hash": "09f8040e6e3f13e8e7d9639ed15f1b073ab7292625922fb5f4142e101a9bff27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb04e525-c140-4848-a22c-c0ffdf89c549", "node_type": "1", "metadata": {"window": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n", "original_text": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. "}, "hash": "839e507a9d9b4abe60428fcb09e6b444ac8340b24fc28a2eb29aa23bc9ce214b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n", "mimetype": "text/plain", "start_char_idx": 1791, "end_char_idx": 2157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb04e525-c140-4848-a22c-c0ffdf89c549", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n", "original_text": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04c0c844-1856-47e4-9810-8810493f1a1e", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF: (1) Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, and (2) Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering.  We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency. ", "original_text": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n"}, "hash": "37f15dfc4871bcfd03d47e20812fa167a8d4dad4f0c4009c15153af35cb7facb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e570ec94-cfbc-424b-8397-ebe1e703565a", "node_type": "1", "metadata": {"window": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations. ", "original_text": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" "}, "hash": "95bbab076b88f3c8da5d281adb81d97da1323e1b4dd0fe90f94707be34422282", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. ", "mimetype": "text/plain", "start_char_idx": 2157, "end_char_idx": 2287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e570ec94-cfbc-424b-8397-ebe1e703565a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations. ", "original_text": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb04e525-c140-4848-a22c-c0ffdf89c549", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We evaluate these six methods on 13 benchmark datasets using ROC-AUC, PR-AUC, and training time metrics.  Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n", "original_text": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data. "}, "hash": "1044dc828c5e491c8f1b225f82e49076b556efcc8ab4a995242e0fa9cf943bba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d91bb205-fe67-4c4b-b8b3-3cae8b00167e", "node_type": "1", "metadata": {"window": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. ", "original_text": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. "}, "hash": "a01a8c7ea89fcf3891d04f5782196bf5020f19ef77de8a8aab5df1b7e568399c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" ", "mimetype": "text/plain", "start_char_idx": 2287, "end_char_idx": 2444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d91bb205-fe67-4c4b-b8b3-3cae8b00167e", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. ", "original_text": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e570ec94-cfbc-424b-8397-ebe1e703565a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Our results indicate that while the proposed hybrid approaches incur higher computational costs, they offer distinct advantages in capturing anomalies within complex manifolds where standard methods fail.  Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations. ", "original_text": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\" "}, "hash": "aee41126b58c17c5dd8583bdd69144981c9eb9e249d067072f776d1110263223", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65defe9a-4106-43a1-a610-5b0f8f81e096", "node_type": "1", "metadata": {"window": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. ", "original_text": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. "}, "hash": "7ba4afe2a3ccb67fd77e224bad714908c6701cd0eb7d4a10ef46dc660e177d11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. ", "mimetype": "text/plain", "start_char_idx": 2444, "end_char_idx": 2571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65defe9a-4106-43a1-a610-5b0f8f81e096", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. ", "original_text": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d91bb205-fe67-4c4b-b8b3-3cae8b00167e", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Ultimately, we demonstrate that there is no single \"winner\" algorithm; rather, the optimal choice depends on the specific geometric structure and dimensionality of the dataset.\n\n ## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. ", "original_text": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values. "}, "hash": "5f554583635e0da45cd6bc1552191fd95568c5eaf0ee1c2a0eb79545a48cf5c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "691436f1-c388-4ad6-bc92-c12751d29dd5", "node_type": "1", "metadata": {"window": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. ", "original_text": "This purely randomized approach offers significant computational efficiency. "}, "hash": "5256486d0a2d7be9c6480ca48ba36cdfbbddfbeb8601ca1fc4fa7d0044cf869a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. ", "mimetype": "text/plain", "start_char_idx": 2571, "end_char_idx": 2778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "691436f1-c388-4ad6-bc92-c12751d29dd5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. ", "original_text": "This purely randomized approach offers significant computational efficiency. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65defe9a-4106-43a1-a610-5b0f8f81e096", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## 1 Introduction\n\nUnsupervised anomaly detection is a fundamental problem in data mining, with applications spanning across many fields including fraud detection, fault diagnosis in industrial systems, and network intrusion detection.  The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. ", "original_text": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances. "}, "hash": "27c3e94c667fe693fd8d34c139bc4a2781eb7922e1ff6a18e2965b2cd35ba186", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f294aeaf-858a-4be6-830c-35d3cce03918", "node_type": "1", "metadata": {"window": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. ", "original_text": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n"}, "hash": "984168bcfd98e31a1f694772e89f8366bb9a5f8a906efe89b1292922b7071111", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This purely randomized approach offers significant computational efficiency. ", "mimetype": "text/plain", "start_char_idx": 2778, "end_char_idx": 2855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f294aeaf-858a-4be6-830c-35d3cce03918", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. ", "original_text": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "691436f1-c388-4ad6-bc92-c12751d29dd5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The diversity of these applications has led to a wide spectrum of algorithmic approaches, primarily categorized into distance-based methods, which assume anomalies are far from their nearest neighbors; density-based methods, which assume that anomalies occur in low-density regions; and model-based methods, which identify deviations from a learned normal profile.\n\n Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. ", "original_text": "This purely randomized approach offers significant computational efficiency. "}, "hash": "9dff0796fc5daae12170d1451b79188647f9b5c5bdcf9a2626dd95dadd245ec3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bf213fb-cb4f-4579-905a-1d0a3c01b861", "node_type": "1", "metadata": {"window": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n", "original_text": "Several variants have been proposed to address these limitations. "}, "hash": "7b4753772aa7c9d3f00e86aa361f767eab265a8a54b5c18f4d5b6618342fbe30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n", "mimetype": "text/plain", "start_char_idx": 2855, "end_char_idx": 3068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bf213fb-cb4f-4579-905a-1d0a3c01b861", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n", "original_text": "Several variants have been proposed to address these limitations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f294aeaf-858a-4be6-830c-35d3cce03918", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Among these, isolation-based methods have been recognized as a highly effective category, particularly for high-dimensional data.  One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. ", "original_text": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n"}, "hash": "0814bdea59d865afc55531e2587864645f105827037556aef3ddadb67a069773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9e2efeb-1a92-4cd0-92b2-78257181aa10", "node_type": "1", "metadata": {"window": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. ", "original_text": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. "}, "hash": "93220c5988ea9e85e9607415d85d51cc73adcc9d6fe86c666cace4e333372945", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Several variants have been proposed to address these limitations. ", "mimetype": "text/plain", "start_char_idx": 3068, "end_char_idx": 3134, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9e2efeb-1a92-4cd0-92b2-78257181aa10", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. ", "original_text": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bf213fb-cb4f-4579-905a-1d0a3c01b861", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "One of the representative algorithms in this class is the Isolation Forest (IF) [4], which operates on the principle that anomalies are \"few and different.\"  IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n", "original_text": "Several variants have been proposed to address these limitations. "}, "hash": "27804afafd0a152c8254b3cc4795edf187cac25e802b5f74729189e9f2b610ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d4cb2aa-94c1-42e9-b4ed-98cd74306c50", "node_type": "1", "metadata": {"window": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. ", "original_text": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. "}, "hash": "0c9c6eead265f1fe4e3f98cc1fa637efc558eeae9c01f8b058d8f17faeef7167", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. ", "mimetype": "text/plain", "start_char_idx": 3134, "end_char_idx": 3292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d4cb2aa-94c1-42e9-b4ed-98cd74306c50", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. ", "original_text": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9e2efeb-1a92-4cd0-92b2-78257181aa10", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "IF constructs an ensemble of binary trees by recursively splitting the data using randomly selected features and split values.  Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. ", "original_text": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts. "}, "hash": "63ce01642cbb5c615faa9f481f54acb0002bd794cf65dbe06c433e793f288aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b483fece-a93b-4dae-8cd0-f0da7c92df71", "node_type": "1", "metadata": {"window": "This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. ", "original_text": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. "}, "hash": "cea7a575aea59beab9a500edc4b7e650d5368fb90be38c2fcf93249db63c6d9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. ", "mimetype": "text/plain", "start_char_idx": 3292, "end_char_idx": 3428, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b483fece-a93b-4dae-8cd0-f0da7c92df71", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. ", "original_text": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d4cb2aa-94c1-42e9-b4ed-98cd74306c50", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Because anomalies are numerically distant from the majority of data, they are susceptible to isolation closer to the root of the tree, resulting in shorter average path lengths compared to normal instances.  This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. ", "original_text": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps. "}, "hash": "75a58bf577c67638f22dd13c6ce6728de5011c910734b0e43a58ecbf96557f6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d161b1c-2779-47b3-83ec-69e381a23dbe", "node_type": "1", "metadata": {"window": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. ", "original_text": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. "}, "hash": "fe3b0cc41590aea825a962daaab25650dde5a4bf4f74aee582aab191815216c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. ", "mimetype": "text/plain", "start_char_idx": 3428, "end_char_idx": 3699, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d161b1c-2779-47b3-83ec-69e381a23dbe", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. ", "original_text": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b483fece-a93b-4dae-8cd0-f0da7c92df71", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This purely randomized approach offers significant computational efficiency.  However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. ", "original_text": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy. "}, "hash": "e335c43c53cc6925fbcd08ebd572fb11c8b67f6ac1c1c2e48b3aad819910c6ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ded047d-5061-4de9-b1d5-0a9391fed308", "node_type": "1", "metadata": {"window": "Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. ", "original_text": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n"}, "hash": "20ee536931da06363f0e34e5f98717a6376797aed71683bbbd42d30c2cc988ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. ", "mimetype": "text/plain", "start_char_idx": 3699, "end_char_idx": 3810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ded047d-5061-4de9-b1d5-0a9391fed308", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. ", "original_text": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d161b1c-2779-47b3-83ec-69e381a23dbe", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, Standard IF suffers from \"axis-parallel\" bias; because it splits data only along coordinate axes, it struggles to capture correlations between features or detect anomalies in more complex distributions.\n\n Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. ", "original_text": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy. "}, "hash": "c0cdf6cc466e0ebc18ab2dfa91734188154bf9f3ce7410eba88967b38fcd7b8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c4ff8be-c51d-40d2-9471-eecbf66e7781", "node_type": "1", "metadata": {"window": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n", "original_text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. "}, "hash": "043c318ed9821a9c491884e197b68501392bcd4a7acc609ae654c6d91f6ad6db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n", "mimetype": "text/plain", "start_char_idx": 3810, "end_char_idx": 4063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c4ff8be-c51d-40d2-9471-eecbf66e7781", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n", "original_text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ded047d-5061-4de9-b1d5-0a9391fed308", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Several variants have been proposed to address these limitations.  The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. ", "original_text": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n"}, "hash": "3671e912afeeb9a83c36a3de1853e8ebab3d7a684515b531e224421de7515b36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b35444df-26dd-4439-ba1f-e8eaf037205d", "node_type": "1", "metadata": {"window": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. ", "original_text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. "}, "hash": "912ada03a2bbdf9ff828e270c5518617240953f891712959788609e4e129a49f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. ", "mimetype": "text/plain", "start_char_idx": 4063, "end_char_idx": 4201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b35444df-26dd-4439-ba1f-e8eaf037205d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. ", "original_text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c4ff8be-c51d-40d2-9471-eecbf66e7781", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The Extended Isolation Forest (EIF) [1] generalizes the splitting condition by using random hyperplanes with arbitrary slopes rather than axis-parallel cuts.  This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n", "original_text": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants. "}, "hash": "15c4c78f9f7e5ffe9f927085137c96ed599c5bffa8b89337e6b72fdd35373ffd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2301a30a-8d28-4128-95dd-009c65e8434d", "node_type": "1", "metadata": {"window": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. ", "original_text": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. "}, "hash": "ab8f5d088c83297b3e698e01d44650c5dcb4e3acdd1ab45932eb3345c7fa987f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. ", "mimetype": "text/plain", "start_char_idx": 4201, "end_char_idx": 4371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2301a30a-8d28-4128-95dd-009c65e8434d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. ", "original_text": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b35444df-26dd-4439-ba1f-e8eaf037205d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This allows the algorithm to capture more complex dependencies and eliminates the \"ghost regions\" often seen in Standard IF score maps.  Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. ", "original_text": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces. "}, "hash": "2379fbb1a528f2f3f67691bb976bdcb7ba29b912ca9850130c4cc6a4dbf434f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e445e9e-1aca-4961-bb6a-3181d628bd7c", "node_type": "1", "metadata": {"window": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. ", "original_text": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. "}, "hash": "15854a6899786d7579e7baf5364bb2554e6380010999bbf069c81bf6465cee12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. ", "mimetype": "text/plain", "start_char_idx": 4371, "end_char_idx": 4563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e445e9e-1aca-4961-bb6a-3181d628bd7c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. ", "original_text": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2301a30a-8d28-4128-95dd-009c65e8434d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Building on this, the Generalized Isolation Forest (GIF) [3] further refines the splitting process to avoid \"empty branches\"\u2014a common inefficiency in EIF where random cuts may separate no data\u2014thereby improving computational speed without sacrificing detection accuracy.  More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. ", "original_text": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically. "}, "hash": "82909a6ac3abde373b12c421d645ffe490677c86540d868f7f418440a82d9906", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bcc5d9d-9351-4b3b-a8ab-3bffebd3661f", "node_type": "1", "metadata": {"window": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n", "original_text": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. "}, "hash": "7c66678e3e4abb1f77b18568948321f294c84458f383acc8c2c40e1d02fa08a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 4563, "end_char_idx": 4791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bcc5d9d-9351-4b3b-a8ab-3bffebd3661f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n", "original_text": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e445e9e-1aca-4961-bb6a-3181d628bd7c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "More recently, the K-Means Isolation Forest (K-Means IF) [2] introduced a density-aware partitioning strategy.  Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. ", "original_text": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF. "}, "hash": "4dbcd75146c851171baf7dd7b39a0198052a8dd79da52b717540eb2e32818bf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4b5bc30-52f8-46da-aa62-d4ab186e9f81", "node_type": "1", "metadata": {"window": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n", "original_text": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n"}, "hash": "9994dc579478edc6ebd1e3d4d64e8c40175f888cd94b41ebebe4613c913407a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. ", "mimetype": "text/plain", "start_char_idx": 4791, "end_char_idx": 4993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4b5bc30-52f8-46da-aa62-d4ab186e9f81", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n", "original_text": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bcc5d9d-9351-4b3b-a8ab-3bffebd3661f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Instead of purely random splits, K-Means IF utilizes K-Means clustering at each tree node to define branches, allowing the tree structure to adapt naturally to the local data density and producing a multi-branch tree rather than a strictly binary one.\n\n In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n", "original_text": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail. "}, "hash": "8d9dcdbd95a97c6c9e9d7f92709aa9c13dc056e0cbca2272da2bdaabf1366dbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f8e6831-d805-4c7f-8ab0-5b791f7aec11", "node_type": "1", "metadata": {"window": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. ", "original_text": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. "}, "hash": "464fd9023a5df6814499f529d3f0cb036b7be03b42b03795e51232fbf4ba2c74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n", "mimetype": "text/plain", "start_char_idx": 4993, "end_char_idx": 5200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f8e6831-d805-4c7f-8ab0-5b791f7aec11", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. ", "original_text": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4b5bc30-52f8-46da-aa62-d4ab186e9f81", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this paper, we conduct a comprehensive comparative study of efficiency and robustness across these existing isolation forest variants.  Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n", "original_text": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n"}, "hash": "653920a20f1888dfc0f36794677db52b6e3662a8bcc1e18fa153e49c797c1e82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "091595c0-9847-4b90-a4ad-47517230564a", "node_type": "1", "metadata": {"window": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees. ", "original_text": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. "}, "hash": "98e27886cfd4d9ce46d313e44ac044fe849e683069a3cb694e783db0b604e43e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. ", "mimetype": "text/plain", "start_char_idx": 5200, "end_char_idx": 5370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "091595c0-9847-4b90-a4ad-47517230564a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees. ", "original_text": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f8e6831-d805-4c7f-8ab0-5b791f7aec11", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, we introduce two novel hybrid algorithms that extend the density-aware partitioning of K-Means IF to address its own limitations in high-dimensional spaces.  The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. ", "original_text": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms. "}, "hash": "6e8ad17bb674288cfc21d2ad9d78e37e1005a5ce752328d092202dd934b08a39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bae0cb65-623f-42a5-9b61-c72e1947f7f7", "node_type": "1", "metadata": {"window": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest. ", "original_text": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. "}, "hash": "9d47e02a472e7fbbe6b3a9cc5e63f8edda54564d68a993fa849c9c6109d913ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 5370, "end_char_idx": 5458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bae0cb65-623f-42a5-9b61-c72e1947f7f7", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest. ", "original_text": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "091595c0-9847-4b90-a4ad-47517230564a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The first approach is Subspace K-Means IF, which projects data into random axis-parallel subspaces before clustering, allowing the algorithm to focus on different feature subsets dynamically.  The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees. ", "original_text": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF. "}, "hash": "6b1a27ff716184979e5b8576252cb91733719baadc173d977799c5ad8ec63531", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5aeadc30-0e5b-4b87-89eb-cfdf5b50a88f", "node_type": "1", "metadata": {"window": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. ", "original_text": "Finally, Section 5 offers concluding remarks.\n\n"}, "hash": "188a88bf4bf0396a3008e39673981ec753d7c5e2a0467a0e85a0dc7d9dc783a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. ", "mimetype": "text/plain", "start_char_idx": 5458, "end_char_idx": 5639, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5aeadc30-0e5b-4b87-89eb-cfdf5b50a88f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. ", "original_text": "Finally, Section 5 offers concluding remarks.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bae0cb65-623f-42a5-9b61-c72e1947f7f7", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The second approach is Extended K-Means Isolation Forest (EKM-IF), which projects data onto random oblique hyperplanes prior to clustering, combining the geometric flexibility of EIF with the density adaptability of K-Means IF.  While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest. ", "original_text": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions. "}, "hash": "83e186f1a7add7b93177a76c6155e61ab1c4552ef839168291157dd0ec484763", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0311c22-faa5-4efc-8968-d79b798dff0d", "node_type": "1", "metadata": {"window": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection. ", "original_text": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n"}, "hash": "8bb673a77f6052d78f94aa69bcb15917afbc6c0c1eb357fd10404a3aab93d99a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, Section 5 offers concluding remarks.\n\n", "mimetype": "text/plain", "start_char_idx": 5639, "end_char_idx": 5686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0311c22-faa5-4efc-8968-d79b798dff0d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection. ", "original_text": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5aeadc30-0e5b-4b87-89eb-cfdf5b50a88f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "While these hybrid methods incur higher training and inference costs due to the complexity of projections and clustering, they aim to offer distinct robustness in scenarios where standard methods fail.  We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. ", "original_text": "Finally, Section 5 offers concluding remarks.\n\n"}, "hash": "515b99089965cbec4ca1f62f9ad9f125246af0ad4a501b92900a97da28b9642f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2480576b-0d5b-4c4d-934c-7b5469139ed1", "node_type": "1", "metadata": {"window": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. ", "original_text": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. "}, "hash": "3a1c6ea3352cda187393a140ad5fccc83c67f675673817d2c663623edd52ef29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n", "mimetype": "text/plain", "start_char_idx": 5686, "end_char_idx": 5885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2480576b-0d5b-4c4d-934c-7b5469139ed1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. ", "original_text": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0311c22-faa5-4efc-8968-d79b798dff0d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We emphasize that there is no single \"winner\" algorithm; rather, our objective is to present the specific trade-offs between various isolation forest algorithms in terms of computational cost and accuracy.\n\n The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection. ", "original_text": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n"}, "hash": "1b8cf3c8389c261a1ed669313872082d6de2e777409a08228ea3db695ee7fc0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf6eee83-6c57-4c93-a9dc-82f8c8dd551d", "node_type": "1", "metadata": {"window": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. ", "original_text": "Fundamentally, an isolation forest functions as an ensemble of isolation trees. "}, "hash": "031fc433d8c9fa7cb4b430b5d0e7a70a6eecc70195f0f69d7bd1d69e390ec743", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. ", "mimetype": "text/plain", "start_char_idx": 5885, "end_char_idx": 6040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf6eee83-6c57-4c93-a9dc-82f8c8dd551d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. ", "original_text": "Fundamentally, an isolation forest functions as an ensemble of isolation trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2480576b-0d5b-4c4d-934c-7b5469139ed1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The remainder of this paper is organized as follows: Section 2 recalls the K-Means clustering algorithm and details the Standard IF, EIF, GIF, and K-Means IF algorithms.  Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. ", "original_text": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points. "}, "hash": "42a7c9c78da0d7b2611fe1fca7f8e6d68ca9c00df23030c5e138129ea497aa7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3001a5c3-14fa-43ea-8e5f-63e06ce33382", "node_type": "1", "metadata": {"window": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. ", "original_text": "Let t denote the number of trees in the forest. "}, "hash": "4ba8037bbc9f12a8ab2720bd33d90937bb7a81da0e91ac6ce937c068cdaec779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fundamentally, an isolation forest functions as an ensemble of isolation trees. ", "mimetype": "text/plain", "start_char_idx": 6040, "end_char_idx": 6120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3001a5c3-14fa-43ea-8e5f-63e06ce33382", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. ", "original_text": "Let t denote the number of trees in the forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf6eee83-6c57-4c93-a9dc-82f8c8dd551d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Section 3 introduces our novel variations: Subspace K-Means IF and Extended K-Means IF.  Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. ", "original_text": "Fundamentally, an isolation forest functions as an ensemble of isolation trees. "}, "hash": "640e561d0eee56356754a2ed038d7a0c145d28a77e2d44664a1721dda34b12e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a0566dd-16a9-44ec-8ccd-137d12c4f2c8", "node_type": "1", "metadata": {"window": "Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. ", "original_text": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. "}, "hash": "ae2e5490a61e3f89c56237b4a35f8c1b987fdf997341c8ebe776e2436f0d56fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let t denote the number of trees in the forest. ", "mimetype": "text/plain", "start_char_idx": 6120, "end_char_idx": 6168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1a0566dd-16a9-44ec-8ccd-137d12c4f2c8", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. ", "original_text": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3001a5c3-14fa-43ea-8e5f-63e06ce33382", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Section 4 presents the experiments and results, evaluating all six methods on 13 benchmark datasets, as well as on a sum of synthetic datasets generated from various distributions.  Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. ", "original_text": "Let t denote the number of trees in the forest. "}, "hash": "c09062eb8236d81c216ef0991e0fa27c6014cdb96f64fc1d24769f54ac36290b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "919f3fb8-8b3a-4fb4-9e56-b5ee4bbbd395", "node_type": "1", "metadata": {"window": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n", "original_text": "First, a component q is randomly selected for projection. "}, "hash": "3fbda333726f237da5145b09a956f4e443c69a677ea22a3e9fa9f4e5eb90876d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. ", "mimetype": "text/plain", "start_char_idx": 6168, "end_char_idx": 6381, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "919f3fb8-8b3a-4fb4-9e56-b5ee4bbbd395", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n", "original_text": "First, a component q is randomly selected for projection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a0566dd-16a9-44ec-8ccd-137d12c4f2c8", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Finally, Section 5 offers concluding remarks.\n\n ## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. ", "original_text": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively. "}, "hash": "af5ffce4abffca84d5ee8b5258c707fd75d1e74c9d8665e1105d84579e2eafd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f917748-9e65-4277-a966-3220742d1a65", "node_type": "1", "metadata": {"window": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. ", "original_text": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. "}, "hash": "46401d9846c5462c15fc8b6b2c4569576d6021cda4ffa9ff7f936a257d78c15b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, a component q is randomly selected for projection. ", "mimetype": "text/plain", "start_char_idx": 6381, "end_char_idx": 6439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f917748-9e65-4277-a966-3220742d1a65", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. ", "original_text": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "919f3fb8-8b3a-4fb4-9e56-b5ee4bbbd395", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## 2 Technical description of the problem\n\nIn this section, we review the Standard Isolation Forest (IF) algorithm and some of its variants: Extended IF (EIF), Generalized IF (GIF), and K-Means IF.\n\n ### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n", "original_text": "First, a component q is randomly selected for projection. "}, "hash": "7c8d757a86b84cbd85bae8b2aca3cc8562548d5b53ef3d7ef937db015cdaa5a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0b528c0-5053-4cc3-b7c0-9c7f879bcfa7", "node_type": "1", "metadata": {"window": "Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node. ", "original_text": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. "}, "hash": "559cecde7396972502980fc5f46f3a7f96abe871857460c7a80fe30152f2836c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. ", "mimetype": "text/plain", "start_char_idx": 6439, "end_char_idx": 6585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0b528c0-5053-4cc3-b7c0-9c7f879bcfa7", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node. ", "original_text": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f917748-9e65-4277-a966-3220742d1a65", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.1 Isolation Forest\n\nThe core premise of the Isolation Forest algorithm is that anomalous data points are more prone to isolation than normal points.  Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. ", "original_text": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range. "}, "hash": "d2aa133d28af254ed5bd25b5cfe83a17e9321052d02d8e3a7da9400988433b89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97700525-ae74-406b-8791-92070f21fd05", "node_type": "1", "metadata": {"window": "Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. ", "original_text": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. "}, "hash": "45489c508b62b54e1cc6de5447bd5031b5881bd17840ec3be9e237c6f82cde44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. ", "mimetype": "text/plain", "start_char_idx": 6585, "end_char_idx": 6698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97700525-ae74-406b-8791-92070f21fd05", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. ", "original_text": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0b528c0-5053-4cc3-b7c0-9c7f879bcfa7", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Fundamentally, an isolation forest functions as an ensemble of isolation trees.  Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node. ", "original_text": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)]. "}, "hash": "8c6ce463da681f4f0abe135c111e37093ca035900ab676d4cf6cab2677741136", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "550d61bf-f13a-4a15-9f2b-5897f463defb", "node_type": "1", "metadata": {"window": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. ", "original_text": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. "}, "hash": "98697fa81564546b14e2dbdec4a6a086462fee0c8f8afc9b9c5cbeed0847502d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. ", "mimetype": "text/plain", "start_char_idx": 6698, "end_char_idx": 6915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "550d61bf-f13a-4a15-9f2b-5897f463defb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. ", "original_text": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97700525-ae74-406b-8791-92070f21fd05", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Let t denote the number of trees in the forest.  Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. ", "original_text": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}. "}, "hash": "48ab43bd2e95cc5918032f84ee32d682f1c68841e9443965141bce02540496c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3857df04-fbfe-4fae-9dc5-221211cbb7d8", "node_type": "1", "metadata": {"window": "First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points. ", "original_text": "This procedure is repeated t times to generate the ensemble of trees.\n\n"}, "hash": "96a06506d7af501d0ccf2507e8673f9b0530d57c605ef9148d5366cd3dd045ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. ", "mimetype": "text/plain", "start_char_idx": 6915, "end_char_idx": 7062, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3857df04-fbfe-4fae-9dc5-221211cbb7d8", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points. ", "original_text": "This procedure is repeated t times to generate the ensemble of trees.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "550d61bf-f13a-4a15-9f2b-5897f463defb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given a dataset X = {X1, X2, ..., Xn}, where X \u2282 R^d and each point Xi = [x_i^(1), x_i^(2), ..., x_i^(d)], for i = 1, ..., n, is a d-dimensional vector, the construction of an isolation tree proceeds recursively.  First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. ", "original_text": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point. "}, "hash": "8e97a7f9f6e9dec609aaac945de649b671f264e7518ccb0156b4702b4684f977", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4abb1dfb-81ae-4d23-826e-fd2a6f543f19", "node_type": "1", "metadata": {"window": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated. ", "original_text": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. "}, "hash": "c3536f7c5e3186359f04143e7e058d8808ce86d168158132e5ae7659ed4aa501", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This procedure is repeated t times to generate the ensemble of trees.\n\n", "mimetype": "text/plain", "start_char_idx": 7062, "end_char_idx": 7133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4abb1dfb-81ae-4d23-826e-fd2a6f543f19", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated. ", "original_text": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3857df04-fbfe-4fae-9dc5-221211cbb7d8", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "First, a component q is randomly selected for projection.  Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points. ", "original_text": "This procedure is repeated t times to generate the ensemble of trees.\n\n"}, "hash": "d69aa9d3d57b00729ed238f2e06f1d9ae88f9d90418aa01e6f8cea0d7c1fc5f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "431aa448-4f1d-426a-b1e5-42c3aca68285", "node_type": "1", "metadata": {"window": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. ", "original_text": "This is done by traversing the tree from the root, applying the split criteria at each node. "}, "hash": "27d024d992299fa43e52c637089b6e1177f4b3b3b93575eadc3b8ca32a1e05aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. ", "mimetype": "text/plain", "start_char_idx": 7133, "end_char_idx": 7231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "431aa448-4f1d-426a-b1e5-42c3aca68285", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. ", "original_text": "This is done by traversing the tree from the root, applying the split criteria at each node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4abb1dfb-81ae-4d23-826e-fd2a6f543f19", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Next, the minimum and maximum values of the points along this dimension are identified, and a split value is uniformly sampled within this range.  Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated. ", "original_text": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches. "}, "hash": "6079177e4227f7f49e05b430d555314fbf2be7e7722a8d72eadb3ec7b22cfa22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58bf8120-65d9-4796-9297-1869217845c4", "node_type": "1", "metadata": {"window": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. ", "original_text": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. "}, "hash": "5d3dab080f8a96ff3d001b9bafd4eab1f3c7581ce1ac50825663a1cbedf61488", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is done by traversing the tree from the root, applying the split criteria at each node. ", "mimetype": "text/plain", "start_char_idx": 7231, "end_char_idx": 7324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58bf8120-65d9-4796-9297-1869217845c4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. ", "original_text": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "431aa448-4f1d-426a-b1e5-42c3aca68285", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Specifically, a value p is chosen randomly from the interval [min_{i=1,...,n} x_i^(q), max_{i=1,...,n} x_i^(q)].  This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. ", "original_text": "This is done by traversing the tree from the root, applying the split criteria at each node. "}, "hash": "3bf7066d44f0b3f29526c264d0c1d3c8ac278e02bad34f4a7cef055bcfc1f1db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36ef7003-8f7c-4443-af41-1aa33f6e537b", "node_type": "1", "metadata": {"window": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. ", "original_text": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. "}, "hash": "6961ce2f00641430bac161ed5a14f2b74545c6a374d90531bdbfb2be6c2d515f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. ", "mimetype": "text/plain", "start_char_idx": 7324, "end_char_idx": 7443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36ef7003-8f7c-4443-af41-1aa33f6e537b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. ", "original_text": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58bf8120-65d9-4796-9297-1869217845c4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This split creates two branches: one containing points where the k-th component is less than or equal to p, formally {xi | x_i^(q) \u2264 p}, and the other containing points where it is greater than p, {xi | x_i^(q) > p}.  This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. ", "original_text": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates. "}, "hash": "09bc69b93027528ba3f2bb11b283c2e1a99d65df1fb354c7d2bca5850f95b671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07b5f8e9-9adc-401d-a262-434d1d600738", "node_type": "1", "metadata": {"window": "This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. ", "original_text": "In this scenario, leaf nodes may contain one or multiple points. "}, "hash": "26dd444fb88b6f214dc30daff4c1cf44d523b72a6c6c1b4682a539870afcd8f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. ", "mimetype": "text/plain", "start_char_idx": 7443, "end_char_idx": 7629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07b5f8e9-9adc-401d-a262-434d1d600738", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. ", "original_text": "In this scenario, leaf nodes may contain one or multiple points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36ef7003-8f7c-4443-af41-1aa33f6e537b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This process repeats recursively on the new nodes until either a pre-defined maximum tree depth is reached or a node contains a single data point.  This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. ", "original_text": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed. "}, "hash": "e4db5bb7ae7ed33b8f130b8f97b143d2b32a0da81fd535f564383713f4fc8cf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f18c47bd-ed48-4995-b97a-71d521af1658", "node_type": "1", "metadata": {"window": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. ", "original_text": "For nodes with multiple points, the depth of a specific point is approximated. "}, "hash": "fc886bdc1dcd0377dccb892eeb8759d79743b66a55889e1d34e2ca8050985d36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this scenario, leaf nodes may contain one or multiple points. ", "mimetype": "text/plain", "start_char_idx": 7629, "end_char_idx": 7694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f18c47bd-ed48-4995-b97a-71d521af1658", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. ", "original_text": "For nodes with multiple points, the depth of a specific point is approximated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07b5f8e9-9adc-401d-a262-434d1d600738", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This procedure is repeated t times to generate the ensemble of trees.\n\n To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. ", "original_text": "In this scenario, leaf nodes may contain one or multiple points. "}, "hash": "5e2756787263985e6eaf3055cf2c9cccc547288f311c1e3e26583efbca3aef10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1248b3df-521f-4b12-aa1b-8423358d44d0", "node_type": "1", "metadata": {"window": "This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n", "original_text": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. "}, "hash": "edd1c72aebca36604f9e9580c5b7bd711bb12ab42dc8cf0b1f99c3f7dc2af4cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For nodes with multiple points, the depth of a specific point is approximated. ", "mimetype": "text/plain", "start_char_idx": 7694, "end_char_idx": 7773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1248b3df-521f-4b12-aa1b-8423358d44d0", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n", "original_text": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f18c47bd-ed48-4995-b97a-71d521af1658", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To determine the anomaly score of a data point, we compute the depth of the leaf node it reaches.  This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. ", "original_text": "For nodes with multiple points, the depth of a specific point is approximated. "}, "hash": "533a407d5997d9fe1894eb5105d9f6076a11a0df45276037c30b4b36bbaa9b54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad604007-7f5d-4071-a143-7624167551be", "node_type": "1", "metadata": {"window": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive. ", "original_text": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. "}, "hash": "a3f6ccf01cc422e0473ca0afef2146ee63b067528af24c67f9036ee55aac6c94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. ", "mimetype": "text/plain", "start_char_idx": 7773, "end_char_idx": 8088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad604007-7f5d-4071-a143-7624167551be", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive. ", "original_text": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1248b3df-521f-4b12-aa1b-8423358d44d0", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This is done by traversing the tree from the root, applying the split criteria at each node.  Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n", "original_text": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant. "}, "hash": "c65a48ba539fcfbf9993f75be72c3b8a8a262badda2c24e3934ffe39d9b0a4a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "313c26dd-2dc7-49a4-95eb-dfa72929216b", "node_type": "1", "metadata": {"window": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al. ", "original_text": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. "}, "hash": "a647120a58c29195e429a07a99fe316524b1b32e5f2688c02b6043860fd99ce9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. ", "mimetype": "text/plain", "start_char_idx": 8088, "end_char_idx": 8250, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "313c26dd-2dc7-49a4-95eb-dfa72929216b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al. ", "original_text": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad604007-7f5d-4071-a143-7624167551be", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Consequently, the anomaly score provided by a single tree is directly tied to the depth at which the point terminates.  Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive. ", "original_text": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly. "}, "hash": "344995368bfb1e3da444294afd3d0dab416433a7ec274697a120d6d8a54b6d7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "995b67a8-842c-4387-9472-ebfb98ef1bb3", "node_type": "1", "metadata": {"window": "In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling. ", "original_text": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. "}, "hash": "47a647f4ac2ddedc8980afa2f166f233d97d27a5bf1a41897ab9b88eb3d29d00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. ", "mimetype": "text/plain", "start_char_idx": 8250, "end_char_idx": 8359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "995b67a8-842c-4387-9472-ebfb98ef1bb3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling. ", "original_text": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "313c26dd-2dc7-49a4-95eb-dfa72929216b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since normal points typically reside deep in the tree while anomalies are isolated near the root, full tree construction is often unnecessary; instead, a maximum depth limit is imposed.  In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al. ", "original_text": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal. "}, "hash": "46a841409b50476d576107ea87418879cdc2e2b048b6e1eb0b992aea41dc8786", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2c21bdd-0290-458f-85ec-10fe38242dd1", "node_type": "1", "metadata": {"window": "For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches. ", "original_text": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. "}, "hash": "f0b076bf6544f2b3954db3d922535e3f7ddceb6eed77cd729497785388344d6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. ", "mimetype": "text/plain", "start_char_idx": 8359, "end_char_idx": 8462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2c21bdd-0290-458f-85ec-10fe38242dd1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches. ", "original_text": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "995b67a8-842c-4387-9472-ebfb98ef1bb3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this scenario, leaf nodes may contain one or multiple points.  For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling. ", "original_text": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly. "}, "hash": "49f8c6bb050477168a719acd131376d3536fb685d4c45908e94b34e643bb7939", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bde706bc-e371-458c-80b4-8bdf28182c05", "node_type": "1", "metadata": {"window": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al. ", "original_text": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n"}, "hash": "5afa9fda3820cae1dafa0f731eb5ecdfb8233eb3dae5d7f893c54bdb8f25875c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. ", "mimetype": "text/plain", "start_char_idx": 8462, "end_char_idx": 8552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bde706bc-e371-458c-80b4-8bdf28182c05", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al. ", "original_text": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c21bdd-0290-458f-85ec-10fe38242dd1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For nodes with multiple points, the depth of a specific point is approximated.  The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches. ", "original_text": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers. "}, "hash": "65c24134fd26d96b78d1538c65d63b30c6684605455824b689ebd45f42d019fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63558a86-2633-4776-8ede-419abe66e2fc", "node_type": "1", "metadata": {"window": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. ", "original_text": "Given that datasets can be large, trees can become computationally expensive. "}, "hash": "398c0fdb49da5c167fd5c938e475909947383640490522335c0cca588331b7db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n", "mimetype": "text/plain", "start_char_idx": 8552, "end_char_idx": 8793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63558a86-2633-4776-8ede-419abe66e2fc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. ", "original_text": "Given that datasets can be large, trees can become computationally expensive. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bde706bc-e371-458c-80b4-8bdf28182c05", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The outlier score is defined by the following formula:\n\ns(x, n) = 2^(-E[h(x)]/c(n)) (1)\n\nwhere:\n\nc(n) = 2 \u00b7 H(n \u2212 1) \u2212 2\u00b7(n\u22121)/n (2)\n\nHere, c(n) represents the average path length h(x) for a dataset of size n, and H(n) \u2248 ln(n) + \u03b3 is the n-th harmonic number, with \u03b3 \u2248 0.577215 being the Euler-Mascheroni constant.  Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al. ", "original_text": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n"}, "hash": "af722a0b3d3098780a47210c85b1f700c2db6b549dcb007adc2cae7e244c1bf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9073de3b-7fee-4b82-9863-e7d35a66ccc5", "node_type": "1", "metadata": {"window": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n", "original_text": "Liu et al. "}, "hash": "a16c4294cb0b6d862c6250ea9e7d8e070529f544ce3a8cd45498d4b4423a3552", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given that datasets can be large, trees can become computationally expensive. ", "mimetype": "text/plain", "start_char_idx": 8793, "end_char_idx": 8871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9073de3b-7fee-4b82-9863-e7d35a66ccc5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n", "original_text": "Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63558a86-2633-4776-8ede-419abe66e2fc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Analyzing the formula, if E[h(x)] \u2248 c(n), the average depth of a point equals the average path length, yielding a score of 0.5, which indicates no clear anomaly.  If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. ", "original_text": "Given that datasets can be large, trees can become computationally expensive. "}, "hash": "a2c9d6fe838458c60e4772eff4fb54e30b8c32f461f5ed6faf04ee04f1f41e50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d73ce5d9-b1df-43e6-8d4a-d16177c7ba01", "node_type": "1", "metadata": {"window": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. ", "original_text": "demonstrated in [4] that isolation forests perform efficiently with subsampling. "}, "hash": "bcb1db81969d15d1d714ee53924b902832a5a85908ba872eda6ad69d65666c00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu et al. ", "mimetype": "text/plain", "start_char_idx": 8871, "end_char_idx": 8882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d73ce5d9-b1df-43e6-8d4a-d16177c7ba01", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. ", "original_text": "demonstrated in [4] that isolation forests perform efficiently with subsampling. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9073de3b-7fee-4b82-9863-e7d35a66ccc5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If E[h(x)] is significantly larger than c(n), the score approaches 0, suggesting the point is likely normal.  Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n", "original_text": "Liu et al. "}, "hash": "93053d9e25367608f4d1d65e11951fdd5c72614228635ff1db22cfe25a12e0dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a32f8df9-47fe-48dc-be72-2182791787e7", "node_type": "1", "metadata": {"window": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. ", "original_text": "Rather than using the entire dataset, trees can be constructed from smaller, random batches. "}, "hash": "4629b5ed6088369109ec24a2a97b0eafa22a2cbbebfe4a3d9e1b1dd2f01af286", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "demonstrated in [4] that isolation forests perform efficiently with subsampling. ", "mimetype": "text/plain", "start_char_idx": 8882, "end_char_idx": 8963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a32f8df9-47fe-48dc-be72-2182791787e7", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. ", "original_text": "Rather than using the entire dataset, trees can be constructed from smaller, random batches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d73ce5d9-b1df-43e6-8d4a-d16177c7ba01", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Conversely, if E[h(x)] is much smaller than c(n), the score approaches 1, indicating a likely anomaly.  Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. ", "original_text": "demonstrated in [4] that isolation forests perform efficiently with subsampling. "}, "hash": "0b8c80ae1c9c83233ec2988ed763349e567b2b7941cf75ab019cd67f83936ee2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab648c1f-4505-492e-91d0-37475815af1f", "node_type": "1", "metadata": {"window": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. ", "original_text": "Table 1 presents the optimal hyperparameters identified by Liu et al. "}, "hash": "e441a74d2a8d2afd497545f02410af80d0d50e76c395e5d56367019e0b28101a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rather than using the entire dataset, trees can be constructed from smaller, random batches. ", "mimetype": "text/plain", "start_char_idx": 8963, "end_char_idx": 9056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab648c1f-4505-492e-91d0-37475815af1f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. ", "original_text": "Table 1 presents the optimal hyperparameters identified by Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a32f8df9-47fe-48dc-be72-2182791787e7", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Thus, s(x) \u2208 [0, 1]; values near 0 suggest inliers, while values near 1 suggest outliers.  The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. ", "original_text": "Rather than using the entire dataset, trees can be constructed from smaller, random batches. "}, "hash": "06d85e077810dcedc8413d27558ee8a43767d9784910a2221464e1f6a3806b6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07a9c05d-d23a-4107-afc3-bfe901f9fd2f", "node_type": "1", "metadata": {"window": "Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" ", "original_text": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. "}, "hash": "15146cb42f0cf45811a539abcef6b5abb36edfb2d52b790ff4f83c2542ab2522", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1 presents the optimal hyperparameters identified by Liu et al. ", "mimetype": "text/plain", "start_char_idx": 9056, "end_char_idx": 9126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07a9c05d-d23a-4107-afc3-bfe901f9fd2f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" ", "original_text": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab648c1f-4505-492e-91d0-37475815af1f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The exponent effectively compares the actual depth of the point to the expected average depth in a random tree of size n. A small ratio implies the point is isolated shallowly (anomalous), whereas a large ratio implies it is deep (normal).\n\n Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. ", "original_text": "Table 1 presents the optimal hyperparameters identified by Liu et al. "}, "hash": "e9af18459da6db2ac24a43781dcd78336f662afab58ee0743e5a02cf018e3d29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61bc9c0a-ed29-4f39-9f12-3dca328620e9", "node_type": "1", "metadata": {"window": "Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al. ", "original_text": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n"}, "hash": "1c7fee2ffa3d1fb462bd6b35732db0fa70006b87eb1d1245efbafdd62ebe23a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. ", "mimetype": "text/plain", "start_char_idx": 9126, "end_char_idx": 9814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61bc9c0a-ed29-4f39-9f12-3dca328620e9", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al. ", "original_text": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07a9c05d-d23a-4107-afc3-bfe901f9fd2f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given that datasets can be large, trees can become computationally expensive.  Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" ", "original_text": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1. "}, "hash": "17201324a1293e9f044e48416793b15f0540379ab17edcc0541e4e260d421267", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20d694c8-b5d6-430d-9034-21683071bd00", "node_type": "1", "metadata": {"window": "demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. ", "original_text": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. "}, "hash": "fd406b05dfc3293132c8cd7c6fcdb0d5245674d7ddb641e51c7db1ac7b41a986", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n", "mimetype": "text/plain", "start_char_idx": 9814, "end_char_idx": 9915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20d694c8-b5d6-430d-9034-21683071bd00", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. ", "original_text": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61bc9c0a-ed29-4f39-9f12-3dca328620e9", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Liu et al.  demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al. ", "original_text": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n"}, "hash": "dc8d7365a80037b8c6d78afcc469eb289b942f70aef3156642af1da162848770", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d1a3f0a-a675-4153-b0ab-cc08ce8a1ec3", "node_type": "1", "metadata": {"window": "Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al. ", "original_text": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. "}, "hash": "12738031f265d9e82e56f13a6411f7c6b70fdd923196d23bab1ba99eca0e9ade", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. ", "mimetype": "text/plain", "start_char_idx": 9915, "end_char_idx": 10111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d1a3f0a-a675-4153-b0ab-cc08ce8a1ec3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al. ", "original_text": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20d694c8-b5d6-430d-9034-21683071bd00", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "demonstrated in [4] that isolation forests perform efficiently with subsampling.  Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. ", "original_text": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data. "}, "hash": "a26837ef47642d8f67467e595c1ed8f1111c59d58ce07625ac6827b76087d963", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c3fc0d9-4e38-425f-9804-f1dee7667bb5", "node_type": "1", "metadata": {"window": "Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. ", "original_text": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. "}, "hash": "2826ad39275e15fc5d788514c742dfb6d54e3a64a502196a9e153defd7d712a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. ", "mimetype": "text/plain", "start_char_idx": 10111, "end_char_idx": 10268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c3fc0d9-4e38-425f-9804-f1dee7667bb5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. ", "original_text": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d1a3f0a-a675-4153-b0ab-cc08ce8a1ec3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Rather than using the entire dataset, trees can be constructed from smaller, random batches.  Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al. ", "original_text": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect. "}, "hash": "46c77a9c6969b814fb52a4e9a04b194163c3433cc2f3cc5971612ac97c6f6e55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98ffa162-5efe-4be3-98b2-032e27520a51", "node_type": "1", "metadata": {"window": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. ", "original_text": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" "}, "hash": "4fdda71dc22b2593a8dfbbd30b76037fb3f570a1ca9cb830b102f7c1fbc49013", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. ", "mimetype": "text/plain", "start_char_idx": 10268, "end_char_idx": 10423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98ffa162-5efe-4be3-98b2-032e27520a51", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. ", "original_text": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c3fc0d9-4e38-425f-9804-f1dee7667bb5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Table 1 presents the optimal hyperparameters identified by Liu et al.  in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. ", "original_text": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases. "}, "hash": "0b8deded2fe6949b508a2d8713c9aa66fd89ffcb22fd3dcedca2d7098785dcfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0780726c-b36e-403d-b0a9-3f16ad8a93ea", "node_type": "1", "metadata": {"window": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. ", "original_text": "This phenomenon, first identified by Hariri et al. "}, "hash": "cd7a0256906815bed3a3cfda1e2eae5e7a1db5f358365ac2908d3300cea8080f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" ", "mimetype": "text/plain", "start_char_idx": 10423, "end_char_idx": 10543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0780726c-b36e-403d-b0a9-3f16ad8a93ea", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. ", "original_text": "This phenomenon, first identified by Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98ffa162-5efe-4be3-98b2-032e27520a51", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [4] regarding ensemble size, subsample size, and tree depth:\n\n**Algorithm 1: Create Isolation Forest**\n**Data:** X - dataset, t - number of trees, \u03c8 - subsampling size\n**Result:** iForest - a list of ITree\n1 **Function** createIForest(X,t,\u03c8)\n2 `iForest` \u2190 \u00d8\n3 `l` \u2190 log\u2082(\u03c8)\n4 **for** i=1:t **do**\n5 X' \u2190 sample(X, \u03c8)\n6 iTree \u2190 createITree(X',l,0)\n7 `iForest` \u2190 `iForest` \u222a {iTree}\n8 **return** `iForest`\n\n| Param | Meaning | Value |\n| :--- | :--- | :--- |\n| \u03c8 | Subsampling size | 256 |\n| t | Number of trees | 100 |\n| l | Maximum depth of a tree | log\u2082(\u03c8) |\n\n**Table 1:** Selection of isolation forest hyper-parameters\n\nIn our experiments, we adopted the values specified in Table 1.  A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. ", "original_text": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\" "}, "hash": "a5b742130615419b04cd7999950b04d504ffcbd4a596d40ee8b27378189c2110", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6badf59-be64-413f-849e-0bf6adc47db9", "node_type": "1", "metadata": {"window": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. ", "original_text": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. "}, "hash": "c090a776fb4d58198fbe58c8f12b363e23b88f65063cd952095a9405118aef8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This phenomenon, first identified by Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 10543, "end_char_idx": 10594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6badf59-be64-413f-849e-0bf6adc47db9", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. ", "original_text": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0780726c-b36e-403d-b0a9-3f16ad8a93ea", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A skeletal overview of the isolation forest algorithm is provided in 1, 2, 3, as introduced in [4].\n\n ### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. ", "original_text": "This phenomenon, first identified by Hariri et al. "}, "hash": "e1404c1aa1b98559e502f77c187500921ade831b7faaa2a45af85d0b66afd5f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2f5bf14-7703-4e5f-8262-714c8fc5f921", "node_type": "1", "metadata": {"window": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. ", "original_text": "To address this shortcoming, Hariri et al. "}, "hash": "6e6b6f08e6cfef909d13ae85ad659e6de31ccd8bc5f6f04b80130a9014b053a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. ", "mimetype": "text/plain", "start_char_idx": 10594, "end_char_idx": 10708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2f5bf14-7703-4e5f-8262-714c8fc5f921", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. ", "original_text": "To address this shortcoming, Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6badf59-be64-413f-849e-0bf6adc47db9", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.2 Extended Isolation Forest\n\nAs illustrated in Figure 1, the Standard Isolation Forest generates \"ghost artifacts\"\u2014regions assigned a low anomaly score despite containing little to no data.  Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. ", "original_text": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data. "}, "hash": "a73e907d337313052de78f3aa864c91a549bc3909541e9d79a359ddeeb4580d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05f7cb31-06e9-43ec-916c-e4122cbc1056", "node_type": "1", "metadata": {"window": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. ", "original_text": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. "}, "hash": "b030490f15a83371d2e2f9463442e33da2945661667c928ea1eac0565a25eb5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To address this shortcoming, Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 10708, "end_char_idx": 10751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05f7cb31-06e9-43ec-916c-e4122cbc1056", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. ", "original_text": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2f5bf14-7703-4e5f-8262-714c8fc5f921", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, line patterns parallel to the coordinate axes are frequently observed, resulting in a data distribution approximation that is far from perfect.  For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. ", "original_text": "To address this shortcoming, Hariri et al. "}, "hash": "6b7bd2933f031f8438d5754aafbe99f0c4d1dc1dca9c1925b870c3d232dbbe75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2cf52ab-288f-4f24-90df-85df14fb4539", "node_type": "1", "metadata": {"window": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. ", "original_text": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. "}, "hash": "47a18284f0bff67fe917b1162e32c289a773e08971d0887300c875abd09e8834", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. ", "mimetype": "text/plain", "start_char_idx": 10751, "end_char_idx": 10871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2cf52ab-288f-4f24-90df-85df14fb4539", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. ", "original_text": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05f7cb31-06e9-43ec-916c-e4122cbc1056", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For instance, consider data at the edge of the distribution: in its immediate vicinity, along a diagonal direction, the anomaly score correctly increases.  However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. ", "original_text": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes. "}, "hash": "53793a4d9fd351dc00bac2cb2fe535827dce6975990df6f0908c373d688bd82b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88046485-2e72-4ed2-93f9-996f8b8bf070", "node_type": "1", "metadata": {"window": "This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. ", "original_text": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. "}, "hash": "2fc26c654f17a7b157b98a2867b43f65cd8ae07efaa4a04cd489c6c56ebe94af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. ", "mimetype": "text/plain", "start_char_idx": 10871, "end_char_idx": 11005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88046485-2e72-4ed2-93f9-996f8b8bf070", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. ", "original_text": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2cf52ab-288f-4f24-90df-85df14fb4539", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, along directions parallel to the axes, the anomaly score remains low, incorrectly indicating \"normal regions.\"  This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. ", "original_text": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082. "}, "hash": "f37c9d11cd92510163c5236bf754762e2be00aae5370ee56c29f8385687b2159", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab6aa12b-3e48-4181-8469-f6c65987b9de", "node_type": "1", "metadata": {"window": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n", "original_text": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. "}, "hash": "afe02cfba0bca9a7233cec188a2e17bb33b113a4d99b67be1ee7b2c6979174bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. ", "mimetype": "text/plain", "start_char_idx": 11005, "end_char_idx": 11138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab6aa12b-3e48-4181-8469-f6c65987b9de", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n", "original_text": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88046485-2e72-4ed2-93f9-996f8b8bf070", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This phenomenon, first identified by Hariri et al.  in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. ", "original_text": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node. "}, "hash": "bf7ed4f3e6081c7faf858c1b6c038b2b0e6965665c82217bd3e72d4fbc7325e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4da7853a-5ad3-4ce8-ab9e-dfef68fcf4df", "node_type": "1", "metadata": {"window": "To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets.", "original_text": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. "}, "hash": "38571015241b7fe8aee253c2fab9c0b2a2d57fca01ab9ac2642227da713e8d2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. ", "mimetype": "text/plain", "start_char_idx": 11138, "end_char_idx": 11290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4da7853a-5ad3-4ce8-ab9e-dfef68fcf4df", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets.", "original_text": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab6aa12b-3e48-4181-8469-f6c65987b9de", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [1], occurs because Standard IF employs orthogonal hyperplanes parallel to the system's axes to separate data.  To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n", "original_text": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch. "}, "hash": "93f781de1b2141fbd13fa971eecd566bddca7fed62142323b209dbd6c83f15da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "953f376c-39b1-4b28-9dc0-b128a5b271dd", "node_type": "1", "metadata": {"window": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n", "original_text": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. "}, "hash": "c04ddb3be3faa72561ccd2914469737d47c0c80b87f4a6c542d5542581324b82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. ", "mimetype": "text/plain", "start_char_idx": 11290, "end_char_idx": 11425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "953f376c-39b1-4b28-9dc0-b128a5b271dd", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n", "original_text": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4da7853a-5ad3-4ce8-ab9e-dfef68fcf4df", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To address this shortcoming, Hariri et al.  proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets.", "original_text": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency. "}, "hash": "91391d9f055f975e1fbd3759711f217af171952052029be93c599c6d0a6ca208", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3413216b-5f81-4716-b5f7-c3bd3e19c4c5", "node_type": "1", "metadata": {"window": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. ", "original_text": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. "}, "hash": "2e0f35b7c9a7db377918980b9cd3892ebf4ea77c61cb520a7a41bb1095f65b1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. ", "mimetype": "text/plain", "start_char_idx": 11425, "end_char_idx": 11588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3413216b-5f81-4716-b5f7-c3bd3e19c4c5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. ", "original_text": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "953f376c-39b1-4b28-9dc0-b128a5b271dd", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "proposed the Extended Isolation Forest (EIF) in [1], a variation that utilizes hyperplanes with randomly chosen slopes.  Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n", "original_text": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane. "}, "hash": "9927b63a19d5bfa5e90435ab9de9bcdb6a4e6f0d143894de5d5aebdf6a161e7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c571f5e-881d-4bda-a058-b3c8455bbbdc", "node_type": "1", "metadata": {"window": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n", "original_text": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. "}, "hash": "2000c39134397beac47ab9696adaf1dc8c9fca49c5256d60c2d7b6cc5bfadc08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. ", "mimetype": "text/plain", "start_char_idx": 11588, "end_char_idx": 11827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c571f5e-881d-4bda-a058-b3c8455bbbdc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n", "original_text": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3413216b-5f81-4716-b5f7-c3bd3e19c4c5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Formally, when building a tree node, a random vector u \u2208 R^d is sampled from N(0, I_d) and subsequently normalized to w = u / ||u||\u2082.  Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. ", "original_text": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube. "}, "hash": "c67d075947796cb5e469a65d8c084eb6af135e0f5a9c4efa72c1c3be91388e3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb61bbf6-64d5-43cf-b8b9-40031e74e23a", "node_type": "1", "metadata": {"window": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n", "original_text": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n"}, "hash": "0e0c263cef0948928eed876cec7d1a15a9134beb98f01504b7a906d3417baeee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. ", "mimetype": "text/plain", "start_char_idx": 11827, "end_char_idx": 11976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb61bbf6-64d5-43cf-b8b9-40031e74e23a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n", "original_text": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c571f5e-881d-4bda-a058-b3c8455bbbdc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Additionally, an intercept vector p \u2208 R^d is randomly chosen within the smallest hypercube enclosing all the datapoints at the node.  The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n", "original_text": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map. "}, "hash": "b69b670ca19082b77d42c5709e44efa11fd64dcf761e346f2a4b963d4e1a2103", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fc03f19-e73a-42ab-b904-1d937361cb32", "node_type": "1", "metadata": {"window": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. ", "original_text": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets."}, "hash": "a2395849faf8e8953d7a1833dcf699afe48ec8f4c96b2bb3e5ae2c0877dfd189", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n", "mimetype": "text/plain", "start_char_idx": 11976, "end_char_idx": 12042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fc03f19-e73a-42ab-b904-1d937361cb32", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. ", "original_text": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb61bbf6-64d5-43cf-b8b9-40031e74e23a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The separation is then determined by (x \u2212 p)^T \u00b7 w > 0 for points going to the right branch, and (x \u2212 p)^T \u00b7 w \u2264 0 for points going to the left branch.  A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n", "original_text": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n"}, "hash": "12b8721d9efe9ff11bed83f3a8d5f4a23e8488ffa92bfc4240cd5857cd497174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10441bc7-1151-4921-9e37-5dd68f7f389a", "node_type": "1", "metadata": {"window": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n", "original_text": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n"}, "hash": "8a7e39cc97b9ed3e98bc8336ec83b8da7a6e9d9238d75fe2ff9f2c15d7a33e6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets.", "mimetype": "text/plain", "start_char_idx": 12042, "end_char_idx": 12134, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10441bc7-1151-4921-9e37-5dd68f7f389a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n", "original_text": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fc03f19-e73a-42ab-b904-1d937361cb32", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A limitation of this approach is that it can generate branches leading to nodes with no data, resulting in computational inefficiency.  This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. ", "original_text": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets."}, "hash": "9384ebd3dc3cdd9e3c59dda59827e82f39c5c7b8225914f0ff582c5b130ef45f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "830702e2-c76e-49bc-b332-a3ba2c4e9179", "node_type": "1", "metadata": {"window": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    ", "original_text": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. "}, "hash": "746d1a381a1f5cec483a11ba6dbf6640f7d1acfdb7eed76ccde9e3fc63666b90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n", "mimetype": "text/plain", "start_char_idx": 12134, "end_char_idx": 12264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "830702e2-c76e-49bc-b332-a3ba2c4e9179", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    ", "original_text": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10441bc7-1151-4921-9e37-5dd68f7f389a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This occurs because the intercept vector is chosen within the enclosing hypercube, which does not guarantee the existence of data on both sides of the hyperplane.  As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n", "original_text": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n"}, "hash": "ed2c46540cd0287599f759f387d7d8da9ca4e2d8c501d0147144619759b3d833", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4166eac-91e5-44ce-b63a-fad969d7deda", "node_type": "1", "metadata": {"window": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    ", "original_text": "They represent different geometric distributions of normal data points.\n"}, "hash": "2973cdfb2ec172feb7becf3e52409ae7c0168fcd6966d0ccd47d59cf04f491e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. ", "mimetype": "text/plain", "start_char_idx": 12264, "end_char_idx": 12444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4166eac-91e5-44ce-b63a-fad969d7deda", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    ", "original_text": "They represent different geometric distributions of normal data points.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "830702e2-c76e-49bc-b332-a3ba2c4e9179", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As the tree grows deeper and the number of points per node decreases, the probability of generating empty nodes increases significantly, related to the ratio between the volume of the convex hull and the volume of the enclosing hypercube.  Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    ", "original_text": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`. "}, "hash": "bc1b183ab48aba073c384b3b3153e065a1ac3b61bc7b7006da0c229f3d673c92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f201c98d-8e57-4bf5-902d-22e3f5555c6d", "node_type": "1", "metadata": {"window": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. ", "original_text": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n"}, "hash": "4b06bc0f2a0a858bff26cd793d1e9b88bed5c6a844b14629473f618d36cc3ba5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They represent different geometric distributions of normal data points.\n", "mimetype": "text/plain", "start_char_idx": 12444, "end_char_idx": 12516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f201c98d-8e57-4bf5-902d-22e3f5555c6d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. ", "original_text": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4166eac-91e5-44ce-b63a-fad969d7deda", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Nevertheless, Figure 1 illustrates how EIF effectively mitigates \"ghost artifacts\" and eliminates axis-parallel patterns from the anomaly score map.  A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    ", "original_text": "They represent different geometric distributions of normal data points.\n"}, "hash": "c332b169369d9575f0f53f66d0c403f0acecfec7a53dd90de396bd60454298e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d20c164-16b7-4bea-9124-44989772b9ee", "node_type": "1", "metadata": {"window": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n", "original_text": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. "}, "hash": "93d5eb7a3a5a1c96c7c0f09cecfb3eca576bf074ed4145efdb7b75fde648e5a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n", "mimetype": "text/plain", "start_char_idx": 12516, "end_char_idx": 12681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d20c164-16b7-4bea-9124-44989772b9ee", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n", "original_text": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f201c98d-8e57-4bf5-902d-22e3f5555c6d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A skeleton overview of the algorithm is provided in 1, 4, and 5.\n\n **Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. ", "original_text": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n"}, "hash": "bfcdf0e3fd396f1fe36f186b53aa421a4ec5f00c8ea6bec373e7c2bd27c595d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "550040a9-de43-4d83-a0a6-440cabf2a29d", "node_type": "1", "metadata": {"window": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. ", "original_text": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n"}, "hash": "3404b5aecd3d9124b599d4f2c8ed85eff444c5dba322336da1ab8a0a3f5067ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. ", "mimetype": "text/plain", "start_char_idx": 12681, "end_char_idx": 12783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "550040a9-de43-4d83-a0a6-440cabf2a29d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. ", "original_text": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d20c164-16b7-4bea-9124-44989772b9ee", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 1: Anomaly detection score maps across different algorithms and synthetic datasets. **\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n", "original_text": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score. "}, "hash": "c0b7e5a9cf72a6741df13320e9c35965d2d4d0faffa11fdaf6a3c1347411daa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f1cb94e-29ef-4270-bbc8-54cf75a58cb0", "node_type": "1", "metadata": {"window": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. ", "original_text": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    "}, "hash": "8b861fae00a66885ddb32d8b795e75057a13bda15582c9f1c5933d3617e0ff37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n", "mimetype": "text/plain", "start_char_idx": 12783, "end_char_idx": 12929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f1cb94e-29ef-4270-bbc8-54cf75a58cb0", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. ", "original_text": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "550040a9-de43-4d83-a0a6-440cabf2a29d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure displays a grid of heatmaps showing anomaly scores for various algorithms across six different synthetic datasets.\n - **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. ", "original_text": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n"}, "hash": "be3ab2e4f51069d4d448d03b1ea0d79ea012d3069fec65b42e1797b620829649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e57919b0-5bb3-42a6-a6db-12c04e9f6e50", "node_type": "1", "metadata": {"window": "They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al. ", "original_text": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    "}, "hash": "cebc52384529c21c3931049203ddc10392981c3cb5beebdc3307eb37fe560665", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    ", "mimetype": "text/plain", "start_char_idx": 12929, "end_char_idx": 13033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e57919b0-5bb3-42a6-a6db-12c04e9f6e50", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al. ", "original_text": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f1cb94e-29ef-4270-bbc8-54cf75a58cb0", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Rows (Datasets):** The datasets are labeled `single_blob`, `double blob`, `sinusoidal`, `spiral`, `spring-radius=3_0_climb_speed=1_5`, and `spring-radius=4_0_climb_speed=2_5`.  They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. ", "original_text": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n    "}, "hash": "f4bd9ea401fdef6297cfb4ecd2a0d710e21212c6eed191a694a845742303fc23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9b350f2-69ed-4c6a-b31b-378939dd5fd6", "node_type": "1", "metadata": {"window": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. ", "original_text": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. "}, "hash": "c5f290ae0470eb4aa2f7a3fe08046083cb971eb362c53cecc7eac0054cf2ec2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    ", "mimetype": "text/plain", "start_char_idx": 13033, "end_char_idx": 13152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a9b350f2-69ed-4c6a-b31b-378939dd5fd6", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. ", "original_text": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e57919b0-5bb3-42a6-a6db-12c04e9f6e50", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "They represent different geometric distributions of normal data points.\n - **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al. ", "original_text": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n    "}, "hash": "fdc1f552836d506aec9b3f32ee062367bde0718db14400112fbe8fe8c53978d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3675ba96-7b94-4414-93fa-c60282b26687", "node_type": "1", "metadata": {"window": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. ", "original_text": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n"}, "hash": "42f7f8bb67a4498869b157c25c8d9f8423842805033c539c3e24bd5603c2db69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. ", "mimetype": "text/plain", "start_char_idx": 13152, "end_char_idx": 13365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3675ba96-7b94-4414-93fa-c60282b26687", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. ", "original_text": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9b350f2-69ed-4c6a-b31b-378939dd5fd6", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Columns (Algorithms):** The algorithms are `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, and `Extended K-Means IF`.\n - **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. ", "original_text": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs. "}, "hash": "7415b7465c99bcc53007f33065d7bc7f226b301dcd05636b9afcee9d6691760a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7686aab4-2178-4ff2-9fec-989e05ab1d4c", "node_type": "1", "metadata": {"window": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. ", "original_text": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. "}, "hash": "2bb14cb0ef6f73009924da0494ac5dbfc0068b0fae95c5c21bd694dde2d79428", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n", "mimetype": "text/plain", "start_char_idx": 13365, "end_char_idx": 13451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7686aab4-2178-4ff2-9fec-989e05ab1d4c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. ", "original_text": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3675ba96-7b94-4414-93fa-c60282b26687", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** Each cell in the grid is a heatmap where color intensity represents the anomaly score.  Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. ", "original_text": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n"}, "hash": "ecc818e37da73f83d619a26965307ac2199193bf039dc92e7c6f684993319d01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f067c1-4aa3-4063-a41a-313cdc636d20", "node_type": "1", "metadata": {"window": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. ", "original_text": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. "}, "hash": "8e6cd2781521375c50d45b2a6326fff7282e4fdc816822f1275843a868e4aa5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. ", "mimetype": "text/plain", "start_char_idx": 13451, "end_char_idx": 13652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16f067c1-4aa3-4063-a41a-313cdc636d20", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. ", "original_text": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7686aab4-2178-4ff2-9fec-989e05ab1d4c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Yellow/light colors indicate high anomaly scores (anomalous regions), while dark blue/purple colors indicate low anomaly scores (normal regions).\n - **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. ", "original_text": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes. "}, "hash": "fc1c713fcf9820073ace595881d29f5c256fc0ff4627fa495934806239ac952c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8b835bf-a037-4a9f-b7d5-78dc1503fe1f", "node_type": "1", "metadata": {"window": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. ", "original_text": "To address this issue, Lesouple et al. "}, "hash": "950418bb80d90e7826fa4f0b96b95d66b4e1da572c71fac524ee9ba390713c2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. ", "mimetype": "text/plain", "start_char_idx": 13652, "end_char_idx": 13783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8b835bf-a037-4a9f-b7d5-78dc1503fe1f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. ", "original_text": "To address this issue, Lesouple et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16f067c1-4aa3-4063-a41a-313cdc636d20", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF` shows clear axis-parallel artifacts (rectangular patterns).\n     - `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. ", "original_text": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead. "}, "hash": "31edd192df19cb5eee2e7a7bb721e6d3c99db88ed3617c349dce042fb3833582", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db68af97-a964-4ce5-a203-26975a4c7700", "node_type": "1", "metadata": {"window": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. ", "original_text": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. "}, "hash": "600863e4dcb64d7d75011f8722d81e6d0dd091e7e784e06d0654b4408981f9c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To address this issue, Lesouple et al. ", "mimetype": "text/plain", "start_char_idx": 13783, "end_char_idx": 13822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db68af97-a964-4ce5-a203-26975a4c7700", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. ", "original_text": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8b835bf-a037-4a9f-b7d5-78dc1503fe1f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` produce smoother, more rounded score maps, mitigating the axis-parallel bias.\n     - `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. ", "original_text": "To address this issue, Lesouple et al. "}, "hash": "09a4ef6532aaa8702daaae22b96a164cf6b0b7fc444049d58b769da9f6c317e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f76c182-5e4d-4626-9dc0-df0381471227", "node_type": "1", "metadata": {"window": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n", "original_text": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. "}, "hash": "2d339765a76e2931ef9f4f56d2b9cdf62bfd2d463ae5df49b2a0408c2f4dd28e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. ", "mimetype": "text/plain", "start_char_idx": 13822, "end_char_idx": 13900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f76c182-5e4d-4626-9dc0-df0381471227", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n", "original_text": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db68af97-a964-4ce5-a203-26975a4c7700", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `K-Means IF` variants (`k=0`, `k=1`, and `Extended K-Means IF`) show score maps that more closely follow the density of the underlying data distributions, especially for complex shapes like spirals and springs.  The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. ", "original_text": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF. "}, "hash": "45a79469e63603797f64500340318317f872f75b2ec6141b2e34f25be06e4ba2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f557d6a-7959-475f-809a-cd6eeab976c4", "node_type": "1", "metadata": {"window": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. ", "original_text": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. "}, "hash": "04fea75375968ac3670a409cbf9c1d71b775fc23b8d2e2d4aaf9a758740bf1ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. ", "mimetype": "text/plain", "start_char_idx": 13900, "end_char_idx": 13999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f557d6a-7959-475f-809a-cd6eeab976c4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. ", "original_text": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f76c182-5e4d-4626-9dc0-df0381471227", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The `Extended K-Means IF` appears to provide the tightest fit to the data manifolds.\n\n ### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n", "original_text": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane. "}, "hash": "6359f35a9c223962189921d8d2842bef3e4d8d9ab158a8d96b5614a3e4caab4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31b9e1e2-190e-4bcd-9e77-8e2b7d43ae53", "node_type": "1", "metadata": {"window": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. ", "original_text": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. "}, "hash": "79fe131f33c5388dcbe247701909d056e2648a13648dbd28acc4ca5dca80e7d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. ", "mimetype": "text/plain", "start_char_idx": 13999, "end_char_idx": 14166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31b9e1e2-190e-4bcd-9e77-8e2b7d43ae53", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. ", "original_text": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f557d6a-7959-475f-809a-cd6eeab976c4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.3 Generalized Isolation Forest\n\nAs discussed in the previous subsection, a significant limitation of EIF is its intercept selection strategy, which can result in branches leading to empty nodes.  As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. ", "original_text": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data. "}, "hash": "d105e2260479bfdf97dad7492440f7737f5b90b39cdc8c7ab5fba99f99f2d638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "298ca95d-d07b-469e-8ec3-27f5f2531f37", "node_type": "1", "metadata": {"window": "To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. ", "original_text": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. "}, "hash": "65d12bca3c4dfbf51332c5ad9cda4d2176f6d3d157012800f7e4ab9e927e8d8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. ", "mimetype": "text/plain", "start_char_idx": 14166, "end_char_idx": 14302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "298ca95d-d07b-469e-8ec3-27f5f2531f37", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. ", "original_text": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31b9e1e2-190e-4bcd-9e77-8e2b7d43ae53", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As the tree depth increases, the probability of generating such empty branches rises, incurring additional computational overhead.  To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. ", "original_text": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets. "}, "hash": "7df255eea7703e55eaf3837502c627d86a43489e919e8541081d6e89b06adc37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9315f7c8-3d57-41c0-9af6-be01678309c5", "node_type": "1", "metadata": {"window": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. ", "original_text": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. "}, "hash": "fdb2b806ca6c6d771ddbf566817617265837502958bc8799292e523a49aee750", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. ", "mimetype": "text/plain", "start_char_idx": 14302, "end_char_idx": 14683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9315f7c8-3d57-41c0-9af6-be01678309c5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. ", "original_text": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "298ca95d-d07b-469e-8ec3-27f5f2531f37", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To address this issue, Lesouple et al.  introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. ", "original_text": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}. "}, "hash": "b79ca99c28133d7581005108b4dd97a4354ccfec70ce8eb12c56eb0e41143613", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e54882a-9966-40d0-813d-edf8356c6555", "node_type": "1", "metadata": {"window": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters. ", "original_text": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n"}, "hash": "f037c7aec6cf6805be1bb73f6cb628c5df80bedb5b21ceed5076301b14dfe303", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. ", "mimetype": "text/plain", "start_char_idx": 14683, "end_char_idx": 14926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4e54882a-9966-40d0-813d-edf8356c6555", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters. ", "original_text": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9315f7c8-3d57-41c0-9af6-be01678309c5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "introduced the Generalized Isolation Forest (GIF) in [3], a variation of EIF.  The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. ", "original_text": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches. "}, "hash": "062668690dc580a3bd3700a015055dda4c2fc3c47251607b8b4dc9e1cc24e6d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "969e936d-f0e6-4769-abc8-853cac8d1f5d", "node_type": "1", "metadata": {"window": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. ", "original_text": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. "}, "hash": "3d1975a27452fb34a83d26c2ebd9dd5ea24a1db3a4ec7426c53218d054dc3544", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n", "mimetype": "text/plain", "start_char_idx": 14926, "end_char_idx": 14986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "969e936d-f0e6-4769-abc8-853cac8d1f5d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. ", "original_text": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e54882a-9966-40d0-813d-edf8356c6555", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The fundamental difference between GIF and EIF lies in the selection of the separation hyperplane.  Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters. ", "original_text": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n"}, "hash": "00a9d8b44aef8ed04c2041438d0d8775c9c180d3a7afe9fe3a6c7adb756de937", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d28edb01-ad2d-478c-a617-7c753a2db3fd", "node_type": "1", "metadata": {"window": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. ", "original_text": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. "}, "hash": "faed3598894f97996d71841a73503ede0e6bea137c163223ff540c47b6dca1e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. ", "mimetype": "text/plain", "start_char_idx": 14986, "end_char_idx": 15153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d28edb01-ad2d-478c-a617-7c753a2db3fd", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. ", "original_text": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "969e936d-f0e6-4769-abc8-853cac8d1f5d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Instead of randomly selecting a hyperplane within the smallest hypercube enclosing the data, GIF selects a hyperplane that passes through the convex hull of the data.  Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. ", "original_text": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al. "}, "hash": "388a30217a3fb4c9e2ddad5d7834a68ff14adf1e577e1e759639580fa23d774b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4933838f-8b87-4ea9-b6c4-205fd4818b06", "node_type": "1", "metadata": {"window": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster. ", "original_text": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. "}, "hash": "bf20161512b6a2d2fe36d0ad37c766899f78b6c8e77789bd6059b38cb85da661", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. ", "mimetype": "text/plain", "start_char_idx": 15153, "end_char_idx": 15261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4933838f-8b87-4ea9-b6c4-205fd4818b06", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster. ", "original_text": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d28edb01-ad2d-478c-a617-7c753a2db3fd", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Because the separation hyperplane intersects the convex hull, it is guaranteed to partition the data points into two non-empty subsets.  Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. ", "original_text": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods. "}, "hash": "4c0dd5b4cef5aa5807b2872795bb54e2ceca7f83fb852db1f9f0c21b87ddb5e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "565e7ddf-db2a-4ab4-884c-24edb8051fc3", "node_type": "1", "metadata": {"window": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. ", "original_text": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. "}, "hash": "3bc31a14fa58e6193e2d16d85ccc250a7c6bbd2b628a7e91f0e6ff20179f70e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. ", "mimetype": "text/plain", "start_char_idx": 15261, "end_char_idx": 15361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "565e7ddf-db2a-4ab4-884c-24edb8051fc3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. ", "original_text": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4933838f-8b87-4ea9-b6c4-205fd4818b06", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Formally, GIF randomly selects a normal unit vector w = u/||u||\u2082, where u \u2208 R^d, u ~ N(0, I_d), and subsequently projects the data points onto it via x^T \u00b7 w. It then computes the minimum and maximum projection values and samples the separation value p uniformly within this interval, i.e., p ~ U(p_min, p_max), where p_min = min{x^T \u00b7 w | x \u2208 X} and p_max = max{x^T \u00b7 w | x \u2208 X}.  Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster. ", "original_text": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes. "}, "hash": "aee71f1dda2a8c7c34c9739ffc1f2c9630db20954c006ffca55eab36b235b658", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fa11643-8997-4e7b-9a79-3cea5395b5fb", "node_type": "1", "metadata": {"window": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. ", "original_text": "Consequently, a node will have k child nodes, where k is the number of identified clusters. "}, "hash": "5d1374db6d3bdf34ce2f2e0bc2311a375903ac21e98f54b3d4b813df7b670f4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. ", "mimetype": "text/plain", "start_char_idx": 15361, "end_char_idx": 15675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fa11643-8997-4e7b-9a79-3cea5395b5fb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. ", "original_text": "Consequently, a node will have k child nodes, where k is the number of identified clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "565e7ddf-db2a-4ab4-884c-24edb8051fc3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Although the overall anomaly score distribution does not shift significantly compared to the EIF algorithm, as depicted in 1, the primary advantage lies in the improved computational performance derived from the elimination of empty branches.  Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. ", "original_text": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries. "}, "hash": "04e1f3f0b1b0ddd83f8cb25fc42d5615b244bf19d3de1c878b6c0760026e94d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82b8b81e-4ce6-4d1d-abcc-f13ff699c9d5", "node_type": "1", "metadata": {"window": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n", "original_text": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. "}, "hash": "e527e16fd71eb1e7270446dba9d6ddad3cc94b16b839b4912d6b0a5b48c8a901", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Consequently, a node will have k child nodes, where k is the number of identified clusters. ", "mimetype": "text/plain", "start_char_idx": 15675, "end_char_idx": 15767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82b8b81e-4ce6-4d1d-abcc-f13ff699c9d5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n", "original_text": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fa11643-8997-4e7b-9a79-3cea5395b5fb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Algorithms 1, 6, and 7 provide a skeletal overview of GIF.\n\n ### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. ", "original_text": "Consequently, a node will have k child nodes, where k is the number of identified clusters. "}, "hash": "cacb776c3c2c0bdb61136e2452dc08faaba7c5cc34671f93adbe4fc34124e362", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbe6c616-b734-49cd-8636-3ac8e2170910", "node_type": "1", "metadata": {"window": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. ", "original_text": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. "}, "hash": "21c59584b1aa276c212adb870ee3dfeaede3528962ec91bdf2a2e0f6ea14d2e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. ", "mimetype": "text/plain", "start_char_idx": 15767, "end_char_idx": 15883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbe6c616-b734-49cd-8636-3ac8e2170910", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. ", "original_text": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82b8b81e-4ce6-4d1d-abcc-f13ff699c9d5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 2.4 K-Means Isolation Forest\n\nA distinct variation of the Isolation Forest algorithm is the K-Means Isolation Forest (K-Means IF), introduced by Karczmarek et al.  in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n", "original_text": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm. "}, "hash": "79c3e9120a5feea5c7d727e66afaafb4cbb3a371b9ca77f2ccee965775f67955", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3224fc3-4606-4ad6-8970-da1801d39162", "node_type": "1", "metadata": {"window": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. ", "original_text": "This assignment is based on the distance from the point to the centroid of the cluster. "}, "hash": "a4fbcc5ee8a8eefacb0d51eb68967d5cc47e9a2770f8dde0771945aa219c8a27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. ", "mimetype": "text/plain", "start_char_idx": 15883, "end_char_idx": 16076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3224fc3-4606-4ad6-8970-da1801d39162", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. ", "original_text": "This assignment is based on the distance from the point to the centroid of the cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbe6c616-b734-49cd-8636-3ac8e2170910", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [2], which represents a hybrid approach combining isolation and density-based anomaly detection methods.  The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. ", "original_text": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to. "}, "hash": "b9571edad4ae0c31fafb33fb21ae91f12a261a4103a0b9e781c3634fade3c385", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7c45820-9207-4084-a68e-737f116b4078", "node_type": "1", "metadata": {"window": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). ", "original_text": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. "}, "hash": "35bbf70c6e9d1e53fb7e46f3eaf2daae9f76ff6c750ae54126a86315ab7426b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This assignment is based on the distance from the point to the centroid of the cluster. ", "mimetype": "text/plain", "start_char_idx": 16076, "end_char_idx": 16164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7c45820-9207-4084-a68e-737f116b4078", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). ", "original_text": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3224fc3-4606-4ad6-8970-da1801d39162", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The primary innovation of this algorithm lies in its strategy for selecting separation hyperplanes.  Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. ", "original_text": "This assignment is based on the distance from the point to the centroid of the cluster. "}, "hash": "60f3d4de3fa2018f4b9584b032acca07667ecdc1c2c0af86dd7d37df7e687886", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d7adfca-6f9d-4f8f-be73-0ef4fe54cf29", "node_type": "1", "metadata": {"window": "Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. ", "original_text": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. "}, "hash": "d910916f4f7bf7bd06c9275b22c72df3e223fbeace965bf4c88c0b4fc71ba8e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. ", "mimetype": "text/plain", "start_char_idx": 16164, "end_char_idx": 16318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d7adfca-6f9d-4f8f-be73-0ef4fe54cf29", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. ", "original_text": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7c45820-9207-4084-a68e-737f116b4078", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Unlike the Standard IF, which randomly selects a separation value on a random component (effectively choosing a hyperplane orthogonal to a random axis), this algorithm randomly selects a component, projects all data onto it, and then applies the K-Means clustering algorithm to determine the partition boundaries.  Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). ", "original_text": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component. "}, "hash": "010efd2b842c79c3d39af767555a872d02d093125b3c56467b73416256ae14d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3469176b-4f69-4770-9542-5e72993aaccb", "node_type": "1", "metadata": {"window": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. ", "original_text": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n"}, "hash": "a6d323f8218abd4eb7b012e64a7c933a8fbc21713ee59b3789bb76a116fc1141", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. ", "mimetype": "text/plain", "start_char_idx": 16318, "end_char_idx": 16422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3469176b-4f69-4770-9542-5e72993aaccb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. ", "original_text": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d7adfca-6f9d-4f8f-be73-0ef4fe54cf29", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Consequently, a node will have k child nodes, where k is the number of identified clusters.  The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. ", "original_text": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes. "}, "hash": "536d11315478479f170a4a4793d4ccd1b31043024868fb1267a72fc8fc306a8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c83aad5e-57e0-45dc-bb28-05f7aa875149", "node_type": "1", "metadata": {"window": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. ", "original_text": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. "}, "hash": "b882102b41443333ca75430a00aa81b85ff3f4043e3b8cbf8de4486436e49dc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n", "mimetype": "text/plain", "start_char_idx": 16422, "end_char_idx": 16560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c83aad5e-57e0-45dc-bb28-05f7aa875149", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. ", "original_text": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3469176b-4f69-4770-9542-5e72993aaccb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The value of k is determined using the \"elbow-rule,\" which we define after a brief review of the K-Means algorithm.  In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. ", "original_text": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n"}, "hash": "be940312ed0cf2c7b9b652898f4ca29177723b0824b4c4bbd3832857988601f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f81fcb7-f7cf-4dbb-8196-2159ffe35aaf", "node_type": "1", "metadata": {"window": "This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. "}, "hash": "968ec5c1962ef60a52297b0e074a37e76cf52114b3f708c13d5efb47cb9ceb9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. ", "mimetype": "text/plain", "start_char_idx": 16560, "end_char_idx": 16635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f81fcb7-f7cf-4dbb-8196-2159ffe35aaf", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c83aad5e-57e0-45dc-bb28-05f7aa875149", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In essence, K-Means IF constructs a tree node by first randomly selecting a component, projecting the data onto it, and then assigning each data point to the cluster it most likely belongs to.  This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. ", "original_text": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max. "}, "hash": "86743dc53c074576c77506c9d72a82ed49dc2dcb907392cdac43ff6ff6ab658d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9e18fcf-ef61-4451-9889-91939d67615b", "node_type": "1", "metadata": {"window": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). "}, "hash": "9d805ecdfc093043f5eda9547014ccc1a1dae5c83a9ff5f79928be5df87883a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. ", "mimetype": "text/plain", "start_char_idx": 16635, "end_char_idx": 17257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c9e18fcf-ef61-4451-9889-91939d67615b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f81fcb7-f7cf-4dbb-8196-2159ffe35aaf", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This assignment is based on the distance from the point to the centroid of the cluster.  Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max. "}, "hash": "911807b96ef52445e0252e550b47f46dd8b4a5f5a9a2b1b5d2b52740c3c51118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1a4ffde-fabb-4f96-bba6-9f370def7249", "node_type": "1", "metadata": {"window": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. ", "original_text": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. "}, "hash": "0189f8bcf18d4872ca3b7ad9eec2a9523b4f495432ecefa5aa9e9d3578fa7198", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). ", "mimetype": "text/plain", "start_char_idx": 17257, "end_char_idx": 17789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1a4ffde-fabb-4f96-bba6-9f370def7249", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. ", "original_text": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9e18fcf-ef61-4451-9889-91939d67615b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since clusters are formed in a 1-dimensional space, the assignment boundaries are dictated by hyperplanes perpendicular to the randomly chosen component.  The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters). "}, "hash": "1ae8f93fdf2ecabfcd5483eb7a86eba0fd95a0a95d8e60a8fe3186ac037b7b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d13383f4-2eaa-4c43-8ee1-09a074b2906a", "node_type": "1", "metadata": {"window": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. ", "original_text": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. "}, "hash": "d39e4c6a8059c654a5b8f2f0414d9a740fd18aa30245fa7615485bbe7eec3676", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. ", "mimetype": "text/plain", "start_char_idx": 17789, "end_char_idx": 17926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d13383f4-2eaa-4c43-8ee1-09a074b2906a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. ", "original_text": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1a4ffde-fabb-4f96-bba6-9f370def7249", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The resulting structure is no longer a binary tree, but a tree with an arbitrary number of child nodes.  We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. ", "original_text": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance. "}, "hash": "b20efb806ee7abaf1a1d71c5d9545a01a96f684113a2503d993d4df9de219ab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cde31fcc-114f-4ccd-b82c-663571cc28a4", "node_type": "1", "metadata": {"window": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. ", "original_text": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. "}, "hash": "c2d066431ff0b1d12dd3c462e5a603643a0b1cd504eb6470a9ef0822b2952e79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. ", "mimetype": "text/plain", "start_char_idx": 17926, "end_char_idx": 18042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cde31fcc-114f-4ccd-b82c-663571cc28a4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. ", "original_text": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d13383f4-2eaa-4c43-8ee1-09a074b2906a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We next recall the K-Means algorithm and the \"elbow-rule\" before detailing the K-Means IF algorithm and its anomaly scoring methodology.\n\n **Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. ", "original_text": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster. "}, "hash": "1a1d792bf28949bd127b86d3f2787056512e6357b778f0be3c92e33130732334", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20b6ee30-bbb4-4eb9-b2cc-3263c539823b", "node_type": "1", "metadata": {"window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n", "original_text": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n"}, "hash": "84165fc116d6f6bfa798b784bd3d2d309549d27b63aae6b384482f1366b63044", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. ", "mimetype": "text/plain", "start_char_idx": 18042, "end_char_idx": 18225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20b6ee30-bbb4-4eb9-b2cc-3263c539823b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n", "original_text": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cde31fcc-114f-4ccd-b82c-663571cc28a4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 2: Create Isolation Tree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. ", "original_text": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible. "}, "hash": "a1e9c131ccdcbc6bd3192f53787ce781a861b0657111fd376212694039369733", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3652a155-fa3b-43e1-af38-fc33036ab741", "node_type": "1", "metadata": {"window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). ", "original_text": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. "}, "hash": "0e3606c27cfcfa9872d29d829a3fa7dccb5028f5aa877a768041a218ac09bbb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n", "mimetype": "text/plain", "start_char_idx": 18225, "end_char_idx": 18426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3652a155-fa3b-43e1-af38-fc33036ab741", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). ", "original_text": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20b6ee30-bbb4-4eb9-b2cc-3263c539823b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw q ~ U(0,1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 p_min \u2190 min_{x\u2208X} x^(q)\n7 p_max \u2190 max_{x\u2208X} x^(q)\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^(q) \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^(q) > p}\n11 **return**\nITree{size \u2190 |X|,\nsplitVal \u2190 p,\nsplitAtt \u2190 q,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 3: Compute Anomaly Score**\n**Data:** x - data point, iTree - isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n", "original_text": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n"}, "hash": "d6fb7771457f3e90317cf187ec5750585710ffda71efdddbdde69b274c1040ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c57fcc0-14f2-47c5-98bc-5845830ca0d1", "node_type": "1", "metadata": {"window": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. ", "original_text": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. "}, "hash": "c2e8ca24cc870456802a9aa4cf2e37e437d24cdec7fba92c18cb852d2c238889", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. ", "mimetype": "text/plain", "start_char_idx": 18426, "end_char_idx": 18535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c57fcc0-14f2-47c5-98bc-5845830ca0d1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. ", "original_text": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3652a155-fa3b-43e1-af38-fc33036ab741", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** l + c(iTree.size)\n4 p \u2190 iTree.splitVal\n5 q \u2190 iTree.splitAtt\n6 **if** x^(q) \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\nGiven k, the number of clusters, the K-Means algorithm begins by randomly selecting k distinct data points from the dataset (these points serve as the initial centroids of the clusters).  It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). ", "original_text": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset. "}, "hash": "64a686c752aecd28b94df1720a5242b36c2ea20617bb555248843c363431924a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1057f9f1-9355-485c-ae5b-e6a58f7e4213", "node_type": "1", "metadata": {"window": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. ", "original_text": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. "}, "hash": "33bf70f864d2d501d5fc6757a83970849168c2474b4ec8ddbdc54d56504e0f82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. ", "mimetype": "text/plain", "start_char_idx": 18535, "end_char_idx": 18945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1057f9f1-9355-485c-ae5b-e6a58f7e4213", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. ", "original_text": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c57fcc0-14f2-47c5-98bc-5845830ca0d1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "It then iterates through the remaining data points, assigning each to the cluster with the closest centroid based on Euclidean distance.  Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. ", "original_text": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}. "}, "hash": "fc35351e9449075672dae6ffb136197c916021ec20bbe70d31b87d7ead25e04e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "554cf390-963e-4e43-b221-9ebab0a7c953", "node_type": "1", "metadata": {"window": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. ", "original_text": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. "}, "hash": "2a0a61874211074689a13352983a00898fc0a05203d4835eb832dfd8f3586a3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. ", "mimetype": "text/plain", "start_char_idx": 18945, "end_char_idx": 19061, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "554cf390-963e-4e43-b221-9ebab0a7c953", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. ", "original_text": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1057f9f1-9355-485c-ae5b-e6a58f7e4213", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Once all points are assigned, new centroids are computed by calculating the mean of the points within each cluster.  The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. ", "original_text": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns. "}, "hash": "96bcc2cc85991b2992390b6a9a496b9e58eef76f58b9645cc0c144cbee55d681", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ade16fc-35ff-4fbf-b267-a16ec724c6d8", "node_type": "1", "metadata": {"window": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. ", "original_text": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n"}, "hash": "0a0edfe77e21bbed8f87f57dc797f09a1098ce1ccf8e37d68e8277361b98ce79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. ", "mimetype": "text/plain", "start_char_idx": 19061, "end_char_idx": 19189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ade16fc-35ff-4fbf-b267-a16ec724c6d8", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. ", "original_text": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "554cf390-963e-4e43-b221-9ebab0a7c953", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The assignment process repeats with the new centroids until convergence is achieved\u2014either a maximum number of iterations is reached or the variation in centroids becomes negligible.  Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. ", "original_text": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal. "}, "hash": "c3517ed16fad20cfe6a77c56843311aa4fad979ae0e351d965f018e4bedf605d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c091f3e-ea21-4150-8def-2d4eac71470e", "node_type": "1", "metadata": {"window": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. ", "original_text": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). "}, "hash": "50ee148d248e3b5e92604012610d45cfdd7300a9e40dbdf4dcf0b0a5b21eabff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n", "mimetype": "text/plain", "start_char_idx": 19189, "end_char_idx": 19370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c091f3e-ea21-4150-8def-2d4eac71470e", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. ", "original_text": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ade16fc-35ff-4fbf-b267-a16ec724c6d8", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since the algorithm's outcome depends on the initial selection of the k centers, it is typically run multiple times, and the best clustering (in terms of minimal intra-cluster variation) is selected.\n\n The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. ", "original_text": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n"}, "hash": "7e7955a8d996f84abb2162edfac65357a0ac2e718a23ee9bdf69ca5f2c6898c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "349e95fd-6e79-45dc-ab6d-74d5bcf518d1", "node_type": "1", "metadata": {"window": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point. ", "original_text": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. "}, "hash": "174fa603b73ff65ca1a3ff2e53754358fac7bce3dd94167ced250e9e0ebe987f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). ", "mimetype": "text/plain", "start_char_idx": 19370, "end_char_idx": 19584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "349e95fd-6e79-45dc-ab6d-74d5bcf518d1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point. ", "original_text": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c091f3e-ea21-4150-8def-2d4eac71470e", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The \"elbow-rule\" is a heuristic method for determining a suitable number of clusters to partition a dataset.  Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. ", "original_text": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}). "}, "hash": "78cd1d35ae57cabcb985645973c307f04e9c780863e1d4bd8665937899a4f33b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4c5ebef-4f9e-43ae-9fc1-19530aac9a9f", "node_type": "1", "metadata": {"window": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. ", "original_text": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. "}, "hash": "0ba2717b1cc365018548b593745376288883ad332ae9f7d2a975d7d6ab8056f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. ", "mimetype": "text/plain", "start_char_idx": 19584, "end_char_idx": 19768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4c5ebef-4f9e-43ae-9fc1-19530aac9a9f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. ", "original_text": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "349e95fd-6e79-45dc-ab6d-74d5bcf518d1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given that the worst clustering occurs at k = 1 (variance equal to the total dataset variance) and the best at k = |X| (where each point forms its own cluster, resulting in zero variance), we observe that the Sum of Squared Errors, SSE = \u03a3_{i=1}^k \u03a3_{x\u2208C_i} ||x \u2212 \u00b5_i||\u00b2, decreases as k increases, where \u00b5_i is the centroid of the i-th cluster, and C_i = {x \u2208 X | ||x \u2212 \u00b5_i||\u00b2 \u2264 ||x \u2212 \u00b5_j||\u00b2, \u2200j = 1, ..., k}.  Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point. ", "original_text": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster. "}, "hash": "00aa146764d85b8bef4269e31237a829d3badc09b82dbd5e1e6f96b4a19c9c48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "043ec17c-199b-40dd-b1fe-dfffff7edf5c", "node_type": "1", "metadata": {"window": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0. ", "original_text": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. "}, "hash": "281afc6c3824be8ffac23ecd8c0cfd8c7a83e4b384774449db2fe1188874c9f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. ", "mimetype": "text/plain", "start_char_idx": 19768, "end_char_idx": 19915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "043ec17c-199b-40dd-b1fe-dfffff7edf5c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0. ", "original_text": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4c5ebef-4f9e-43ae-9fc1-19530aac9a9f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Knowing that increasing k generally improves clustering, the goal is to find a k that provides diminishing returns.  Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. ", "original_text": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius. "}, "hash": "f69eac2e449ee5289d2bcd7313d9e4bc5d36999d313ecb2c8298f7ab19b6e8f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5819ea7a-f8ca-4cb7-9230-568bde82897f", "node_type": "1", "metadata": {"window": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). ", "original_text": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. "}, "hash": "2a0bd9b1cf67a35fc6218dd51efd457d5cb7d37435bd87bbde365e05fcae6d83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. ", "mimetype": "text/plain", "start_char_idx": 19915, "end_char_idx": 20059, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5819ea7a-f8ca-4cb7-9230-568bde82897f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). ", "original_text": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "043ec17c-199b-40dd-b1fe-dfffff7edf5c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Using the \"elbow-rule,\" we iterate through different values of k starting from 1 until the improvement in SSE becomes marginal.  If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0. ", "original_text": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0. "}, "hash": "a0d59698afb1098eecad1318c7d0a6fc0cb69a63d25d5bfc460f00fa33ab4629", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85cb6917-653f-4003-84cf-459594e0a86a", "node_type": "1", "metadata": {"window": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. ", "original_text": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. "}, "hash": "6bf8547d9cce5613e3f414f8140068e1dda572c250fc8e85bc7c7eb84682a7be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. ", "mimetype": "text/plain", "start_char_idx": 20059, "end_char_idx": 20168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85cb6917-653f-4003-84cf-459594e0a86a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. ", "original_text": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5819ea7a-f8ca-4cb7-9230-568bde82897f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If SSE is plotted against k, a sharp decrease is initially observed, followed by a flattening of the curve; the \"elbow\" point marks where the rate of decrease slows significantly.\n\n Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). ", "original_text": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal. "}, "hash": "71487283dc643efd8aaa71798211d7536a2ce39d4aa8ae005a395a4128973c76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11a13617-42b6-4268-8a0b-b36ab84bdd92", "node_type": "1", "metadata": {"window": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. ", "original_text": "Note that a split is chosen with respect to the closest cluster to the input data point. "}, "hash": "4ae886a4bfc5b9d730cfb9b63ea6695f362e5ff822bd97e89aa851042f71b1ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. ", "mimetype": "text/plain", "start_char_idx": 20168, "end_char_idx": 20295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "11a13617-42b6-4268-8a0b-b36ab84bdd92", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. ", "original_text": "Note that a split is chosen with respect to the closest cluster to the input data point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85cb6917-653f-4003-84cf-459594e0a86a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Returning to the K-Means based Isolation Forest, at every node, we compute the clustering of the resident data points projected onto a randomly chosen component using the elbow rule (typically finding k \u2208 {4, 5}).  The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. ", "original_text": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly. "}, "hash": "7881b4c0abef4f96ca218a31240b0defddc2149430b54ea64179b82bdd9d943f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92fb98ed-1856-4b5e-957f-4cc22af5880a", "node_type": "1", "metadata": {"window": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n", "original_text": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. "}, "hash": "2596f3dec2e31c250d47017d6cfe951438903dcf3af5554491ec90344f2517dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that a split is chosen with respect to the closest cluster to the input data point. ", "mimetype": "text/plain", "start_char_idx": 20295, "end_char_idx": 20384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92fb98ed-1856-4b5e-957f-4cc22af5880a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n", "original_text": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11a13617-42b6-4268-8a0b-b36ab84bdd92", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The number of branches originating from that node corresponds to the number of clusters found, with each branch receiving the points closest to the centroid of its respective cluster.  The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. ", "original_text": "Note that a split is chosen with respect to the closest cluster to the input data point. "}, "hash": "5e49f04303bef444bde116643c33abb4fa1a376943a9672089f5920a89d5f804", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "818d669d-e553-4144-b651-1e60e6ce8e86", "node_type": "1", "metadata": {"window": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. ", "original_text": "On the other hand, a normal point will have a score close to 0. "}, "hash": "54775b438433ba8cc2f12577762f6367c49f3d058c327ff16ac5903ed9c101b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. ", "mimetype": "text/plain", "start_char_idx": 20384, "end_char_idx": 20653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "818d669d-e553-4144-b651-1e60e6ce8e86", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. ", "original_text": "On the other hand, a normal point will have a score close to 0. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92fb98ed-1856-4b5e-957f-4cc22af5880a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The score obtained at each split is computed by the formula:\n\ns(x) = 1 \u2212 d(x, \u03bc)/r (3)\n\nwhere \u00b5 is the cluster center and r is the cluster radius.  We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n", "original_text": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large. "}, "hash": "d6654d73f48659809d7d2fedcfeabd27e17c4e4f4e89e1848379b7884f380146", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc18d176-f81e-4b90-8887-3be3743c35c1", "node_type": "1", "metadata": {"window": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. ", "original_text": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). "}, "hash": "b05669787043da7b267c1cc03491efbc7bf5f23f109387cab701a7520ee735f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, a normal point will have a score close to 0. ", "mimetype": "text/plain", "start_char_idx": 20653, "end_char_idx": 20717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc18d176-f81e-4b90-8887-3be3743c35c1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. ", "original_text": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "818d669d-e553-4144-b651-1e60e6ce8e86", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We emphasize that if x belongs to the cluster, its distance to the center will be smaller than the radius (i.e., d(x, \u00b5)/r < 1), thus s(x) \u2265 0.  If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. ", "original_text": "On the other hand, a normal point will have a score close to 0. "}, "hash": "c8c1cbb744fed79fe3b674f4858eccb25333d68e514503422f210ddc461ea087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34aa43ba-86b8-43bb-9352-82b9e31d0ec3", "node_type": "1", "metadata": {"window": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. ", "original_text": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. "}, "hash": "059277c628c832cb7f8163fa7dafe6194900292f8ad2b7f508fcd1a4ead5812f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). ", "mimetype": "text/plain", "start_char_idx": 20717, "end_char_idx": 20767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34aa43ba-86b8-43bb-9352-82b9e31d0ec3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. ", "original_text": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc18d176-f81e-4b90-8887-3be3743c35c1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If x is very close to the cluster center, s(x) approaches 1, indicating confidence that the point is normal.  Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. ", "original_text": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e). "}, "hash": "5db51cab88dd3f9e720cec84888ccc814dc7b0f2daa2a0e4c4f0495da389443a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41629d3a-f42c-4b1f-824a-a2892257f672", "node_type": "1", "metadata": {"window": "Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n", "original_text": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. "}, "hash": "4aea6135f47f029af466c2da0530bc74807c9e63899d4d8ce622fde827224380", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. ", "mimetype": "text/plain", "start_char_idx": 20767, "end_char_idx": 20935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41629d3a-f42c-4b1f-824a-a2892257f672", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n", "original_text": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34aa43ba-86b8-43bb-9352-82b9e31d0ec3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Conversely, if x is far from the cluster, then d(x, \u00b5)/r > 1, resulting in a highly negative s(x), which indicates an anomaly.  Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. ", "original_text": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score. "}, "hash": "3f688df821c354460bd91623d63f76278469a61ad5f0a7d54b9468c0d4e0d0e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcf516d8-5a66-4965-9993-47864439a096", "node_type": "1", "metadata": {"window": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. ", "original_text": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n"}, "hash": "1b5298f0a570d995ed2b41d5b7f62d2e764e3fe99bb948541a3a8707acfec7d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. ", "mimetype": "text/plain", "start_char_idx": 20935, "end_char_idx": 21050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bcf516d8-5a66-4965-9993-47864439a096", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. ", "original_text": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41629d3a-f42c-4b1f-824a-a2892257f672", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Note that a split is chosen with respect to the closest cluster to the input data point.  To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n", "original_text": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1. "}, "hash": "8e596bcc9a945c2e99cfa37f8222a13f53768241bbcc95341b0d893ed11830fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8325471-a907-48c5-a9f6-a53e528d38d0", "node_type": "1", "metadata": {"window": "On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. ", "original_text": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. "}, "hash": "76cdebb37e012c48c5d2f334744cae299f11a3fa447e62e808d3b9ddb1c1e620", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n", "mimetype": "text/plain", "start_char_idx": 21050, "end_char_idx": 21127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8325471-a907-48c5-a9f6-a53e528d38d0", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. ", "original_text": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcf516d8-5a66-4965-9993-47864439a096", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To compute the anomaly score at the forest level, we use the formula:\n\na(x) = 1 \u2212 (1/t) \u03a3_{i=1}^t (1/M_i) \u03a3_{j=1}^{M_i} s_j(x) (4)\n\nwhere M_i, i = 1, ..., t is the sequence of splits taken by an input point for tree i. If a point is anomalous, a(x) will be very large.  On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. ", "original_text": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n"}, "hash": "2fa375165c360e2e4c4e8540fbf77893c77cce641e80256e11b8577130bab31b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9af9ee5-96e5-4448-a8dc-f5e41d3f4386", "node_type": "1", "metadata": {"window": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. ", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. "}, "hash": "185b4d6243c9aebb97307339efabe84d605582a574b45f730e84f14146d445df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. ", "mimetype": "text/plain", "start_char_idx": 21127, "end_char_idx": 21202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9af9ee5-96e5-4448-a8dc-f5e41d3f4386", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. ", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8325471-a907-48c5-a9f6-a53e528d38d0", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "On the other hand, a normal point will have a score close to 0.  Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. ", "original_text": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max. "}, "hash": "398ae265a5df726aff55b15ced7bb65544d2b23290f484e455d15dcb09cdade3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebcb17f2-0d2d-4e5b-8c87-b7fd9de7f5d1", "node_type": "1", "metadata": {"window": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. "}, "hash": "00f6be7b1689d7361f923ef5b8eaf160da68e4bd4a48bf532f8d4bb515c85a32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. ", "mimetype": "text/plain", "start_char_idx": 21202, "end_char_idx": 21862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebcb17f2-0d2d-4e5b-8c87-b7fd9de7f5d1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9af9ee5-96e5-4448-a8dc-f5e41d3f4386", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Since s(x) \u2208 (-\u221e, 1], this implies a(x) \u2208 [0, \u221e).  In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. ", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max. "}, "hash": "967ed238a3848ce9dcaa4a013ef8acffc4fbd1db6b84fa2438cd6a42efc42eee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4577b4c4-778e-4b4d-a851-1936d1262ec5", "node_type": "1", "metadata": {"window": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. ", "original_text": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n"}, "hash": "fe92038231dd92b82d1d85e5dfe478d5ecc0eff330a9f7965e2ffa641c588b60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 21862, "end_char_idx": 22448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4577b4c4-778e-4b4d-a851-1936d1262ec5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. ", "original_text": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebcb17f2-0d2d-4e5b-8c87-b7fd9de7f5d1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In our experiments, to ensure consistent anomaly scores across all evaluated methods, we applied the sigmoid function \u03c3(x) = 1 / (1 + e^-x) to the final anomaly score.  With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF. "}, "hash": "c47251ca3c7f99c778f84036eb609dea9e04ce0a93ce65961b7b1ca2c4934fae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83faeec1-7322-4449-82b6-84cd11d4c769", "node_type": "1", "metadata": {"window": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. ", "original_text": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. "}, "hash": "bea3290c966f65a1c8670d6d44154f9138ae6464f5831cfb0417c8d89ad35f35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n", "mimetype": "text/plain", "start_char_idx": 22448, "end_char_idx": 22540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83faeec1-7322-4449-82b6-84cd11d4c769", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. ", "original_text": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4577b4c4-778e-4b4d-a851-1936d1262ec5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "With this transformation, normal points receive scores close to 0, while anomalous ones receive scores close to 1.  A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. ", "original_text": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n"}, "hash": "3e45b89e7ac38378f78a621f95d6ef8cd56534a34496b876867058fb8ca8f888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "985162bf-2c11-4bbd-b291-4018f239ccdb", "node_type": "1", "metadata": {"window": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2]. ", "original_text": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. "}, "hash": "71b083c6c034954b479cf004b1682b97ca515c8c06f2f6de64c4debb2d2d13c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 22540, "end_char_idx": 22835, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "985162bf-2c11-4bbd-b291-4018f239ccdb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2]. ", "original_text": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83faeec1-7322-4449-82b6-84cd11d4c769", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A skeletal overview of the K-Means IF algorithm can be found in 1, 8 and 9.\n\n **Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. ", "original_text": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF. "}, "hash": "b1f98203fdb21ca2197b3993f42de3bffa1cd132a587537f22636584a933fb25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db9c3ea-ba9f-4267-b5f5-b713da64803a", "node_type": "1", "metadata": {"window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries. ", "original_text": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. "}, "hash": "ca94542cd83f27a513630a5342182e09e5890cff163cc1c4a0d89dbffc705026", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. ", "mimetype": "text/plain", "start_char_idx": 22835, "end_char_idx": 23041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4db9c3ea-ba9f-4267-b5f5-b713da64803a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries. ", "original_text": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "985162bf-2c11-4bbd-b291-4018f239ccdb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 4: Create Extended ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2]. ", "original_text": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components. "}, "hash": "e320587797731d369622f99e6e1b08139a0783d9661901ca1615c53176ce2ee5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fa8afbd-f7de-4046-ae05-1915c8936ce1", "node_type": "1", "metadata": {"window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. ", "original_text": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. "}, "hash": "1ecf2fe4f86ef725fdd7d5ae38486408e8e604f6b4346c85b0e1d3e09bf1dbbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. ", "mimetype": "text/plain", "start_char_idx": 23041, "end_char_idx": 23175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3fa8afbd-f7de-4046-ae05-1915c8936ce1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. ", "original_text": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db9c3ea-ba9f-4267-b5f5-b713da64803a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** ITree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min X, p_min \u2208 R^d\n7 p_max \u2190 max X, p_max \u2208 R^d\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w \u2264 0}\n10 X_r \u2190 {x | x \u2208 X, (x \u2212 p)^T \u00b7 w > 0}\n11 **return**\nITree{size \u2190 |X|,\nintercept \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 5: Compute Anomaly Score EIF**\n**Data:** x - data point, iTree - extended isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries. ", "original_text": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes. "}, "hash": "f914a0292b8e389f12318b65cacbaa7c554d000f5aab1214939b20322f566857", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "865ec469-a135-4ad6-b470-0d1fbc9a59e5", "node_type": "1", "metadata": {"window": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. ", "original_text": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. "}, "hash": "cdd47012299f4512f99dadd62cce2b077c99bed8fe3c0893e7676d51ef68c11f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. ", "mimetype": "text/plain", "start_char_idx": 23175, "end_char_idx": 23348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "865ec469-a135-4ad6-b470-0d1fbc9a59e5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. ", "original_text": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fa8afbd-f7de-4046-ae05-1915c8936ce1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree, lvl, l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.intercept\n6 **if** (x \u2212 p)^T \u00b7 w \u2264 0 **then**\n7 | **return** 1 + score(x, iTree.left, lvl + 1, l)\n8 **return** 1 + score(x, iTree.right, lvl + 1, l)\n\n## 3 Alternative solutions\n\nIn this section, we introduce two novel variations of the Isolation Forest algorithm that combine the random projection strategies of Extended IF (EIF) with the clustering-based partitioning of K-Means IF.  We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. ", "original_text": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid. "}, "hash": "a71bdccbfb50dc3618afa7fc4b7d20c34a1078a78629e419a7883578cf3fffe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63ae8677-4dc1-430c-a4d7-69702ee7a8ac", "node_type": "1", "metadata": {"window": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. ", "original_text": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. "}, "hash": "2946640b02a04e1f262738e0fa6e07d1d1090195e464fe047dcb125f28c90aad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. ", "mimetype": "text/plain", "start_char_idx": 23348, "end_char_idx": 23536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63ae8677-4dc1-430c-a4d7-69702ee7a8ac", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. ", "original_text": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "865ec469-a135-4ad6-b470-0d1fbc9a59e5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We refer to these newly introduced methods as Subspace K-Means IF and Extended K-Means IF.\n\n ### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. ", "original_text": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning. "}, "hash": "fa9b76b2f0b9b6eb5ca8b41840d14b3f56d6710fb591bcf2dcd63bdb6a811a13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bcad16d-26d5-4aa4-abf0-ce0e384b6c89", "node_type": "1", "metadata": {"window": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. ", "original_text": "in [2]. "}, "hash": "d0a5763d01c6b88c7d3eb4504b9e7c54a44dc1c8b4b78977f21fac781bac54ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. ", "mimetype": "text/plain", "start_char_idx": 23536, "end_char_idx": 23673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1bcad16d-26d5-4aa4-abf0-ce0e384b6c89", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. ", "original_text": "in [2]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63ae8677-4dc1-430c-a4d7-69702ee7a8ac", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "### 3.1 Subspace K-Means Isolation Forest\n\nThe Subspace K-Means Isolation Forest is a variation of the Isolation Forest algorithm that brings together the random selection of a subspace (parallel to the coordinate axes) with the clustering-based partitioning mechanism introduced by K-Means IF.  More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. ", "original_text": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al. "}, "hash": "9f5f8912fcd4fa991bbee4ae5030dbe0fa1ae468b9e8c9b3c0b9c50547908b7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23c93f5e-3972-48fd-bd5c-a352b3634701", "node_type": "1", "metadata": {"window": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. ", "original_text": "However, the distinction between these two versions lies in the complexity of the partition boundaries. "}, "hash": "9eb0f20c34cbfe2f403afa37fa8eaa7926027bdfbad1b004e7c069ce3fbf4b1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in [2]. ", "mimetype": "text/plain", "start_char_idx": 23673, "end_char_idx": 23681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23c93f5e-3972-48fd-bd5c-a352b3634701", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. ", "original_text": "However, the distinction between these two versions lies in the complexity of the partition boundaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bcad16d-26d5-4aa4-abf0-ce0e384b6c89", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "More precisely, an isolation node is recursively constructed by first selecting k random components, where 1 \u2264 k \u2264 d, and then projecting the dataset into the subspace defined by these selected components.  Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. ", "original_text": "in [2]. "}, "hash": "a54780abe8f5fb014e3248356bd79556ba682464c95dd3403c732d2837e72b1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ad3ba73-3e11-4710-8f9c-06aa2d36838b", "node_type": "1", "metadata": {"window": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n", "original_text": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. "}, "hash": "81018b3fe34fcb159386db34050c08728095e2d1000a50f569044eff4354c1f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the distinction between these two versions lies in the complexity of the partition boundaries. ", "mimetype": "text/plain", "start_char_idx": 23681, "end_char_idx": 23785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ad3ba73-3e11-4710-8f9c-06aa2d36838b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n", "original_text": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23c93f5e-3972-48fd-bd5c-a352b3634701", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Subsequently, the K-Means clustering algorithm is applied, utilizing the \"elbow\" rule to cluster the points and generate child nodes.  The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. ", "original_text": "However, the distinction between these two versions lies in the complexity of the partition boundaries. "}, "hash": "9fd87a032bbe72819cd6b8702dacbc2877fc97e94a6244e80b6fa3d3e2637d13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "261d715d-d152-4b25-8231-48480f69b434", "node_type": "1", "metadata": {"window": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. ", "original_text": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. "}, "hash": "f60ab8b20ba55d252e1be33ff508db29fee8182d7032c9d123ccabfb2e303dcb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. ", "mimetype": "text/plain", "start_char_idx": 23785, "end_char_idx": 23914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "261d715d-d152-4b25-8231-48480f69b434", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. ", "original_text": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ad3ba73-3e11-4710-8f9c-06aa2d36838b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The assignment of a point to a cluster follows the same logic as in K-Means IF, specifically determining membership based on the Euclidean distance to the nearest centroid.  It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n", "original_text": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes. "}, "hash": "fdf97fd420865e945561cbf202d3a53ada9d8f3f8c130f4f23b6197cd7197ed7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fefb874-4a04-4d29-ba0e-f6191e1320bb", "node_type": "1", "metadata": {"window": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. ", "original_text": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. "}, "hash": "4215e723c4c6c8a35267fbff32b5c242678526626d43ae26d7715db261a74051", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. ", "mimetype": "text/plain", "start_char_idx": 23914, "end_char_idx": 24275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fefb874-4a04-4d29-ba0e-f6191e1320bb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. ", "original_text": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "261d715d-d152-4b25-8231-48480f69b434", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "It is important to note that this algorithm represents a generalization of K-Means IF, as the standard K-Means IF projects data onto a single dimension before clustering and partitioning.  In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. ", "original_text": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells. "}, "hash": "7d528ed6982326ee8e10eb41d0728fde8fe69442f045b2023df5911069d372c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2ebebd4-4064-45d8-868d-a36301acaf1c", "node_type": "1", "metadata": {"window": "in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. ", "original_text": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. "}, "hash": "b8db33e9e931eb48c5d46934a917952720347325270fd22981b07bab2e578bcb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. ", "mimetype": "text/plain", "start_char_idx": 24275, "end_char_idx": 24385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f2ebebd4-4064-45d8-868d-a36301acaf1c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. ", "original_text": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fefb874-4a04-4d29-ba0e-f6191e1320bb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In fact, the Subspace K-Means Isolation Forest with k = 1 is mathematically equivalent to the K-Means IF introduced by Karczmarek et al.  in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. ", "original_text": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data. "}, "hash": "80a6b0006e31b13d2def3680fe890a2779195a830974666dfd435371977babae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebeb3d00-74a8-4317-9b64-52767b2e9868", "node_type": "1", "metadata": {"window": "However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. ", "original_text": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. "}, "hash": "d2c0b04296db3ded5da845ce5e4f48fa52619f1f8549c4fe9219a1d1569de2dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. ", "mimetype": "text/plain", "start_char_idx": 24385, "end_char_idx": 24570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebeb3d00-74a8-4317-9b64-52767b2e9868", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. ", "original_text": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2ebebd4-4064-45d8-868d-a36301acaf1c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "in [2].  However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. ", "original_text": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering. "}, "hash": "576e5014b100f46b4f6124f026a165c916d380b50f466cca018b7f489067840b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efe1304f-b092-49a4-9c81-fc48a0872b18", "node_type": "1", "metadata": {"window": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. ", "original_text": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n"}, "hash": "5686cd60393781b139c4f05045d98993b53c4e3390dca44296ff3356cc50db31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. ", "mimetype": "text/plain", "start_char_idx": 24570, "end_char_idx": 24791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "efe1304f-b092-49a4-9c81-fc48a0872b18", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. ", "original_text": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebeb3d00-74a8-4317-9b64-52767b2e9868", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, the distinction between these two versions lies in the complexity of the partition boundaries.  Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. ", "original_text": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees. "}, "hash": "f42973a1f56e18dca9c79babec6cc873bf83f028db643fb80162ce38c31445d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5215d65f-147e-4b9a-8e46-eb39905fd340", "node_type": "1", "metadata": {"window": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n", "original_text": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. "}, "hash": "53410caffdde09d859b1181e6c8877beb180d3ab4014b4330eb1f2444597f558", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n", "mimetype": "text/plain", "start_char_idx": 24791, "end_char_idx": 24986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5215d65f-147e-4b9a-8e46-eb39905fd340", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n", "original_text": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efe1304f-b092-49a4-9c81-fc48a0872b18", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Recall that in the original K-Means IF, the separation boundaries are effectively hyperplanes orthogonal to the coordinate axes.  In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. ", "original_text": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n"}, "hash": "478801126ed1fdbd2e8e701762965f75f175c74295ffd561dc16a3764a1d86bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e996f6bc-fb59-4f87-8d33-27c4a4804fcd", "node_type": "1", "metadata": {"window": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. ", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. "}, "hash": "89822311621c0ae2a0893e9ffdee3a1df8e8389d05a9022aa5e0f28f75351e39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. ", "mimetype": "text/plain", "start_char_idx": 24986, "end_char_idx": 25064, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e996f6bc-fb59-4f87-8d33-27c4a4804fcd", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. ", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5215d65f-147e-4b9a-8e46-eb39905fd340", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In contrast, in the subspace version, the resulting separation boundaries form polytopes that enclose the data clusters within the projected dimensions, with the dimensionality defined by the hyperparameter k. For instance, if X \u2282 R\u00b2 and k = 2 (i.e., clustering is performed in the original space at each node), the resulting partitions resemble Voronoi cells.  As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n", "original_text": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max. "}, "hash": "2bb41099d80d4e88e1e4efffc7bf41dd1277b17b38b6140660c567470462785c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b2817c4-2fbd-4f89-92f5-350611e23289", "node_type": "1", "metadata": {"window": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. "}, "hash": "7d48bb6138b9ca083a7144b97a1219dce7476a4fa2def8ddd353108f9d4c5962", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. ", "mimetype": "text/plain", "start_char_idx": 25064, "end_char_idx": 25716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b2817c4-2fbd-4f89-92f5-350611e23289", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e996f6bc-fb59-4f87-8d33-27c4a4804fcd", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As the depth of the tree increases, these cells become increasingly granular, tightly encapsulating the data.  This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. ", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max. "}, "hash": "1b9112071446d5abca36f05f891cc962294c714da1d28abd416ca989b621dc10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b332a54d-251e-418d-9422-3dcc76ed22fe", "node_type": "1", "metadata": {"window": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. "}, "hash": "f8043a267c10d01e870527d3615f8846bcd8b2d622f3a6cf0a8cd0aaaa5a812d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. ", "mimetype": "text/plain", "start_char_idx": 25716, "end_char_idx": 26127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b332a54d-251e-418d-9422-3dcc76ed22fe", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b2817c4-2fbd-4f89-92f5-350611e23289", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This characteristic allows the method to capture correlations between attributes more effectively than the original K-Means IF, which treats dimensions independently during clustering.  The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max. "}, "hash": "e83b2c0ab917d328dcc62909fa257c3b3ca0beb88f2b69810801d48963f5c6dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c54a0465-0cb0-4f2e-8a14-00a7f3a5db19", "node_type": "1", "metadata": {"window": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases. ", "original_text": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. "}, "hash": "af6db82dfd9f8830cd072a8b1152b9a45756e6ff44b2d4b65567af88ed9ebc94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 26127, "end_char_idx": 27042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c54a0465-0cb0-4f2e-8a14-00a7f3a5db19", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases. ", "original_text": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b332a54d-251e-418d-9422-3dcc76ed22fe", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The anomaly scoring for a point is calculated using the same rules as in the original K-Means IF: at every node level, we calculate the membership score for the assigned cluster and average these scores across all trees.  Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n", "original_text": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF. "}, "hash": "61257a3ed6c221e6fac8d7327c2812f9c38e09040a7a034dfaf9aa5aebce49fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58687e71-7f3b-4b38-b32f-979afbfe6173", "node_type": "1", "metadata": {"window": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line. ", "original_text": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n"}, "hash": "645a08fd8a5bd59edd8eaa75e4092af835615c3d5c763d9133daec510177b302", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. ", "mimetype": "text/plain", "start_char_idx": 27042, "end_char_idx": 27358, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58687e71-7f3b-4b38-b32f-979afbfe6173", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line. ", "original_text": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c54a0465-0cb0-4f2e-8a14-00a7f3a5db19", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given the substantial algorithmic similarity to the original method\u2014differing primarily in the multi-dimensional projection step\u2014we omit a redundant explicit overview of the algorithm skeleton.\n\n **Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases. ", "original_text": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix. "}, "hash": "8454d1574f013ec798b11ba6fc9c12b6c0311fe7984f1c65a02fd5b72a221283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "217febcf-7e91-43ec-bad5-f5297e839593", "node_type": "1", "metadata": {"window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. ", "original_text": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. "}, "hash": "2d17c7e2a218949915ecc5eaa4d17ce20f11da323cabc9285af3da92257bd412", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n", "mimetype": "text/plain", "start_char_idx": 27358, "end_char_idx": 27511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "217febcf-7e91-43ec-bad5-f5297e839593", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. ", "original_text": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58687e71-7f3b-4b38-b32f-979afbfe6173", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 6: Create Generalized ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line. ", "original_text": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n"}, "hash": "d1cf0463291972190516aec1829f454b4640dfee5aecf76aa53a287fca29a028", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a7dcb9f-3e01-4091-a4e8-764f87540f69", "node_type": "1", "metadata": {"window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. ", "original_text": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. "}, "hash": "8b44fc9d1619b9119991ca861d12605888b1b9d9c959fef55438a81cd9657ff9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. ", "mimetype": "text/plain", "start_char_idx": 27511, "end_char_idx": 27755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a7dcb9f-3e01-4091-a4e8-764f87540f69", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. ", "original_text": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "217febcf-7e91-43ec-bad5-f5297e839593", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ N(0, I_d)\n5 w \u2190 u/||u||\u2082\n6 p_min \u2190 min{x^T \u00b7 w | x \u2208 X}\n7 p_max \u2190 max{x^T \u00b7 w | x \u2208 X}\n8 draw p ~ U(p_min, p_max)\n9 X_l \u2190 {x | x \u2208 X, x^T \u00b7 w \u2264 p}\n10 X_r \u2190 {x | x \u2208 X, x^T \u00b7 w > p}\n11 **return**\nITree{size \u2190 |X|,\nthresh \u2190 p,\nnormal \u2190 w,\nleft \u2190 createITree(X_l, l, lvl + 1),\nright \u2190 createITree(X_r, l, lvl + 1)}\n\n**Algorithm 7: Compute Anomaly Score GIF**\n**Data:** x - data point, iTree - generalized isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. ", "original_text": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace. "}, "hash": "82b670d1e70c5905aa0f34c54fe1a064dcf7f3d954a37487e78c5c05e051eb88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb53b34f-dc32-4902-8333-3bb5f3f2fc1c", "node_type": "1", "metadata": {"window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. ", "original_text": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n"}, "hash": "79c3649c9a17653d15bc23d1905a49011c99cc275608a937415fd44f8fce80b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. ", "mimetype": "text/plain", "start_char_idx": 27755, "end_char_idx": 27877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb53b34f-dc32-4902-8333-3bb5f3f2fc1c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. ", "original_text": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a7dcb9f-3e01-4091-a4e8-764f87540f69", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x,iTree,lvl,l)\n2 **if** iTree.size \u2264 1 or lvl = l **then**\n3 | **return** 1 + c(iTree.size)\n4 w \u2190 iTree.normal\n5 p \u2190 iTree.thresh\n6 **if** x^T \u00b7 w \u2264 p **then**\n7 | **return** 1 + score(x, iTree.left,lvl + 1,l)\n8 **return** 1 + score(x, iTree.right, lvl + 1,l)\n\n**Algorithm 8: Create K-Means ITree**\n**Data:** X - data points, l - max.  height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. ", "original_text": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space. "}, "hash": "baf872ca081a6bd94ec525aeb85babcabd37f8c5b3091eac36a33a54867286eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cca4bb13-9c51-4218-9904-1208b547263a", "node_type": "1", "metadata": {"window": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. ", "original_text": "To better visualize this methodology, consider the following cases. "}, "hash": "2eaf03d92cb974275b68f34a57a7550f13d10c4a3198a66e36c46943b5366557", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n", "mimetype": "text/plain", "start_char_idx": 27877, "end_char_idx": 28106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cca4bb13-9c51-4218-9904-1208b547263a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. ", "original_text": "To better visualize this methodology, consider the following cases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb53b34f-dc32-4902-8333-3bb5f3f2fc1c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height, lvl - current height\n**Result:** iTree - a node in the isolation tree\n1 **Function** createITree(X,l,lvl)\n2 **if** |X| \u2264 1 or lvl = l **then**\n3 | **return** iTree{size \u2190 |X|}\n4 draw u ~ U(0, 1)\n5 q \u2190 \u230ad \u00b7 q\u230b\n6 X' = {x^(q) | x \u2208 X}\n7 Let C be a set of sets representing the partitioning of dataset X into clusters with respect to X' using \"elbow\" rule and K-Means algorithm\n8 Let \u00b5 be the set of centroids of the resulted clusters\n9 Let radii be the set of radii of the resulted clusters\n10 child_nodes = \u00d8\n11 **for** i=1:k **do**\n12 | child_nodes \u2190 child_nodes \u222a {createITree(C_i, l, lvl +1)}\n13 **return** iTree{size \u2190 |X|,\nprojAtt \u2190 q,\ncentroids \u2190 \u00b5,\nradii \u2190 radii,\nchildren \u2190 child_nodes}\n\n### 3.2 Extended K-Means Isolation Forest\n\nThe Extended K-Means Isolation Forest algorithm represents another variation of the Isolation Forest framework, offering an alternative generalization to the K-Means IF.  The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. ", "original_text": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n"}, "hash": "34b98a467c2b23b63e9853f03515c734b6273c7c654d79f60a53883dc7278932", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6960f2ca-3803-459b-b6ab-9e145975c335", "node_type": "1", "metadata": {"window": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n", "original_text": "If k = 1, the initial dataset is projected onto a line. "}, "hash": "5d2c717e8c346c90f4aa4ba6ec7f17e77c7ac15951f74b0e2e4187999477bef6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To better visualize this methodology, consider the following cases. ", "mimetype": "text/plain", "start_char_idx": 28106, "end_char_idx": 28174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6960f2ca-3803-459b-b6ab-9e145975c335", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n", "original_text": "If k = 1, the initial dataset is projected onto a line. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cca4bb13-9c51-4218-9904-1208b547263a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The primary distinction of this method lies in its projection strategy: rather than projecting data onto a single axis (as in the original K-Means IF) or a subset of axes (as in Subspace K-Means IF), this method provides the flexibility to project data into a general subspace via a random normal projection matrix.  Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. ", "original_text": "To better visualize this methodology, consider the following cases. "}, "hash": "b36bd3181ba08dd29a2154603dcbc6b0ee4af7fb2719084efc1fb398ffcc64ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ace775d0-6ef8-4958-ab08-af07ab5f4a8e", "node_type": "1", "metadata": {"window": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. ", "original_text": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. "}, "hash": "99ac1c72b75a7249c434d4193b2b9848fecd4579367ad55ae81e91fe8467f43b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If k = 1, the initial dataset is projected onto a line. ", "mimetype": "text/plain", "start_char_idx": 28174, "end_char_idx": 28230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ace775d0-6ef8-4958-ab08-af07ab5f4a8e", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. ", "original_text": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6960f2ca-3803-459b-b6ab-9e145975c335", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Following this projection, the data points are clustered and scored using the same mechanisms employed in the previously discussed K-Means IF variants.\n\n Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n", "original_text": "If k = 1, the initial dataset is projected onto a line. "}, "hash": "0fe1826d71813c16eb33f5f140ae2b15123ba232e67b0c7d42c267360ec2644c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6af0cb52-dd88-406c-a69f-e5e27996bdc6", "node_type": "1", "metadata": {"window": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. ", "original_text": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. "}, "hash": "3d0daa3f0bbdbb2f25f9cc3118f8a6d70e6355cf72405541ad4904cb309d527b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 28230, "end_char_idx": 28438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6af0cb52-dd88-406c-a69f-e5e27996bdc6", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. ", "original_text": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ace775d0-6ef8-4958-ab08-af07ab5f4a8e", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Formally, when constructing a node, we initialize a random matrix W \u2208 R^(d\u00d7k) (where elements are drawn from a standard normal distribution), with d denoting the dimensionality of the input data and k the dimensionality of the target subspace.  We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. ", "original_text": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF. "}, "hash": "26a99c5fb314051c9872d75fe2a53951fb54d94fa9802e54e11c96c32790ac3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d42e31ca-c60e-4d3a-b743-7d4ca04e55fc", "node_type": "1", "metadata": {"window": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. ", "original_text": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. "}, "hash": "0815bf84dc9676d3e89909c6a8840cde137f341b5edcec74f3ac7250910ad576", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. ", "mimetype": "text/plain", "start_char_idx": 28438, "end_char_idx": 28705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d42e31ca-c60e-4d3a-b743-7d4ca04e55fc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. ", "original_text": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6af0cb52-dd88-406c-a69f-e5e27996bdc6", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We then compute the dot product X' = X \u00b7 W \u2208 R^(n\u00d7k), which yields the projection of the initial data into the new space.  In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. ", "original_text": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier. "}, "hash": "f3599a55ebb7e115c4c03f3a3410cc4054b9b65ac6bd224c7510a149f8314908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd595804-47b8-4bea-88b8-75a0b5c0df11", "node_type": "1", "metadata": {"window": "To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time. ", "original_text": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. "}, "hash": "5ebda1283d78932487a4b2aaaae8cd157f9d6f0823117495f05bf7f5b01bb4e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. ", "mimetype": "text/plain", "start_char_idx": 28705, "end_char_idx": 28832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd595804-47b8-4bea-88b8-75a0b5c0df11", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time. ", "original_text": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d42e31ca-c60e-4d3a-b743-7d4ca04e55fc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In this context, W acts as a weight matrix, quantifying the contribution of each component in the initial space to the components in the target space; effectively, W captures correlations between features in the original space.\n\n To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. ", "original_text": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space. "}, "hash": "0558fb5db32274f66d12102b25f8b9fa951062b5104f564c6145c76e4d91eb51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "620f912c-8302-400c-be8d-4f084efb1163", "node_type": "1", "metadata": {"window": "If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. ", "original_text": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n"}, "hash": "560ce2df9acd986c540112369d043713408ec1b6483b02194ad8dfbe56faba93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. ", "mimetype": "text/plain", "start_char_idx": 28832, "end_char_idx": 29002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "620f912c-8302-400c-be8d-4f084efb1163", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. ", "original_text": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd595804-47b8-4bea-88b8-75a0b5c0df11", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To better visualize this methodology, consider the following cases.  If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time. ", "original_text": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods. "}, "hash": "fc6eb578ee34921bcca70a874678fd44f90be614aac97a62ecb15ba137d3daeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc973df1-8575-4e7b-a643-cb131760caf1", "node_type": "1", "metadata": {"window": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. ", "original_text": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. "}, "hash": "b438f79fda286a39640f95499a0587d989d47b4217c791ccb9752dea13f8e101", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n", "mimetype": "text/plain", "start_char_idx": 29002, "end_char_idx": 29248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc973df1-8575-4e7b-a643-cb131760caf1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. ", "original_text": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "620f912c-8302-400c-be8d-4f084efb1163", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If k = 1, the initial dataset is projected onto a line.  Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. ", "original_text": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n"}, "hash": "56065bc0125d140fd3bb2db5f42061f71ccc1fb2102d1b5552ac98921e65cca9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "771257a7-05db-45b5-bfc4-8e8da61ce53b", "node_type": "1", "metadata": {"window": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. "}, "hash": "a9db0c8ddde2db3afea14f6b93bf26ce34f5abca2cbe37185c9b78f12addf5fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. ", "mimetype": "text/plain", "start_char_idx": 29248, "end_char_idx": 29395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "771257a7-05db-45b5-bfc4-8e8da61ce53b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc973df1-8575-4e7b-a643-cb131760caf1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, if W = [e_i], where e_i \u2208 R^d is a standard basis vector, the algorithm simply projects the data onto the i-th component, rendering the Extended K-Means IF equivalent to the original K-Means IF.  If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. ", "original_text": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max. "}, "hash": "727ac64778cbb8b1485788fc0a4d05711d7ca6d55b9d0d87b3219d97139073d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db25b647-29a7-4448-9577-9a4d479de4fc", "node_type": "1", "metadata": {"window": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. ", "original_text": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. "}, "hash": "c0e5516bef7717cc32848a354a369a1b0defa834b64177a0c2a3c99a2f4f2157", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 29395, "end_char_idx": 30169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db25b647-29a7-4448-9577-9a4d479de4fc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. ", "original_text": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "771257a7-05db-45b5-bfc4-8e8da61ce53b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "If W is composed of selected basis vectors W = [e_i1, e_i2, ..., e_ik], with distinct indices i1, ..., ik \u2264 d, the algorithm projects the data into the subspace spanned by these specific components, making it equivalent to the Subspace K-Means IF introduced earlier.  Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods. ", "original_text": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF. "}, "hash": "7c6ba51f194643f0b2baedcdf4907524936e9d9e1c5426531baf737cef16e248", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6067cadb-b4b8-4b68-b629-e90d9b28f31e", "node_type": "1", "metadata": {"window": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. ", "original_text": "The performance metrics considered include ROC-AUC, PR-AUC, and training time. "}, "hash": "f82a41b3a6cf92be28dd49709cd4854765dcfd002829ef6dcf39e1af4adbc80c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. ", "mimetype": "text/plain", "start_char_idx": 30169, "end_char_idx": 30275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6067cadb-b4b8-4b68-b629-e90d9b28f31e", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. ", "original_text": "The performance metrics considered include ROC-AUC, PR-AUC, and training time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db25b647-29a7-4448-9577-9a4d479de4fc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Finally, if W is chosen at random and k = d, the newly projected data represents a rotation of the data in the original space.  Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. ", "original_text": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2. "}, "hash": "b97ce2a4336205c4df5896eb2d83f12c10e7699cde61584c6f66e1798a2f9ef1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd31bb9f-2402-4b3d-ac4b-c8914af1cdcc", "node_type": "1", "metadata": {"window": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). ", "original_text": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. "}, "hash": "f8e466444c51e1ed9ef25efc8d21c294afd362c55d6c2f6d95d5dabc2a733711", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The performance metrics considered include ROC-AUC, PR-AUC, and training time. ", "mimetype": "text/plain", "start_char_idx": 30275, "end_char_idx": 30354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd31bb9f-2402-4b3d-ac4b-c8914af1cdcc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). ", "original_text": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6067cadb-b4b8-4b68-b629-e90d9b28f31e", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Once the data is projected, we simply apply the K-Means clustering mechanics (utilizing the elbow rule) and the same scoring function defined for the K-Means IF methods.  Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. ", "original_text": "The performance metrics considered include ROC-AUC, PR-AUC, and training time. "}, "hash": "65f8e15abe01a7e23c76ac6b9d28220d292702edd573c795fae78467fed2f3a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41d61a86-41c8-48c1-a545-870b5e794680", "node_type": "1", "metadata": {"window": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n", "original_text": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. "}, "hash": "0c43eecc0dc4370317199303d0490619d405cc7b7908eabb97805ae3f8aa0b32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. ", "mimetype": "text/plain", "start_char_idx": 30354, "end_char_idx": 30565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41d61a86-41c8-48c1-a545-870b5e794680", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n", "original_text": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd31bb9f-2402-4b3d-ac4b-c8914af1cdcc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given the structural similarity to the prior algorithms\u2014differing only in the selection of the projection matrix and the data transformation step\u2014we omit a redundant algorithmic skeleton, as the generalization is straightforward for the reader.\n\n **Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). ", "original_text": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees. "}, "hash": "9b04b61e94dfa1b59026787a691608c1460bd30087e9a8ec64baf3385c134621", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d728a34e-c0b8-4a3e-ab47-0a7a35db3a04", "node_type": "1", "metadata": {"window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont. ", "original_text": "However, the subsampling size and maximum tree depth remained consistent with the other methods. "}, "hash": "6fc79980539d21dac33b752b7b641827b027cb9e0df74cc72807ee4e768b2793", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. ", "mimetype": "text/plain", "start_char_idx": 30565, "end_char_idx": 30770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d728a34e-c0b8-4a3e-ab47-0a7a35db3a04", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont. ", "original_text": "However, the subsampling size and maximum tree depth remained consistent with the other methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41d61a86-41c8-48c1-a545-870b5e794680", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Algorithm 9: Compute Anomaly Score K-Means IF**\n**Data:** x - data point, iTree - K-Means isolation tree, lvl - height of current node, l - max.  height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n", "original_text": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference. "}, "hash": "0e5837ea0ce97b96af18f7d6e7608248b24cfd23f3b989f680cf56856a94ffef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e78c87d-6ddc-4821-b3e4-f5f7bd9815c6", "node_type": "1", "metadata": {"window": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets.", "original_text": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. "}, "hash": "5491c52f2ae6f652cb2de928fb27fc86b6dd06da4ac128a530f058a729e25987", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the subsampling size and maximum tree depth remained consistent with the other methods. ", "mimetype": "text/plain", "start_char_idx": 30770, "end_char_idx": 30867, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e78c87d-6ddc-4821-b3e4-f5f7bd9815c6", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets.", "original_text": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d728a34e-c0b8-4a3e-ab47-0a7a35db3a04", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "height\n**Result:** score - anomaly score of data point\n1 **Function** score(x, iTree,lvl,l)\n2 **if** iTree.size < 1 or lvl = l **then**\n3 | **return** 0\n4 child_nodes \u2190 iTree.children\n5 q \u2190 iTree.projAtt\n6 radii \u2190 iTree.radii\n7 \u00b5 \u2190 iTree.centroids\n8 best_score \u2190 -\u221e\n9 best_child \u2190 \u00d8\n10 **for** i = 1 : length(child_nodes) **do**\n11 | s \u2190 1 - |x^(q) - \u00b5_i| / radii_i\n12 | **if** s > best_score **then**\n13 | | best_score \u2190 s\n14 | | best_child \u2190 child_nodes_i\n15 **return** best_score + score(x,best_child, lvl + 1,l)\n\n## 4 Experiments and Results\n\nIn this section, we present the experimental results obtained using Standard Isolation Forest (IF), Extended IF (EIF), Generalized IF (GIF), K-Means IF, and our proposed variations: Subspace K-Means IF and Extended K-Means IF.  We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont. ", "original_text": "However, the subsampling size and maximum tree depth remained consistent with the other methods. "}, "hash": "a263b4a1d1f2fbd875649f076ca31f78e456c952eed049f39bedf7895b77066a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90bf25a3-2864-4c2b-a853-92555a337596", "node_type": "1", "metadata": {"window": "The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n", "original_text": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. "}, "hash": "10b72765477dd31d09a066422053fe510994fbf12b95e24a607fcddaafc754d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. ", "mimetype": "text/plain", "start_char_idx": 30867, "end_char_idx": 31019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90bf25a3-2864-4c2b-a853-92555a337596", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n", "original_text": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e78c87d-6ddc-4821-b3e4-f5f7bd9815c6", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We evaluated these methods on 13 benchmark datasets from the ODDS library, which are detailed in Table 2.  The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets.", "original_text": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2. "}, "hash": "4ec8239216ebbad4d0eb5a253d47f656c93db6a65a1d49f8e05ae48b28c2fe4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "090cbb4c-bcdf-43af-9f47-13f0f065a6c0", "node_type": "1", "metadata": {"window": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n", "original_text": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). "}, "hash": "8df93fa750e9157063593bf2015b58ba727812913616f599b4f1152d518414db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. ", "mimetype": "text/plain", "start_char_idx": 31019, "end_char_idx": 31136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "090cbb4c-bcdf-43af-9f47-13f0f065a6c0", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n", "original_text": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90bf25a3-2864-4c2b-a853-92555a337596", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The performance metrics considered include ROC-AUC, PR-AUC, and training time.  For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n", "original_text": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds. "}, "hash": "3904b8ffdb5fc67b3365faa13156d5e2d6bdc87b48df8e8c61fe4abc23cab878", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d45727f-0fc4-4910-aa91-53227777ccf1", "node_type": "1", "metadata": {"window": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n", "original_text": "The results are illustrated in Figures 2, 3, and 4.\n\n"}, "hash": "01ad087ed7c9a6cc06d96fa9921d5619a053bd0540ff10afaecf3fb4cd904a55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). ", "mimetype": "text/plain", "start_char_idx": 31136, "end_char_idx": 31253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d45727f-0fc4-4910-aa91-53227777ccf1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n", "original_text": "The results are illustrated in Figures 2, 3, and 4.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "090cbb4c-bcdf-43af-9f47-13f0f065a6c0", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For the Standard IF, EIF, GIF, and K-Means IF algorithms, we adopted the parameter settings listed in Table 1, specifically a subsampling size of 256, a maximum tree height of 8, and a forest size of 100 trees.  In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n", "original_text": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range). "}, "hash": "0468949bb6c8dcb4e859ad19fe9663a8370a7db28fb1294131d628a6bfdaeac9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aea0b6c-45ff-4c4e-b7f0-c68e415dbba4", "node_type": "1", "metadata": {"window": "However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). ", "original_text": "| Dataset | Samples | Features | Cont. "}, "hash": "4cc25185e6502ccacb70fe5715d45d96fd877fbebef6d9c8883bb385619471c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results are illustrated in Figures 2, 3, and 4.\n\n", "mimetype": "text/plain", "start_char_idx": 31253, "end_char_idx": 31306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3aea0b6c-45ff-4c4e-b7f0-c68e415dbba4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). ", "original_text": "| Dataset | Samples | Features | Cont. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d45727f-0fc4-4910-aa91-53227777ccf1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In contrast, for experiments involving K-Means Based Methods, the ensemble size was reduced to 10 estimators due to the significantly higher computational cost associated with both training and inference.  However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n", "original_text": "The results are illustrated in Figures 2, 3, and 4.\n\n"}, "hash": "56e0a0a80aa29ca222e0d3c0ea955ba273cfb99ea8d847800a5a6a7cc943abb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62ab6aa5-6dd3-41fb-b4a7-16d2cea4237a", "node_type": "1", "metadata": {"window": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n", "original_text": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets."}, "hash": "67a43827b399cb5906c039abaa145dd9c5f89ec120c539f2b43d67115c577379", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | Samples | Features | Cont. ", "mimetype": "text/plain", "start_char_idx": 31306, "end_char_idx": 31345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62ab6aa5-6dd3-41fb-b4a7-16d2cea4237a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n", "original_text": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aea0b6c-45ff-4c4e-b7f0-c68e415dbba4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "However, the subsampling size and maximum tree depth remained consistent with the other methods.  The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). ", "original_text": "| Dataset | Samples | Features | Cont. "}, "hash": "8fb6ff50ed50526b93f625f85df17d3290d79fba265674661050784cb8849b4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d06d51ea-fada-4d3e-8380-cbd11d029eed", "node_type": "1", "metadata": {"window": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets. ", "original_text": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n"}, "hash": "6830d8593012fdca7ae30ba0b165111c721935d2c43e12a2bf2e1044159c013d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets.", "mimetype": "text/plain", "start_char_idx": 31345, "end_char_idx": 31939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d06d51ea-fada-4d3e-8380-cbd11d029eed", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets. ", "original_text": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62ab6aa5-6dd3-41fb-b4a7-16d2cea4237a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The maximum number of clusters allowed at a tree node was set to 5, implying that a tree in-node could have a maximum of 5 children and a minimum of 2.  Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n", "original_text": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets."}, "hash": "d4472a39f9dc6fd30dc62c82a9eeae4fe93f558f309c9698fdccfb40d5112605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d400ece-8a30-4509-810e-b17bf54b471b", "node_type": "1", "metadata": {"window": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. ", "original_text": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n"}, "hash": "98d3074183a028316d1032027901b9c78c294f70ed1648af23b23aba0969c050", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n", "mimetype": "text/plain", "start_char_idx": 31939, "end_char_idx": 32059, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d400ece-8a30-4509-810e-b17bf54b471b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. ", "original_text": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d06d51ea-fada-4d3e-8380-cbd11d029eed", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Given the stochastic nature of tree construction, each experiment was repeated 50 times with different random seeds.  We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets. ", "original_text": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n"}, "hash": "17219c93caf2c662bba18657d1ef1178015b1bb6d1d080c487adba70df125238", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44f40515-ebc6-4723-90e9-92a7581b4237", "node_type": "1", "metadata": {"window": "The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation. ", "original_text": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n"}, "hash": "e65295694458c678cc50a90531c8aa1d273738f4a8933a4b45df7efe368bd4c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n", "mimetype": "text/plain", "start_char_idx": 32059, "end_char_idx": 32128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44f40515-ebc6-4723-90e9-92a7581b4237", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation. ", "original_text": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d400ece-8a30-4509-810e-b17bf54b471b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We report the mean performance along with the 95% confidence interval (reflecting the 2.5% to 97.5% quantile range).  The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. ", "original_text": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n"}, "hash": "a6a9ff6b9f42ab0b3402b554425477d0b0f61773f511c448858c8ee31b74466c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea27f806-3162-43d2-880a-9d68e3440dbf", "node_type": "1", "metadata": {"window": "| Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n", "original_text": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). "}, "hash": "1295493f829dbe72546fac696be69c796eb4fa7e19cab34bc3e1db918b8091a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n", "mimetype": "text/plain", "start_char_idx": 32128, "end_char_idx": 32197, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea27f806-3162-43d2-880a-9d68e3440dbf", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "| Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n", "original_text": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44f40515-ebc6-4723-90e9-92a7581b4237", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The results are illustrated in Figures 2, 3, and 4.\n\n | Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation. ", "original_text": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n"}, "hash": "4bfda629f0b3e785ffbac1e13c9f76d298c7ad1a18ffda07c4329658c9f47c9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53a823c8-72d4-4c18-af8a-12df6f921343", "node_type": "1", "metadata": {"window": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets.", "original_text": "Error bars represent the 95% confidence intervals from 50 runs.\n"}, "hash": "796e28480f64ec8af667e2da4fa586b22a3deb0bdfd5dd5d1dfc6961f5b3f0f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). ", "mimetype": "text/plain", "start_char_idx": 32197, "end_char_idx": 32460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "53a823c8-72d4-4c18-af8a-12df6f921343", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets.", "original_text": "Error bars represent the 95% confidence intervals from 50 runs.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea27f806-3162-43d2-880a-9d68e3440dbf", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "| Dataset | Samples | Features | Cont.  |\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n", "original_text": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`). "}, "hash": "e7af275d8267678c6f9af79939a1861f04091b4cd901d4d57a794940138f8236", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5356bc8a-0904-4ddf-aff6-0b1af3ae2517", "node_type": "1", "metadata": {"window": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n", "original_text": "- **Observations:** No single algorithm consistently outperforms all others across all datasets. "}, "hash": "7a55695f9d999887843768d9bd752a13b7af07d905d3c5bb43419bfc6270f0c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error bars represent the 95% confidence intervals from 50 runs.\n", "mimetype": "text/plain", "start_char_idx": 32460, "end_char_idx": 32524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5356bc8a-0904-4ddf-aff6-0b1af3ae2517", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n", "original_text": "- **Observations:** No single algorithm consistently outperforms all others across all datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53a823c8-72d4-4c18-af8a-12df6f921343", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "|\n| :--- | :--- | :--- | :--- |\n| Aloi | 50000 | 27 | 3.02 |\n| ANN Thyroid | 6916 | 21 | 3.61 |\n| Breast Cancer | 367 | 30 | 2.72 |\n| Cardio | 1831 | 21 | 9.61 |\n| Forest Cover | 286048 | 10 | 0.96 |\n| Ionosphere | 351 | 33 | 35.90 |\n| Letter | 1600 | 32 | 6.25 |\n| Mammography | 11183 | 6 | 2.32 |\n| Pen Gloabl | 809 | 16 | 11.12 |\n| Pen Local | 6724 | 16 | 0.15 |\n| Satellite | 6435 | 36 | 31.64 |\n| Shuttle | 46464 | 9 | 1.89 |\n| Speech | 3686 | 400 | 1.65 |\n\n**Table 2:** ODDS benchmark datasets stats\n\n**Figure 2: Comparative analysis of ROC-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets.", "original_text": "Error bars represent the 95% confidence intervals from 50 runs.\n"}, "hash": "13550cb0c59d0256ec91559840f8af5e1d17903769d7fa731fc16cd196dbec43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a12ae13-af83-4f0f-9db0-f1108e0ba753", "node_type": "1", "metadata": {"window": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n", "original_text": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. "}, "hash": "f40fd5499ee28c0e122e67b48ef3df86fb09b06cba2db3bcdc1f1f01e41ea70a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:** No single algorithm consistently outperforms all others across all datasets. ", "mimetype": "text/plain", "start_char_idx": 32524, "end_char_idx": 32621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a12ae13-af83-4f0f-9db0-f1108e0ba753", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n", "original_text": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5356bc8a-0904-4ddf-aff6-0b1af3ae2517", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a dot plot showing the mean ROC-AUC scores for six different algorithms across 13 benchmark datasets.\n - **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n", "original_text": "- **Observations:** No single algorithm consistently outperforms all others across all datasets. "}, "hash": "2aed31fc6bec580c74e3585b531f5677bead49e2c948920f557f0299f8c00901", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a75c470-3fdb-42a2-929b-cbca27fba653", "node_type": "1", "metadata": {"window": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n", "original_text": "For others like `Speech` and `Pen Global`, there is more variation. "}, "hash": "5c54de43b1ae14d04c10fabe303890b35b29b283790ecb41e823d11e83056a05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. ", "mimetype": "text/plain", "start_char_idx": 32621, "end_char_idx": 32716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a75c470-3fdb-42a2-929b-cbca27fba653", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n", "original_text": "For others like `Speech` and `Pen Global`, there is more variation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a12ae13-af83-4f0f-9db0-f1108e0ba753", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Y-axis:** Lists the datasets (Speech, Shuttle, Satellite, etc.).\n - **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n", "original_text": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC. "}, "hash": "b4164fc3a7264671406045a5541036b9fb96f7289cbc03a2ab6be7ea6d32a6f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b065ccd-2dee-4715-bc4b-d49660e8fc64", "node_type": "1", "metadata": {"window": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n", "original_text": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n"}, "hash": "e0e4b527701bf4ad2b217d1d7794271236848936829569adb3bce5a786ccc6bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For others like `Speech` and `Pen Global`, there is more variation. ", "mimetype": "text/plain", "start_char_idx": 32716, "end_char_idx": 32784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b065ccd-2dee-4715-bc4b-d49660e8fc64", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n", "original_text": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a75c470-3fdb-42a2-929b-cbca27fba653", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **X-axis:** Represents the ROC-AUC score, ranging from 0.3 to 1.0.\n - **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n", "original_text": "For others like `Speech` and `Pen Global`, there is more variation. "}, "hash": "121bbdfdd3547f785b84d182d09850dcb243f8b64ac4e188dea4d6615a9c92b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2886475-83c2-4b23-97e4-7ac2ffa7fc9c", "node_type": "1", "metadata": {"window": "Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. ", "original_text": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets."}, "hash": "0c13b1371a8655025ac9be8c2f24fbd360b99b4cd5a2d1616d4686de75ef7281", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n", "mimetype": "text/plain", "start_char_idx": 32784, "end_char_idx": 32883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2886475-83c2-4b23-97e4-7ac2ffa7fc9c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. ", "original_text": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b065ccd-2dee-4715-bc4b-d49660e8fc64", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** For each dataset, there are six colored dots, each representing the mean ROC-AUC score of an algorithm (`Standard IF`, `Generalized IF`, `Extended IF`, `K-Means IF`, `K-Means IF Random Hyperplane` [Subspace K-Means IF], and `Extended K-Means IF`).  Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n", "original_text": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n"}, "hash": "acb78e2f67efdc65abd05aff16df6bda58f2f064c07f86603a86a2ebd023fc1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1555276f-c1a5-4cf0-8867-f5f0aa5a342c", "node_type": "1", "metadata": {"window": "- **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. ", "original_text": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n"}, "hash": "c937b399ad93c1357fcf055fb93abc9e039016f5f5278cdd3cfa8f0dfea67c59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets.", "mimetype": "text/plain", "start_char_idx": 32883, "end_char_idx": 32969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1555276f-c1a5-4cf0-8867-f5f0aa5a342c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. ", "original_text": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2886475-83c2-4b23-97e4-7ac2ffa7fc9c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Error bars represent the 95% confidence intervals from 50 runs.\n - **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. ", "original_text": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets."}, "hash": "c275e0b7106e1c5540632500bf8841dab57c4cd2caf4ce232a79b97955967eb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0ee2149-41ba-4bde-b66f-20259fb61920", "node_type": "1", "metadata": {"window": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n", "original_text": "- **Y-axis:** Lists the 13 benchmark datasets.\n"}, "hash": "2c85cf691fa4d7b5c82299bc2db79209a6e25aac6ad987548792678b6df71184", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n", "mimetype": "text/plain", "start_char_idx": 32969, "end_char_idx": 33079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0ee2149-41ba-4bde-b66f-20259fb61920", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n", "original_text": "- **Y-axis:** Lists the 13 benchmark datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1555276f-c1a5-4cf0-8867-f5f0aa5a342c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:** No single algorithm consistently outperforms all others across all datasets.  For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. ", "original_text": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n"}, "hash": "af55797b5e3c21f5cf3c21f38f41e1501861585a566caed12518ad410356e67c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "babe2bac-4eb7-4313-88d5-c1a2611f1330", "node_type": "1", "metadata": {"window": "For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets.", "original_text": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n"}, "hash": "61b4a50b3c31732ff019c7f8d789a5d160e4a0e26208f20c1d30970f893bd9d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Y-axis:** Lists the 13 benchmark datasets.\n", "mimetype": "text/plain", "start_char_idx": 33079, "end_char_idx": 33126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "babe2bac-4eb7-4313-88d5-c1a2611f1330", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets.", "original_text": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0ee2149-41ba-4bde-b66f-20259fb61920", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For some datasets like `Shuttle` and `Satellite`, most methods perform well with high ROC-AUC.  For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n", "original_text": "- **Y-axis:** Lists the 13 benchmark datasets.\n"}, "hash": "0209670721de0704cc98d897de76735ae0c529b396476bf92884b5234cf30aba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfa231bf-d0bf-4164-ac6e-f9b89faa8e5d", "node_type": "1", "metadata": {"window": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n", "original_text": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n"}, "hash": "9cfcc6084d72dd85f2059b17db87d5b2de78a78a23428f0e9e264317f7209a43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n", "mimetype": "text/plain", "start_char_idx": 33126, "end_char_idx": 33194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfa231bf-d0bf-4164-ac6e-f9b89faa8e5d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n", "original_text": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "babe2bac-4eb7-4313-88d5-c1a2611f1330", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For others like `Speech` and `Pen Global`, there is more variation.  The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets.", "original_text": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n"}, "hash": "7c7ad281ffb56e81a186227ada75783ae9c6b8604db14fd5ce2721ed873268b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47405690-5cf1-448a-a4d5-fec2388edbf4", "node_type": "1", "metadata": {"window": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n", "original_text": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. "}, "hash": "b3345892ffafd2f4ab3e99425c13c4fc65d707cc424f2c7d0c3f3b6ae344db26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n", "mimetype": "text/plain", "start_char_idx": 33194, "end_char_idx": 33339, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47405690-5cf1-448a-a4d5-fec2388edbf4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n", "original_text": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfa231bf-d0bf-4164-ac6e-f9b89faa8e5d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The K-Means based methods sometimes show wider confidence intervals, indicating more variability.\n\n **Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n", "original_text": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n"}, "hash": "ae388c3dda2de15fd53b79d4c126a5c13424dc41ddc373182a82e27417e7ce98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d002aed0-0b46-4401-b8a0-21db7e6f9952", "node_type": "1", "metadata": {"window": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n", "original_text": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. "}, "hash": "ed40fda350732c7edfec72613cec52f21bd8e0a72f02afeb40afcd3e00e70035", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. ", "mimetype": "text/plain", "start_char_idx": 33339, "end_char_idx": 33457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d002aed0-0b46-4401-b8a0-21db7e6f9952", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n", "original_text": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47405690-5cf1-448a-a4d5-fec2388edbf4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 3: Comparative analysis of PR-AUC performance across ODDS benchmark datasets. **\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n", "original_text": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC. "}, "hash": "61b6d34b2954031a48252bd046a8e7d1c276cf3cb951c7a0fd31f9c5199e6c35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eefcdbbd-8daa-4fb9-b585-72798e2b2f97", "node_type": "1", "metadata": {"window": "- **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n", "original_text": "The relative performance of algorithms varies significantly from one dataset to another.\n\n"}, "hash": "1fd3cb7051789edb3244a5197b1f6bdf16a1ed69b99d7691fb0800566fbcfbd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. ", "mimetype": "text/plain", "start_char_idx": 33457, "end_char_idx": 33605, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eefcdbbd-8daa-4fb9-b585-72798e2b2f97", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n", "original_text": "The relative performance of algorithms varies significantly from one dataset to another.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d002aed0-0b46-4401-b8a0-21db7e6f9952", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a dot plot similar to Figure 2, but it shows the mean Precision-Recall AUC (PR-AUC) scores.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n", "original_text": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF. "}, "hash": "18ecdfdf3896be247c072dc8c914b632dc12c65a6f635e9f7b679ec25db34fb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5163c95-74be-4fc8-ade5-e054cfab47b2", "node_type": "1", "metadata": {"window": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time. ", "original_text": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets."}, "hash": "034dff0c33bfeaa1e261ef7f6a38e2efc35fe8263a31f77d3acbce717116086c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The relative performance of algorithms varies significantly from one dataset to another.\n\n", "mimetype": "text/plain", "start_char_idx": 33605, "end_char_idx": 33695, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5163c95-74be-4fc8-ade5-e054cfab47b2", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time. ", "original_text": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eefcdbbd-8daa-4fb9-b585-72798e2b2f97", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n", "original_text": "The relative performance of algorithms varies significantly from one dataset to another.\n\n"}, "hash": "611f3f9d3bbe5771590a84a86739a09795d0c83f78ae781bbcf9fe5051c1443a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34212689-e5b5-427e-a18b-73ebba63ec82", "node_type": "1", "metadata": {"window": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. ", "original_text": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n"}, "hash": "59a0c705b5ce11bef7188a97c430017b410bca8206f32e40885c6086db731947", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets.", "mimetype": "text/plain", "start_char_idx": 33695, "end_char_idx": 33785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34212689-e5b5-427e-a18b-73ebba63ec82", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. ", "original_text": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5163c95-74be-4fc8-ade5-e054cfab47b2", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **X-axis:** Represents the PR-AUC score, ranging from 0.0 to 1.0.\n - **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time. ", "original_text": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets."}, "hash": "6e5316610df3664c5e65cdb512b1a9dba835185ffe09584ae2e2c94c679a4f06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df4ab1af-80aa-47e6-975a-c92d7b2f88bb", "node_type": "1", "metadata": {"window": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n", "original_text": "- **Y-axis:** Lists the 13 benchmark datasets.\n"}, "hash": "c470a7793047a20b946a164818c9d67cbe607dec8bc1f63cc83fd5bb267b2ecd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n", "mimetype": "text/plain", "start_char_idx": 33785, "end_char_idx": 33903, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df4ab1af-80aa-47e6-975a-c92d7b2f88bb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n", "original_text": "- **Y-axis:** Lists the 13 benchmark datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34212689-e5b5-427e-a18b-73ebba63ec82", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** Each dataset has six colored dots representing the mean PR-AUC for each of the six algorithms, with 95% confidence interval bars.\n - **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. ", "original_text": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n"}, "hash": "b233c2e7e3103e10138f9cdba28be43fdcfd3982260228241143c17143c88fd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b2d496e-bf3a-4000-b8bc-e5e5a25e4de2", "node_type": "1", "metadata": {"window": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. ", "original_text": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n"}, "hash": "232d2170200a9c5622972998e87629e516719553c95e633c523cf5faabc9e8c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Y-axis:** Lists the 13 benchmark datasets.\n", "mimetype": "text/plain", "start_char_idx": 33903, "end_char_idx": 33950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b2d496e-bf3a-4000-b8bc-e5e5a25e4de2", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. ", "original_text": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df4ab1af-80aa-47e6-975a-c92d7b2f88bb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:** The performance differences between algorithms are more pronounced in PR-AUC compared to ROC-AUC.  For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n", "original_text": "- **Y-axis:** Lists the 13 benchmark datasets.\n"}, "hash": "2727c4deb8ac9ae7fb153b6dca1e34c51f3786f97b12951c523204a12ce49261", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4f0efd4-ada3-481a-830f-e2563b4cddfe", "node_type": "1", "metadata": {"window": "The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1. ", "original_text": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n"}, "hash": "632c5b6828f52359189e23fe6d5c0edf86c116727891bcb6ada33b485cbc0af2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n", "mimetype": "text/plain", "start_char_idx": 33950, "end_char_idx": 34039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b4f0efd4-ada3-481a-830f-e2563b4cddfe", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1. ", "original_text": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b2d496e-bf3a-4000-b8bc-e5e5a25e4de2", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For datasets like `Ionosphere` and `Forest Cover`, the K-Means based methods and Extended IF variants show a noticeable advantage over Standard IF.  The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. ", "original_text": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n"}, "hash": "a7a1f129041c093fe5206099bff59c68d8c26191a1c1ef8027171d32df19c319", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "399431a4-5089-4d33-aa49-ba5954b046a6", "node_type": "1", "metadata": {"window": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. ", "original_text": "- **Observations:** There is a clear separation in training time. "}, "hash": "40365c91e3e8b676e306ca897628979869ed04134ada2126acee4d8b8ed1939e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n", "mimetype": "text/plain", "start_char_idx": 34039, "end_char_idx": 34172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "399431a4-5089-4d33-aa49-ba5954b046a6", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. ", "original_text": "- **Observations:** There is a clear separation in training time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4f0efd4-ada3-481a-830f-e2563b4cddfe", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The relative performance of algorithms varies significantly from one dataset to another.\n\n **Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1. ", "original_text": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n"}, "hash": "4c955b8e6def8b94d346a46130e0a455f7fec262f89e2f135946aded408145ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89a956a1-c371-4015-aabb-771efa254f92", "node_type": "1", "metadata": {"window": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n", "original_text": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. "}, "hash": "3004d6790cb3e84243e3ad6d566810717ebf0e90925d067505c84f02becf447d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:** There is a clear separation in training time. ", "mimetype": "text/plain", "start_char_idx": 34172, "end_char_idx": 34238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "89a956a1-c371-4015-aabb-771efa254f92", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n", "original_text": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "399431a4-5089-4d33-aa49-ba5954b046a6", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 4: Comparative analysis of training time duration across ODDS benchmark datasets. **\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. ", "original_text": "- **Observations:** There is a clear separation in training time. "}, "hash": "24db6bc87c3047acafb315dc500229062c565681937e2bba9ceaaec6bde96612", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6f52feb-27ad-46eb-a87d-f44f2e0a74b3", "node_type": "1", "metadata": {"window": "- **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. ", "original_text": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n"}, "hash": "e5b882be74398ef7c37c71e5e71222a9bd3cc314b04a15ab542ddb1e916f9293", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. ", "mimetype": "text/plain", "start_char_idx": 34238, "end_char_idx": 34341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f6f52feb-27ad-46eb-a87d-f44f2e0a74b3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. ", "original_text": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89a956a1-c371-4015-aabb-771efa254f92", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a dot plot showing the mean training time in seconds for the six algorithms across the 13 datasets.\n - **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n", "original_text": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds. "}, "hash": "42f88fee96dce2a907d348fbe9479f47336366e71e86a49a7c6213a15179f266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cc000af-0af4-400a-9bb4-07e3cc64c601", "node_type": "1", "metadata": {"window": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. ", "original_text": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. "}, "hash": "9bb60708355dcb4624bb20060dc67ac84e98edbc7fda3446eb606a006be4b87b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 34341, "end_char_idx": 34592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cc000af-0af4-400a-9bb4-07e3cc64c601", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. ", "original_text": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6f52feb-27ad-46eb-a87d-f44f2e0a74b3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Y-axis:** Lists the 13 benchmark datasets.\n - **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. ", "original_text": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n"}, "hash": "68a36a7e01ab8dba200704a31669d48702a2634f4ef352749f4c339296d89388", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "148310b3-a15c-42ae-85e6-44177c20c432", "node_type": "1", "metadata": {"window": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. ", "original_text": "The resulting anomaly score heatmaps are presented in Figure 1. "}, "hash": "c1d5bcb0ef8234fb2e6be76423560961e5f2d939fbdd7a3759b69c824c22bba6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. ", "mimetype": "text/plain", "start_char_idx": 34592, "end_char_idx": 34828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "148310b3-a15c-42ae-85e6-44177c20c432", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. ", "original_text": "The resulting anomaly score heatmaps are presented in Figure 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cc000af-0af4-400a-9bb4-07e3cc64c601", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **X-axis:** Represents the training time in seconds, on a linear scale from 0 to 1000.\n - **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. ", "original_text": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters. "}, "hash": "ef1a2b26f49908f5041a9618eb8692f762b88c85bc4d28a6e0e5de554098cac1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3189d7b1-36f2-4aa3-9207-91ec3ea8bc89", "node_type": "1", "metadata": {"window": "- **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. ", "original_text": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. "}, "hash": "7dd3bfc7eff081d69b7437bc566166b4b447f027f53b1592c8878820b3f7c87b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The resulting anomaly score heatmaps are presented in Figure 1. ", "mimetype": "text/plain", "start_char_idx": 34828, "end_char_idx": 34892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3189d7b1-36f2-4aa3-9207-91ec3ea8bc89", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. ", "original_text": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "148310b3-a15c-42ae-85e6-44177c20c432", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Content:** For each dataset, six colored dots show the mean training time for each algorithm, with 95% confidence interval bars.\n - **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. ", "original_text": "The resulting anomaly score heatmaps are presented in Figure 1. "}, "hash": "e83cfdad4d5fcc0d237fa835df76a4a93867d9a949723e65cc39c5947e9a12b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c64c28e-a51a-41c6-9d9e-54ba059307fc", "node_type": "1", "metadata": {"window": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. ", "original_text": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n"}, "hash": "d5a49f762574b5d267698c968de629d96a0448d1f5f88a0623497bcd6e30ed5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. ", "mimetype": "text/plain", "start_char_idx": 34892, "end_char_idx": 35109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c64c28e-a51a-41c6-9d9e-54ba059307fc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. ", "original_text": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3189d7b1-36f2-4aa3-9207-91ec3ea8bc89", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:** There is a clear separation in training time.  `Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. ", "original_text": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF. "}, "hash": "cf2398a7248f778c597fff79d8a3c06032591742386c912f88fc2b5531aafa79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "456fb6ef-06f4-4658-a1ef-08626cba444f", "node_type": "1", "metadata": {"window": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. ", "original_text": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. "}, "hash": "366e39741dc06ab3feae442237356a00fa3d2d23518706c24fa8a48de66d212a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n", "mimetype": "text/plain", "start_char_idx": 35109, "end_char_idx": 35356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "456fb6ef-06f4-4658-a1ef-08626cba444f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. ", "original_text": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c64c28e-a51a-41c6-9d9e-54ba059307fc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "`Standard IF`, `Generalized IF`, and `Extended IF` are very fast, typically training in a few seconds.  In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. ", "original_text": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n"}, "hash": "18e61ae07a8a0f8810c7c603582fee907efbb1d65cb7d626b3d99e58055666f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f622cc31-be5e-4ee7-9193-493f71892da5", "node_type": "1", "metadata": {"window": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n", "original_text": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. "}, "hash": "089cffc00757d7d0aae970cf4ca7bb5ab16ecacb978761b9bff44f13acbc89ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. ", "mimetype": "text/plain", "start_char_idx": 35356, "end_char_idx": 35468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f622cc31-be5e-4ee7-9193-493f71892da5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n", "original_text": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "456fb6ef-06f4-4658-a1ef-08626cba444f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In contrast, the three K-Means based variants (`K-Means IF`, `K-Means IF Random Hyperplane`, and `Extended K-Means IF`) are significantly slower, with training times often an order of magnitude higher, reaching hundreds of seconds for some datasets.\n\n We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. ", "original_text": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions. "}, "hash": "052c47717d7cf5dca4704f66774de791708d640514589ac1a9b0e3eb192a24cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1ab1501-c83f-4e2f-8f18-e4a485fff75f", "node_type": "1", "metadata": {"window": "The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. ", "original_text": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. "}, "hash": "810960213ece1afb78191c53fa8d47369ca95cf665ce6e58e8ced23940463ba2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. ", "mimetype": "text/plain", "start_char_idx": 35468, "end_char_idx": 35704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1ab1501-c83f-4e2f-8f18-e4a485fff75f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. ", "original_text": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f622cc31-be5e-4ee7-9193-493f71892da5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "We further evaluated the methods against diverse synthetic datasets drawn from multiple distributions, including single-cluster blobs, double-cluster blobs, sinusoidal shapes, spirals, and helicoidal structures with varying parameters.  The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n", "original_text": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores. "}, "hash": "1efada6951b725d5c9e5d6d2e813238d6e7732539431e232d1c711f771858d71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2614ec5e-1da4-4438-b630-5d00f4c59e43", "node_type": "1", "metadata": {"window": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. ", "original_text": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. "}, "hash": "975467fe93e75271a0f9e02fbb1a5fd4c861e862c512f167ae74576f106a0a6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. ", "mimetype": "text/plain", "start_char_idx": 35704, "end_char_idx": 35878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2614ec5e-1da4-4438-b630-5d00f4c59e43", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. ", "original_text": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1ab1501-c83f-4e2f-8f18-e4a485fff75f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The resulting anomaly score heatmaps are presented in Figure 1.  As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. ", "original_text": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts. "}, "hash": "18202454a744377bf5ac656b594c4d82d35bfb691b4cde938dd120460507ec45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d44ff002-7d89-4f0d-baf1-b10d1fb99ae7", "node_type": "1", "metadata": {"window": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. ", "original_text": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. "}, "hash": "12e8c94b254e4d2c04c28466e1fc1742c3c28be6139f338705fb06caf7ffb504", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. ", "mimetype": "text/plain", "start_char_idx": 35878, "end_char_idx": 36047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d44ff002-7d89-4f0d-baf1-b10d1fb99ae7", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. ", "original_text": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2614ec5e-1da4-4438-b630-5d00f4c59e43", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "As observed, the K-Means IF variants generally exhibit a superior fit to the underlying data distributions, with Subspace K-Means IF demonstrating the most robust performance, followed closely by Extended K-Means IF.  Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. ", "original_text": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root. "}, "hash": "d39353c0a9147c0405ff3141935c358ca1f76af4c0e629f44b09f40d9bf37fbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a883a263-67cb-4863-b099-231fa7868e8a", "node_type": "1", "metadata": {"window": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n", "original_text": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. "}, "hash": "8d04d33998ce0b3275989db5b693d9cc57fede44a471c9f20706f5507f812ed5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. ", "mimetype": "text/plain", "start_char_idx": 36047, "end_char_idx": 36210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a883a263-67cb-4863-b099-231fa7868e8a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n", "original_text": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d44ff002-7d89-4f0d-baf1-b10d1fb99ae7", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Notably, because the original K-Means IF applies clustering strictly along single dimensions, linear artifacts remain visible in its score maps, though these are less pronounced than the axis-parallel artifacts characteristic of the Standard IF.\n\n Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. ", "original_text": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures. "}, "hash": "51e6feb3711ac3d1dbce8de5e33f93b5eccc0269c2e7f8f888795e6b2d9d1c51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e05385c-09ed-4ec1-9bf1-5dc179e6adc4", "node_type": "1", "metadata": {"window": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "original_text": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n"}, "hash": "af126033167a68a5dc474fc39d4f7e44cedadc61ea9e1e4f436e9c3497e45766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. ", "mimetype": "text/plain", "start_char_idx": 36210, "end_char_idx": 36538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4e05385c-09ed-4ec1-9bf1-5dc179e6adc4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "original_text": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a883a263-67cb-4863-b099-231fa7868e8a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Figures 5, 6, 7, 8, 9, and 10 provide detailed visualizations for each method across the dataset distributions.  These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n", "original_text": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space. "}, "hash": "a348cd4e09e4c275cc44311086c4badbe4cbce4d674871cf48aa3ec469e63c7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e49ca396-fa53-47a3-9de5-c68040686d14", "node_type": "1", "metadata": {"window": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest. ", "original_text": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. "}, "hash": "cbf8d0154cf3edc5df1e6f5e9ac532b6997c990bd592c8c72e17a4eb84093cc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n", "mimetype": "text/plain", "start_char_idx": 36538, "end_char_idx": 36655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e49ca396-fa53-47a3-9de5-c68040686d14", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest. ", "original_text": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e05385c-09ed-4ec1-9bf1-5dc179e6adc4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "These figures depict the structure of a sample tree, the isolation path taken by a selected inlier and outlier, the constructed separation hyperplanes, and a radial view of the forest highlighting the top 100 inlier and outlier scores.  A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "original_text": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n"}, "hash": "c3f1f4dd01dcb22c12c21e9ab65c90c1260ca8298cc4cdfc595a8ae8323fe3e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c4f6513-8313-4ff4-9f25-4f7dabd8aedb", "node_type": "1", "metadata": {"window": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n", "original_text": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. "}, "hash": "da950cc70aafc3a951a023018079e8d78c25bbbabb36f227368b1a6e3929f824", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. ", "mimetype": "text/plain", "start_char_idx": 36655, "end_char_idx": 36863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c4f6513-8313-4ff4-9f25-4f7dabd8aedb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n", "original_text": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e49ca396-fa53-47a3-9de5-c68040686d14", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "A distinct structural difference is observable in the K-Means-based methods, which tend to generate shorter and wider trees compared to their random projection counterparts.  Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest. ", "original_text": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*. "}, "hash": "5495f469fef983b75d254540071d2236981a58b24e2b9704d468a7279149aed9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13a2dc98-8510-4f97-af53-34aa9b8ea51b", "node_type": "1", "metadata": {"window": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. ", "original_text": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. "}, "hash": "b11e5dbb837e0dd22998a40c24fe1045358051ad52ed6fba329b68764e1686d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. ", "mimetype": "text/plain", "start_char_idx": 36863, "end_char_idx": 36984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13a2dc98-8510-4f97-af53-34aa9b8ea51b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. ", "original_text": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c4f6513-8313-4ff4-9f25-4f7dabd8aedb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Furthermore, for IF, EIF, and GIF, the radial plots reveal a visible clustering of anomalies near the center, confirming that anomalies are isolated closer to the root.  Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n", "original_text": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions. "}, "hash": "2f78a04ce25ba0ae391821fa1847578a4728e6252755b2c7c406f12f7c32e66f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47b81c65-e583-4901-9aac-7029b6c24b15", "node_type": "1", "metadata": {"window": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest. ", "original_text": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n"}, "hash": "dbfb847fe1e6f6b973c9f0717100adb71c61e5bdec29e4341a29959067cc7a34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. ", "mimetype": "text/plain", "start_char_idx": 36984, "end_char_idx": 37162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47b81c65-e583-4901-9aac-7029b6c24b15", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest. ", "original_text": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13a2dc98-8510-4f97-af53-34aa9b8ea51b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Regarding the separation boundaries, the Subspace K-Means IF yields particularly interesting results, isolating data clusters via complex Voronoi cell structures.  While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. ", "original_text": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost. "}, "hash": "dfb4c4438cc0ec84f9279656c206b6c3d9e35ab68bff5286955a3256d3a786bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cf7d932-4d04-4f94-ba83-bd75ecb21c6f", "node_type": "1", "metadata": {"window": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n", "original_text": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. "}, "hash": "9d7989303ff8a0dc30a88ca2f17f7347a0f0755886ffb61e2b8e8c5f41b3c2e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n", "mimetype": "text/plain", "start_char_idx": 37162, "end_char_idx": 37458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cf7d932-4d04-4f94-ba83-bd75ecb21c6f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n", "original_text": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47b81c65-e583-4901-9aac-7029b6c24b15", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "While the separation boundaries for Extended K-Means IF are not explicitly plotted due to the complexity of visualizing high-dimensional projections, they are analytically similar to those of Subspace K-Means IF, differing only in that the Voronoi cells are formed in the projected space rather than the original feature space.  To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest. ", "original_text": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n"}, "hash": "88312c1463fa35c75426cbacde2b7427e5f36af0412f8d384ff522bd18630443", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc240197-3cb6-4307-adfa-110a434a6c12", "node_type": "1", "metadata": {"window": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "original_text": "Extended isolation forest. "}, "hash": "938d7309ba26300cfb89fd551c1e9fc7e0243faf2fd23f50adae70a25a588990", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "mimetype": "text/plain", "start_char_idx": 37458, "end_char_idx": 37536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc240197-3cb6-4307-adfa-110a434a6c12", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "original_text": "Extended isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cf7d932-4d04-4f94-ba83-bd75ecb21c6f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "To ensure reproducibility, the complete code used for these experiments is available in our GitHub repository here.\n\n ## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n", "original_text": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. "}, "hash": "8cea93ff341dad9701dddd4a1ea7bcb3f256325aeb948f06d38d3a37f3866d9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f12c950-51f7-4e5b-bb37-e67d9a09aa70", "node_type": "1", "metadata": {"window": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection. ", "original_text": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n"}, "hash": "364bd605781fc096c714a8dc226e053e54659f4b3ba24ba7835934a2fb4b5e82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extended isolation forest. ", "mimetype": "text/plain", "start_char_idx": 37536, "end_char_idx": 37563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f12c950-51f7-4e5b-bb37-e67d9a09aa70", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection. ", "original_text": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc240197-3cb6-4307-adfa-110a434a6c12", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## 5 Conclusion\n\nIn this work, we presented a comparative study of isolation-based anomaly detection methods and introduced two hybrid variants: *Subspace K-Means IF* and *Extended K-Means Isolation Forest*.  These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "original_text": "Extended isolation forest. "}, "hash": "93c4a185015077669360a167ff2684f90c465e029420c197bac1942e95abd931", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c739102-2d59-4f7c-888c-38744104eaa8", "node_type": "1", "metadata": {"window": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n", "original_text": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. "}, "hash": "8b1d8769083809b3f33fbb7da61607d36e0a6bb17accb578217aecc9c946c0a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n", "mimetype": "text/plain", "start_char_idx": 37563, "end_char_idx": 37642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c739102-2d59-4f7c-888c-38744104eaa8", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n", "original_text": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f12c950-51f7-4e5b-bb37-e67d9a09aa70", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "These new methods integrate random projections with clustering to better capture complex, non-linear data distributions.  Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection. ", "original_text": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n"}, "hash": "432b9b080e0b4751d053b0aae9739f1d1d42f950b78bce9cb80755c19c85ef8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42e1a371-4c95-459f-b835-8e79681c746b", "node_type": "1", "metadata": {"window": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "K-means-based isolation forest. "}, "hash": "a813a78af8954aacdf74de5d86b17f229dd68804f8979bda1e947d7738e86954", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. ", "mimetype": "text/plain", "start_char_idx": 37642, "end_char_idx": 37709, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42e1a371-4c95-459f-b835-8e79681c746b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "K-means-based isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c739102-2d59-4f7c-888c-38744104eaa8", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Our experiments show that while the proposed algorithms effectively mitigate the axis-parallel artifacts of Standard IF, this robustness comes with increased computational cost.  Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n", "original_text": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. "}, "hash": "e14bd87026cfd5930747dee31c801639e5d35b385ff5d68170a6d96451a9b4f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "912ac5e1-797b-4f45-84b6-5627acebe1a8", "node_type": "1", "metadata": {"window": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "*Knowledge-based systems*, 195:105659, 2020.\n\n"}, "hash": "52a157437c2415d045e72862fc43e44a876abe239b4f64033246cacfe7851540", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K-means-based isolation forest. ", "mimetype": "text/plain", "start_char_idx": 37709, "end_char_idx": 37741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "912ac5e1-797b-4f45-84b6-5627acebe1a8", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "*Knowledge-based systems*, 195:105659, 2020.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42e1a371-4c95-459f-b835-8e79681c746b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Ultimately, the choice of algorithm depends on the specific trade-off between speed and geometric sensitivity required by the application; Standard IF, EIF and GIF remain ideal for efficiency, while K-Means IF and our hybrid approaches are superior for detecting anomalies in complex manifolds.\n\n ## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "K-means-based isolation forest. "}, "hash": "e24a96d6c335823d8fb3f240da805eda2b9f416745d37c25ba60c02e29e4c533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "703cbd47-12c3-4151-b87e-b692c28926a5", "node_type": "1", "metadata": {"window": "Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. ", "original_text": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. "}, "hash": "e007dd7ddfd11d470ea734cb1408643b238ec324dd512a74cdcef935743e286c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Knowledge-based systems*, 195:105659, 2020.\n\n", "mimetype": "text/plain", "start_char_idx": 37741, "end_char_idx": 37787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "703cbd47-12c3-4151-b87e-b692c28926a5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. ", "original_text": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "912ac5e1-797b-4f45-84b6-5627acebe1a8", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "## References\n\n[1] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "*Knowledge-based systems*, 195:105659, 2020.\n\n"}, "hash": "ee4f89508f686c7359f4b232f15181147fc61e2163c36f57524f628fdb454849", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d8768c1-6d8d-45a1-832c-78a0b52c2c40", "node_type": "1", "metadata": {"window": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n", "original_text": "Generalized isolation forest for anomaly detection. "}, "hash": "f5be30e518bf881ee20634b43bc9b2ce0e8d898e1f2a3b0c081729d3cef29bf1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. ", "mimetype": "text/plain", "start_char_idx": 37787, "end_char_idx": 37862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d8768c1-6d8d-45a1-832c-78a0b52c2c40", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n", "original_text": "Generalized isolation forest for anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "703cbd47-12c3-4151-b87e-b692c28926a5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Extended isolation forest.  *IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. ", "original_text": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret. "}, "hash": "c18e96201bc708de3d03883227f3222a2c2f68d1c3ae2fa45fb295d62580ab42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58df96fc-139c-4edb-9811-fc570bc4f0b0", "node_type": "1", "metadata": {"window": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset.", "original_text": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n"}, "hash": "bbb24b44c271453b0ddeb721973f36e4792d64497ce4dbfca42e00b99cecb9c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generalized isolation forest for anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 37862, "end_char_idx": 37914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58df96fc-139c-4edb-9811-fc570bc4f0b0", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset.", "original_text": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d8768c1-6d8d-45a1-832c-78a0b52c2c40", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "*IEEE transactions on knowledge and data engineering*, 33(4):1479\u20131489, 2019.\n\n [2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n", "original_text": "Generalized isolation forest for anomaly detection. "}, "hash": "4611dea380d86fe91e00c1a183aa6bd37f5996e8f0f4c98b79625dd09766b8e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2fa36f2-5c73-4396-8233-e752f7f69277", "node_type": "1", "metadata": {"window": "K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n", "original_text": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "b10f239f6c13d421025dac20fd986ad6582fc221269f51180f678b5e49b84f32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n", "mimetype": "text/plain", "start_char_idx": 37914, "end_char_idx": 37965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2fa36f2-5c73-4396-8233-e752f7f69277", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n", "original_text": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58df96fc-139c-4edb-9811-fc570bc4f0b0", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "[2] Pawe\u0142 Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al.  K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset.", "original_text": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n"}, "hash": "2db1d2fd4028a7c88ad7eebeb98250f2c7f86d4bb50bbe323fa6e9f8f0eb5fd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd1dc2d1-ff0b-43c9-b522-cc90d3db7047", "node_type": "1", "metadata": {"window": "*Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n", "original_text": "Isolation forest. "}, "hash": "30bba1827e59312bf87f0e19866fd89d11886ff9e17d5db2172ef9bdf7e4d676", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "mimetype": "text/plain", "start_char_idx": 37965, "end_char_idx": 38016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd1dc2d1-ff0b-43c9-b522-cc90d3db7047", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "*Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n", "original_text": "Isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2fa36f2-5c73-4396-8233-e752f7f69277", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "K-means-based isolation forest.  *Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n", "original_text": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "e41d132e9de33538de328965a170e1dd3c51f039ca6073da3d6e56c4eb5ebc63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a77810b-076c-401c-86ad-e581dec200ea", "node_type": "1", "metadata": {"window": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.  ", "original_text": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. "}, "hash": "d917bd3d92fa627d486d93334c3cda062171243cead89b37bd18ebdcc8fbfa3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation forest. ", "mimetype": "text/plain", "start_char_idx": 38016, "end_char_idx": 38034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a77810b-076c-401c-86ad-e581dec200ea", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.  ", "original_text": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd1dc2d1-ff0b-43c9-b522-cc90d3db7047", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "*Knowledge-based systems*, 195:105659, 2020.\n\n [3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n", "original_text": "Isolation forest. "}, "hash": "295b0e4411a4762b0916e4101102f9fbbe149cea7e73be42d312a5896e26cded", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c15fbcb-fded-469e-8baa-c319f82c3737", "node_type": "1", "metadata": {"window": "Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree. ", "original_text": "IEEE, 2008.\n\n"}, "hash": "088b45296c46a92f8a3d80419c95532f3f3501ff33a36d441c5bc388ae707a05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. ", "mimetype": "text/plain", "start_char_idx": 38034, "end_char_idx": 38112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c15fbcb-fded-469e-8baa-c319f82c3737", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree. ", "original_text": "IEEE, 2008.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a77810b-076c-401c-86ad-e581dec200ea", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "[3] Julien Lesouple, C\u00e9dric Baudoin, Marc Spigai, and Jean-Yves Tourneret.  Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.  ", "original_text": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422. "}, "hash": "97b148fd5683cc6029f44b08c2eb6f773dde89500a8cb78287be12302c77d595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90bfb705-a018-4b2c-9b1a-035a46005944", "node_type": "1", "metadata": {"window": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier. ", "original_text": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset."}, "hash": "8351502e52a913bb2267c9f63fa43e596530dd988188e6071e5b19c37ef42436", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE, 2008.\n\n", "mimetype": "text/plain", "start_char_idx": 38112, "end_char_idx": 38125, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90bfb705-a018-4b2c-9b1a-035a46005944", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier. ", "original_text": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c15fbcb-fded-469e-8baa-c319f82c3737", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Generalized isolation forest for anomaly detection.  *Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree. ", "original_text": "IEEE, 2008.\n\n"}, "hash": "66d35ecce80d070d4c49aeff5971a841eb8f69df2cdb5bdd9eff5bf7acb96a84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5fcc287-19af-4304-9d78-d20cd6529801", "node_type": "1", "metadata": {"window": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue). ", "original_text": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n"}, "hash": "d849845eda28d40e9c6f50f7d2a5ccb4cce3c9a001d2f784407a61a6089805ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset.", "mimetype": "text/plain", "start_char_idx": 38125, "end_char_idx": 38268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5fcc287-19af-4304-9d78-d20cd6529801", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue). ", "original_text": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90bfb705-a018-4b2c-9b1a-035a46005944", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "*Pattern Recognition Letters*, 149:109\u2013119, 2021.\n\n [4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier. ", "original_text": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset."}, "hash": "8a2fcc3e2945562793aa00a533ed9435ab18d41949ac9dabae8154e95d3d44fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b86f9a9-827f-42cb-bfd1-d5ea1e91e420", "node_type": "1", "metadata": {"window": "Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n    ", "original_text": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n"}, "hash": "7adc1710b7485e478bf0aec193f3e33b207e87881fb9b58dcf5bcadd8054b41e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n", "mimetype": "text/plain", "start_char_idx": 38268, "end_char_idx": 38453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b86f9a9-827f-42cb-bfd1-d5ea1e91e420", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n    ", "original_text": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5fcc287-19af-4304-9d78-d20cd6529801", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "[4] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue). ", "original_text": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n"}, "hash": "854f83c848e4254fe2dab618d32a3e0b3c867990390d1896cd8be5f59327c96d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e90af6a6-2c6b-4288-a3b5-681bea62bfc1", "node_type": "1", "metadata": {"window": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.  ", "original_text": "- **Columns (Visualizations):**\n    1.  "}, "hash": "90b3bb69add1e2245c7fb219e4fc1f283dfa796173b3d1064eb8b39618d3167a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n", "mimetype": "text/plain", "start_char_idx": 38453, "end_char_idx": 38592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e90af6a6-2c6b-4288-a3b5-681bea62bfc1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.  ", "original_text": "- **Columns (Visualizations):**\n    1.  "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b86f9a9-827f-42cb-bfd1-d5ea1e91e420", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Isolation forest.  In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n    ", "original_text": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n"}, "hash": "4dae0252300061d2b2f1768ad24a7e0c3f04893992c9ed2563f99d235689c1c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c130071b-6944-4ee9-bc30-977d2501c5be", "node_type": "1", "metadata": {"window": "IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). ", "original_text": "**Tree Structure:** A sample isolation tree. "}, "hash": "d33dbbe0a01b262a192afff12b9a1e61cb2a105a29da9b38a134c4afcb31774d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Columns (Visualizations):**\n    1.  ", "mimetype": "text/plain", "start_char_idx": 38592, "end_char_idx": 38632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c130071b-6944-4ee9-bc30-977d2501c5be", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). ", "original_text": "**Tree Structure:** A sample isolation tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e90af6a6-2c6b-4288-a3b5-681bea62bfc1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "In *2008 eighth ieee international conference on data mining*, pages 413\u2013422.  IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.  ", "original_text": "- **Columns (Visualizations):**\n    1.  "}, "hash": "dba3a99f0dcfbdcfe2860109a7198ee08842f8a486edbdc0dc2483ba09c18282", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "940ea1e0-ee92-4c2e-baa2-960084da4891", "node_type": "1", "metadata": {"window": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers. ", "original_text": "Blue paths trace an inlier, red paths trace an outlier. "}, "hash": "8e7cea0b30c37b4d359168235a310fadcc4a12f56a153a9a4627809e7910dff2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Tree Structure:** A sample isolation tree. ", "mimetype": "text/plain", "start_char_idx": 38632, "end_char_idx": 38677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "940ea1e0-ee92-4c2e-baa2-960084da4891", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers. ", "original_text": "Blue paths trace an inlier, red paths trace an outlier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c130071b-6944-4ee9-bc30-977d2501c5be", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "IEEE, 2008.\n\n ---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). ", "original_text": "**Tree Structure:** A sample isolation tree. "}, "hash": "608284ca07de3796b31d1b269e71277e04faf15f39622a7236cdf0cf62ed30c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b062b6ef-d799-4f75-8413-d57edc47bb93", "node_type": "1", "metadata": {"window": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    ", "original_text": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue). "}, "hash": "e0576e4f38e509a8387b7c994e58d01b94ef7411c2bbd6d7311401a128ef02dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Blue paths trace an inlier, red paths trace an outlier. ", "mimetype": "text/plain", "start_char_idx": 38677, "end_char_idx": 38733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b062b6ef-d799-4f75-8413-d57edc47bb93", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    ", "original_text": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "940ea1e0-ee92-4c2e-baa2-960084da4891", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "---\n\n**Figure 5: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'single_blob' dataset. **\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers. ", "original_text": "Blue paths trace an inlier, red paths trace an outlier. "}, "hash": "71e1bbbe53bc8207f4257affd86e8da61bcfb22d2172993b288dfef2c84a58b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fff6571-9389-4b1f-bea3-54b5db07b7aa", "node_type": "1", "metadata": {"window": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.  ", "original_text": "K-Means IF trees are wider and shorter.\n    "}, "hash": "64029a4d3c552d44e3371a759bb16675df8437f0036aac1b4eaa4cbfd6c20c6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue). ", "mimetype": "text/plain", "start_char_idx": 38733, "end_char_idx": 38814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fff6571-9389-4b1f-bea3-54b5db07b7aa", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.  ", "original_text": "K-Means IF trees are wider and shorter.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b062b6ef-d799-4f75-8413-d57edc47bb93", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure is a grid of visualizations, with each row corresponding to a different anomaly detection algorithm and each column showing a different aspect of its internal mechanism.\n - **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    ", "original_text": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue). "}, "hash": "455e954d7d833e3913da635e8416e4b75542ce8c4da574b33364c44b3f5e62a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58f9a934-5085-4916-a3c3-708c356b1583", "node_type": "1", "metadata": {"window": "- **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. ", "original_text": "2.  "}, "hash": "2f80af6ae568abe8aa876cd801dae2b14b79c657956437a59050e7defa84e18f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K-Means IF trees are wider and shorter.\n    ", "mimetype": "text/plain", "start_char_idx": 38814, "end_char_idx": 38858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58f9a934-5085-4916-a3c3-708c356b1583", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. ", "original_text": "2.  "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fff6571-9389-4b1f-bea3-54b5db07b7aa", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Rows (Algorithms):** `Standard IF (PyOD)`, `Generalized IF`, `Extended IF`, `K-Means IF k=0`, `K-Means IF k=1`, `Extended K-Means IF`.\n - **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.  ", "original_text": "K-Means IF trees are wider and shorter.\n    "}, "hash": "bd1de78d27b8df24f035dd43fa67e805a301ef439576a2eef6d38035bfe34da2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dd2dddd-7003-4361-b015-756b85c668b6", "node_type": "1", "metadata": {"window": "**Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines. ", "original_text": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). "}, "hash": "b6a4e76f8df202099c5915a3996db506b19aad4c15fca140c7f8d8a04f00739e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.  ", "mimetype": "text/plain", "start_char_idx": 38858, "end_char_idx": 38862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3dd2dddd-7003-4361-b015-756b85c668b6", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines. ", "original_text": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58f9a934-5085-4916-a3c3-708c356b1583", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Columns (Visualizations):**\n    1.   **Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. ", "original_text": "2.  "}, "hash": "6e9b9caaf5c51dce5ce68751a4b2725f05665c81f1878fe505ce80eb2cfc5aad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d227192-9bd4-4cf7-8d91-1937635c6cdc", "node_type": "1", "metadata": {"window": "Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines. ", "original_text": "Blue dots are inliers, red dots are outliers. "}, "hash": "eb901267c68747be4b9e48b05f06586e030a0ae63ff55f04455a4521f1748a9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). ", "mimetype": "text/plain", "start_char_idx": 38862, "end_char_idx": 38966, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d227192-9bd4-4cf7-8d91-1937635c6cdc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines. ", "original_text": "Blue dots are inliers, red dots are outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dd2dddd-7003-4361-b015-756b85c668b6", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Tree Structure:** A sample isolation tree.  Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines. ", "original_text": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score). "}, "hash": "682d885b9597e1c05045cbab95b1766a673a985d3f6a609c6e60404cda138d35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fff935a3-548e-49e8-9bc3-75d195aab9bc", "node_type": "1", "metadata": {"window": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    ", "original_text": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    "}, "hash": "c792c906b4fec20432120cb3c2114af86c96a83ab97f73206961c88aee09ef83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Blue dots are inliers, red dots are outliers. ", "mimetype": "text/plain", "start_char_idx": 38966, "end_char_idx": 39012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fff935a3-548e-49e8-9bc3-75d195aab9bc", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    ", "original_text": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d227192-9bd4-4cf7-8d91-1937635c6cdc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Blue paths trace an inlier, red paths trace an outlier.  The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines. ", "original_text": "Blue dots are inliers, red dots are outliers. "}, "hash": "becbcd7a92fed7439aa8a6ffc5aaa87cbd97d9909ab4d410b3cf6f4dbba4178e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a666fa5b-2113-430d-a577-9a5fa52090ad", "node_type": "1", "metadata": {"window": "K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.  ", "original_text": "3.  "}, "hash": "f37155c2a591762c02399f481c4672c78efb24e8e86cd5ba2751546072433688", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    ", "mimetype": "text/plain", "start_char_idx": 39012, "end_char_idx": 39126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a666fa5b-2113-430d-a577-9a5fa52090ad", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.  ", "original_text": "3.  "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fff935a3-548e-49e8-9bc3-75d195aab9bc", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The outlier (red) has a much shorter path to a leaf node than the inlier (blue).  K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    ", "original_text": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n    "}, "hash": "4f9af86af615c1db4177a36afdc6e81b6ed0f4c06cf570940739829ddfeab08a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1876d9a-7a5a-4486-8279-18e8ec65babe", "node_type": "1", "metadata": {"window": "2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. ", "original_text": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. "}, "hash": "b131bbd470f1797a0b2f0f20e98fa1888e82e2e37bd654164e06cbcc99093aea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.  ", "mimetype": "text/plain", "start_char_idx": 39126, "end_char_idx": 39130, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c1876d9a-7a5a-4486-8279-18e8ec65babe", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. ", "original_text": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a666fa5b-2113-430d-a577-9a5fa52090ad", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "K-Means IF trees are wider and shorter.\n     2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.  ", "original_text": "3.  "}, "hash": "1975c5c62bb6c2b5d9e3827fcfaca93c8ba45309f2c90cac31ff96a87b049e94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e11fb74-e35a-4700-afed-faf29eed21bf", "node_type": "1", "metadata": {"window": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n", "original_text": "For `Standard IF`, these are axis-parallel lines. "}, "hash": "7f4ab5e1ca69a4641c34838906df8c2e100e22414ba9edbc6e9aaa9f748a779a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. ", "mimetype": "text/plain", "start_char_idx": 39130, "end_char_idx": 39232, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e11fb74-e35a-4700-afed-faf29eed21bf", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n", "original_text": "For `Standard IF`, these are axis-parallel lines. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1876d9a-7a5a-4486-8279-18e8ec65babe", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "2.   **Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. ", "original_text": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier. "}, "hash": "a688afb0c30c4093c42b2fe6be8b64c82846e8eeea9c27d63131325ec63869e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "588d00f8-dbd4-4bec-9f3c-cf6967e14857", "node_type": "1", "metadata": {"window": "Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n", "original_text": "For `Generalized/Extended IF`, they are oblique lines. "}, "hash": "d13c529cd6ad3aa46a3a89db13c1a5de0a3f70684e3602f3e18409a04666ef3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For `Standard IF`, these are axis-parallel lines. ", "mimetype": "text/plain", "start_char_idx": 39232, "end_char_idx": 39282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "588d00f8-dbd4-4bec-9f3c-cf6967e14857", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n", "original_text": "For `Generalized/Extended IF`, they are oblique lines. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e11fb74-e35a-4700-afed-faf29eed21bf", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Radial View:** A scatter plot where points are arranged by angle (random) and radius (anomaly score).  Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n", "original_text": "For `Standard IF`, these are axis-parallel lines. "}, "hash": "a7567b81b2f03119f3b74e05eebe7d9356333482f8bfe606c21c23f39a656208", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cb94fdb-68e0-4ff7-92de-c5df74509f3f", "node_type": "1", "metadata": {"window": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset.", "original_text": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    "}, "hash": "891e42597c5c150d2b1421b848e6d16db5e7b3d340c47332de30275507ef4e0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For `Generalized/Extended IF`, they are oblique lines. ", "mimetype": "text/plain", "start_char_idx": 39282, "end_char_idx": 39337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9cb94fdb-68e0-4ff7-92de-c5df74509f3f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset.", "original_text": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "588d00f8-dbd4-4bec-9f3c-cf6967e14857", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Blue dots are inliers, red dots are outliers.  Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n", "original_text": "For `Generalized/Extended IF`, they are oblique lines. "}, "hash": "e72786ca64ba182b468c2cf40dabcf9a045f6889131c4e9f45e277738b92f802", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d95ac7a-2303-44ae-94a0-3350ed69f1a5", "node_type": "1", "metadata": {"window": "3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n", "original_text": "4.  "}, "hash": "cb24b21f34c6255485c7b22d9cc6115268f3be6cbbe5ad9148d0f7b1b1cea4cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    ", "mimetype": "text/plain", "start_char_idx": 39337, "end_char_idx": 39410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d95ac7a-2303-44ae-94a0-3350ed69f1a5", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n", "original_text": "4.  "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cb94fdb-68e0-4ff7-92de-c5df74509f3f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "Outliers (red) are clustered near the center (high anomaly score), while inliers (blue) are on the periphery.\n     3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset.", "original_text": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n    "}, "hash": "26dc6289cb9543e7ebedb1075f6acb3a4f3b1ec9f220522bf01afe5f063aac6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20b08cfd-d766-46c9-8ba1-62c4a5788966", "node_type": "1", "metadata": {"window": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    ", "original_text": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. "}, "hash": "1d85b5db42571033d8038d0d7f18d61f11ea181cc74fbf85956d6865d16d1f24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.  ", "mimetype": "text/plain", "start_char_idx": 39410, "end_char_idx": 39414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20b08cfd-d766-46c9-8ba1-62c4a5788966", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    ", "original_text": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d95ac7a-2303-44ae-94a0-3350ed69f1a5", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "3.   **Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n", "original_text": "4.  "}, "hash": "9d672c2d52a0efb3251d6bf89a5ef03ece1335b948fc629e496b4c6293897f78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3f85053-c71c-4337-994d-e6eaff8c044b", "node_type": "1", "metadata": {"window": "For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    ", "original_text": "The outlier is isolated with fewer partitions than the inlier.\n"}, "hash": "a600931ff70993b717f28940255b0400f13d66396e9c2661c8a3ac586fc999c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. ", "mimetype": "text/plain", "start_char_idx": 39414, "end_char_idx": 39518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3f85053-c71c-4337-994d-e6eaff8c044b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    ", "original_text": "The outlier is isolated with fewer partitions than the inlier.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20b08cfd-d766-46c9-8ba1-62c4a5788966", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Inlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific inlier.  For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    ", "original_text": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier. "}, "hash": "3b00524037dc2b2f24de2d8a1f18e4b50dc196e5c7fe7dcd286daa7003311b4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d408674-3e7f-40d1-a9a6-25f61a4a8498", "node_type": "1", "metadata": {"window": "For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. ", "original_text": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n"}, "hash": "bf061416ea28a20a41d21e49ab51e19b965a95be021a6af74490364c591db700", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The outlier is isolated with fewer partitions than the inlier.\n", "mimetype": "text/plain", "start_char_idx": 39518, "end_char_idx": 39581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d408674-3e7f-40d1-a9a6-25f61a4a8498", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. ", "original_text": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3f85053-c71c-4337-994d-e6eaff8c044b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For `Standard IF`, these are axis-parallel lines.  For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    ", "original_text": "The outlier is isolated with fewer partitions than the inlier.\n"}, "hash": "ff84192e0ab6dce14df299016b830bb586287a8792c88d71535e8ea846440eba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb439642-199f-419d-938d-46b1d7def6db", "node_type": "1", "metadata": {"window": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n    ", "original_text": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset."}, "hash": "aac2266b2fd33b6adc951ded163eb3bb94b3fc79612c248ff724129b996ad9c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n", "mimetype": "text/plain", "start_char_idx": 39581, "end_char_idx": 39774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb439642-199f-419d-938d-46b1d7def6db", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n    ", "original_text": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d408674-3e7f-40d1-a9a6-25f61a4a8498", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For `Generalized/Extended IF`, they are oblique lines.  For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. ", "original_text": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n"}, "hash": "6e51647acd541ac04effb92a73031b4c5c0133962ecb5653f780e0b22b913b9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb7cb90f-5f21-4ba0-a4a1-08fa4da7d984", "node_type": "1", "metadata": {"window": "4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n    ", "original_text": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n"}, "hash": "00547b842f63341a970f4822817cdced2d1ff01ab6e26091b8d33b45be3ac9d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset.", "mimetype": "text/plain", "start_char_idx": 39774, "end_char_idx": 39912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb7cb90f-5f21-4ba0-a4a1-08fa4da7d984", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n    ", "original_text": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb439642-199f-419d-938d-46b1d7def6db", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "For `K-Means IF`, they are complex polytope/Voronoi-like boundaries.\n     4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n    ", "original_text": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset."}, "hash": "d9e9e4831c34942203fadb6066d5c599259191fe12c35be7d773298f8a084f2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71d1cd57-28f9-4945-82ac-ce31332ec9d8", "node_type": "1", "metadata": {"window": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n", "original_text": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    "}, "hash": "f28c44636639cad5bf32cb44886499979137b64de09fdd47705b05226d418fc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n", "mimetype": "text/plain", "start_char_idx": 39912, "end_char_idx": 40060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71d1cd57-28f9-4945-82ac-ce31332ec9d8", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n", "original_text": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb7cb90f-5f21-4ba0-a4a1-08fa4da7d984", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "4.   **Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n    ", "original_text": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n"}, "hash": "59eafb147ec760f03cf7bd9c1429f09f0f092c0f08cf2f9a86077c52d0a7ab37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8935337-8eb5-480e-b06c-4959ff8e7e96", "node_type": "1", "metadata": {"window": "The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset.", "original_text": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    "}, "hash": "b33d257d47b6d16f9b6705594fcb0aa9a1a50ac762755828e5ce9e6c300b8539", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    ", "mimetype": "text/plain", "start_char_idx": 40060, "end_char_idx": 40177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8935337-8eb5-480e-b06c-4959ff8e7e96", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset.", "original_text": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71d1cd57-28f9-4945-82ac-ce31332ec9d8", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Outlier Path Partitions:** The sequence of separating hyperplanes used to isolate a specific outlier.  The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n", "original_text": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n    "}, "hash": "228ce6df13510d46811a3caa26bde42f47bcf18f1de9fad6e1b20063acb0e6ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1af85981-6caa-4c05-8ab8-d8caae6627e2", "node_type": "1", "metadata": {"window": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n", "original_text": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. "}, "hash": "fb53dcb3cb8dbf000dd49291d142d1810bcb32488f9c20bbd3be5dd750f6eee5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    ", "mimetype": "text/plain", "start_char_idx": 40177, "end_char_idx": 40282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1af85981-6caa-4c05-8ab8-d8caae6627e2", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n", "original_text": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8935337-8eb5-480e-b06c-4959ff8e7e96", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The outlier is isolated with fewer partitions than the inlier.\n - **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset.", "original_text": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n    "}, "hash": "2665cc5d22358e4af97e6733112fc72d4aa6c02a2cc37b4360786f99fe6a5783", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba75a2a8-4624-4c4e-af2b-11f3dea8d338", "node_type": "1", "metadata": {"window": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    ", "original_text": "The tree structures show how the data is split between the two blobs at an early level.\n    "}, "hash": "a3e4e5a910d7c14e708df2424333cc93ab073dce1e2f411232113ee5912dfd8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. ", "mimetype": "text/plain", "start_char_idx": 40282, "end_char_idx": 40430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba75a2a8-4624-4c4e-af2b-11f3dea8d338", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    ", "original_text": "The tree structures show how the data is split between the two blobs at an early level.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1af85981-6caa-4c05-8ab8-d8caae6627e2", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Note:** The last row (`Extended K-Means IF`) states \"Partitions unavailable (Random Projection)\" for the last two columns, as visualizing the partitions in the projected space is complex.\n\n **Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n", "original_text": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions. "}, "hash": "97f45029a344d5485605f0ff560d44af26b8534a0b2c7828b19b445762c53cf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "578c8502-14b8-40d0-9321-7707e664cfed", "node_type": "1", "metadata": {"window": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    ", "original_text": "- The radial view clearly shows two dense rings of inliers.\n    "}, "hash": "8530ebbbe8363b526daf91b5d23af643da55469b63da2c43c7f5a32ef21c619a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The tree structures show how the data is split between the two blobs at an early level.\n    ", "mimetype": "text/plain", "start_char_idx": 40430, "end_char_idx": 40522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "578c8502-14b8-40d0-9321-7707e664cfed", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    ", "original_text": "- The radial view clearly shows two dense rings of inliers.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba75a2a8-4624-4c4e-af2b-11f3dea8d338", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 6: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'double_blob' dataset. **\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    ", "original_text": "The tree structures show how the data is split between the two blobs at an early level.\n    "}, "hash": "a5a7d2ea5cafff2b09ff8ef2bcdd97816199e443b38ee7b5cb65fb6e3e5ead31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daf85a54-8c9f-4aa9-8a09-574c0f4a239b", "node_type": "1", "metadata": {"window": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    ", "original_text": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n"}, "hash": "8dbc08e3e0eb6376af04a1f9527e09a9d9ffd069ae83944009329f70a90ec829", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The radial view clearly shows two dense rings of inliers.\n    ", "mimetype": "text/plain", "start_char_idx": 40522, "end_char_idx": 40586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "daf85a54-8c9f-4aa9-8a09-574c0f4a239b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    ", "original_text": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "578c8502-14b8-40d0-9321-7707e664cfed", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5 but uses the 'double_blob' dataset, which consists of two distinct clusters of normal points.\n - **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    ", "original_text": "- The radial view clearly shows two dense rings of inliers.\n    "}, "hash": "c75662f274fe1579ddc35088af5e75ba08e7565069a8bf9de1a3ebf8663b8dc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7b037e4-7c3e-4d8c-baa2-6619b5132c02", "node_type": "1", "metadata": {"window": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n", "original_text": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset."}, "hash": "8fdf242e4da7f515bbc08d9a58621800e2388c7bd4d01268e7beb53da798344c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n", "mimetype": "text/plain", "start_char_idx": 40586, "end_char_idx": 40671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d7b037e4-7c3e-4d8c-baa2-6619b5132c02", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n", "original_text": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "daf85a54-8c9f-4aa9-8a09-574c0f4a239b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF` struggles to separate the two blobs cleanly with its axis-parallel cuts.\n     - `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    ", "original_text": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n"}, "hash": "40ff977a096be9370b3e025632250ab06f141ce463c2202408cdb9bba978dcf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "511f27c4-0d87-45b0-ab60-9343d508925f", "node_type": "1", "metadata": {"window": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset.", "original_text": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n"}, "hash": "4496db714b13761b6350a3bf214c5782ed7c08c800c165cde967db0dc8cd754b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset.", "mimetype": "text/plain", "start_char_idx": 40671, "end_char_idx": 40808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "511f27c4-0d87-45b0-ab60-9343d508925f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset.", "original_text": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7b037e4-7c3e-4d8c-baa2-6619b5132c02", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` use oblique lines that can more effectively separate the blobs.\n     - The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n", "original_text": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset."}, "hash": "061449df7cc6c9a84bf5ae5d0fdf136a8058170339a34bd2bf7d56f5942f6787", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9e1a143-bdac-4452-afad-87be2b7c16fe", "node_type": "1", "metadata": {"window": "The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. ", "original_text": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    "}, "hash": "9b9942a7b8ae9a52c866b9f7f787716b25774baf03727e3da57c6f8735c93ab6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n", "mimetype": "text/plain", "start_char_idx": 40808, "end_char_idx": 40928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c9e1a143-bdac-4452-afad-87be2b7c16fe", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. ", "original_text": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "511f27c4-0d87-45b0-ab60-9343d508925f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` variants are particularly effective, as their clustering-based partitions naturally identify and separate the two dense regions.  The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset.", "original_text": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n"}, "hash": "0c3041c7fe17eb589b4cf303946b2181243badf7bd0d3cb04e3ad3bc9e33f114", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ff3a39b-c1a1-45bc-be91-cf59220d157d", "node_type": "1", "metadata": {"window": "- The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n", "original_text": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    "}, "hash": "59c58ea3d1c0601ef2fcfe991d28bc71badd0ac0dcb77a1e8448477755479e0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    ", "mimetype": "text/plain", "start_char_idx": 40928, "end_char_idx": 41037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ff3a39b-c1a1-45bc-be91-cf59220d157d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n", "original_text": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9e1a143-bdac-4452-afad-87be2b7c16fe", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The tree structures show how the data is split between the two blobs at an early level.\n     - The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. ", "original_text": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n    "}, "hash": "0aad098c1c1fb9197d7efd8da6c0abdec762d529772ab927a3d1c592fe64ec17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1996c69d-cc39-4ece-ba4d-a8e34eab1b08", "node_type": "1", "metadata": {"window": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    ", "original_text": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    "}, "hash": "3e54b182cd54a4993d1d03efff1ee2a982e7cd5616728ccc8584c0666e1e9491", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    ", "mimetype": "text/plain", "start_char_idx": 41037, "end_char_idx": 41132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1996c69d-cc39-4ece-ba4d-a8e34eab1b08", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    ", "original_text": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ff3a39b-c1a1-45bc-be91-cf59220d157d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The radial view clearly shows two dense rings of inliers.\n     - The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n", "original_text": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n    "}, "hash": "fdf94bf7ec84af7bfba845990e6ea82d76a05376e596cc2e13e651421912ffe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b75f58d2-c38b-4672-9c0f-0272a51273bb", "node_type": "1", "metadata": {"window": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    ", "original_text": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n"}, "hash": "28647419523f89514aaa78dd5c81cc8dbcebd3d252a80104e38a4297ddbd98af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    ", "mimetype": "text/plain", "start_char_idx": 41132, "end_char_idx": 41323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b75f58d2-c38b-4672-9c0f-0272a51273bb", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    ", "original_text": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1996c69d-cc39-4ece-ba4d-a8e34eab1b08", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The partition visualizations show how an inlier deep within one blob is isolated.\n\n **Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    ", "original_text": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n    "}, "hash": "d43f35dd47f2d7bbdfcf61f0275ca0ead7390543398da71aeb2edc0a071f4472", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0454fe10-9d15-4b70-9fe8-b0c0b6c8527a", "node_type": "1", "metadata": {"window": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. ", "original_text": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset."}, "hash": "08d17dd91c656726f6794686cfeee776c2aa57b3520e9bebf4f8d2a6513af818", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n", "mimetype": "text/plain", "start_char_idx": 41323, "end_char_idx": 41471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0454fe10-9d15-4b70-9fe8-b0c0b6c8527a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. ", "original_text": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b75f58d2-c38b-4672-9c0f-0272a51273bb", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 7: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'sinusoidal' dataset. **\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    ", "original_text": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n"}, "hash": "0f91c6660a576daff7130825448f9c661d708d37647193b5c5d7182de56f1c2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c740d709-62c6-4e1f-bf25-d6cc540e4cf4", "node_type": "1", "metadata": {"window": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n", "original_text": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. "}, "hash": "9efed498ad8535f33a898a4e8c2687f017657684d0035ce16b44e35374894691", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset.", "mimetype": "text/plain", "start_char_idx": 41471, "end_char_idx": 41604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c740d709-62c6-4e1f-bf25-d6cc540e4cf4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n", "original_text": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0454fe10-9d15-4b70-9fe8-b0c0b6c8527a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5 but uses a dataset where normal points lie on a sinusoidal curve.\n - **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. ", "original_text": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset."}, "hash": "c08879be9569b1bf86bfe2b6cb890111f254cbe7cc021a22a706b687681a5001", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c2ca715-9c83-4cd3-83e4-5830e4b709c2", "node_type": "1", "metadata": {"window": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset.", "original_text": "This represents a more complex non-linear manifold.\n"}, "hash": "651f816164dd66070621a77bb57e2373297c903e4ecc1decf4a3e9a4cbcf8f64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. ", "mimetype": "text/plain", "start_char_idx": 41604, "end_char_idx": 41701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c2ca715-9c83-4cd3-83e4-5830e4b709c2", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset.", "original_text": "This represents a more complex non-linear manifold.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c740d709-62c6-4e1f-bf25-d6cc540e4cf4", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF`'s axis-parallel cuts create a blocky approximation of the curve.\n     - `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n", "original_text": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution. "}, "hash": "a714aec44a0c5bf424eab5d527da6b8cac3f8588ff3c00760b9e55f9e9f19245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "494d0361-5f62-4ce7-9e9a-6037c62f0c13", "node_type": "1", "metadata": {"window": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n", "original_text": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    "}, "hash": "2d90297ab72b25914f95d1ba74c5504ed5de6da5ff2ee65e83e9515e4fd17eb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This represents a more complex non-linear manifold.\n", "mimetype": "text/plain", "start_char_idx": 41701, "end_char_idx": 41753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "494d0361-5f62-4ce7-9e9a-6037c62f0c13", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n", "original_text": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c2ca715-9c83-4cd3-83e4-5830e4b709c2", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` use oblique lines to better follow the curve's shape.\n     - The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset.", "original_text": "This represents a more complex non-linear manifold.\n"}, "hash": "17fc2132ce8dc3400a7e328c40ec387f2dd5780ed3a5482dd0998f54af0668c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e67dd1a-8efe-47e9-8063-df6b810a01e3", "node_type": "1", "metadata": {"window": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    ", "original_text": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    "}, "hash": "ec684a2080f39428d4a20885610f874f61bc17ade98a5aadb36d14340eaa3699", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    ", "mimetype": "text/plain", "start_char_idx": 41753, "end_char_idx": 41880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e67dd1a-8efe-47e9-8063-df6b810a01e3", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    ", "original_text": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "494d0361-5f62-4ce7-9e9a-6037c62f0c13", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` variants, especially `k=1` (Subspace), create partitions that more closely hug the sinusoidal manifold, resulting in a more accurate isolation of inliers and outliers.\n     - An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n", "original_text": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n    "}, "hash": "6357e9bb16a414f35f337e74fd8feaac9457f5a9bd74bf86867aeb17fabc670d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9600900c-209d-4551-955d-c12e3ddda99f", "node_type": "1", "metadata": {"window": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n    ", "original_text": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. "}, "hash": "607b1df9dd82e2a7754d722afac051113b6a8c3cd23fc92e03b6d2c4120b6376", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    ", "mimetype": "text/plain", "start_char_idx": 41880, "end_char_idx": 41994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9600900c-209d-4551-955d-c12e3ddda99f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n    ", "original_text": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e67dd1a-8efe-47e9-8063-df6b810a01e3", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- An outlier far from the curve is isolated very quickly (short red path), while an inlier on the curve requires many partitions (long blue path).\n\n **Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    ", "original_text": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n    "}, "hash": "c1599eaba7ac30e97245e00a7899dc42db7a6f1977e69b91b53c9c3c94c55782", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a34b95cb-bb1a-4ffe-ab03-7aebb16f2649", "node_type": "1", "metadata": {"window": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    ", "original_text": "The partitions attempt to follow the curve of the spiral arms.\n\n"}, "hash": "3e8c49d98da2a8257e97ba030b8688f74074386829f29efd5c0a81a465ee0871", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. ", "mimetype": "text/plain", "start_char_idx": 41994, "end_char_idx": 42175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a34b95cb-bb1a-4ffe-ab03-7aebb16f2649", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    ", "original_text": "The partitions attempt to follow the curve of the spiral arms.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9600900c-209d-4551-955d-c12e3ddda99f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 8: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spiral' dataset. **\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n    ", "original_text": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods. "}, "hash": "17ce8005e60e8ed45d6237724d3cc506ef470a43592b94bcb898614c43e19290", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a770b126-4503-4589-82bb-195c73bee83f", "node_type": "1", "metadata": {"window": "This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n", "original_text": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset."}, "hash": "f31cefd6d6d534c0019aa2a4e4692eec0181948d5bef7ad9201cdaf46b9a1d18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The partitions attempt to follow the curve of the spiral arms.\n\n", "mimetype": "text/plain", "start_char_idx": 42175, "end_char_idx": 42239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a770b126-4503-4589-82bb-195c73bee83f", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n", "original_text": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a34b95cb-bb1a-4ffe-ab03-7aebb16f2649", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5 but for a spiral-shaped data distribution.  This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    ", "original_text": "The partitions attempt to follow the curve of the spiral arms.\n\n"}, "hash": "860e0fc880f8b3bec9ffbc043069936300b5f8d0a3f6fe572178fe207592645b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d5bf85d-0352-439a-9d77-48eb65591a7b", "node_type": "1", "metadata": {"window": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset.", "original_text": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n"}, "hash": "28c1c7db3930e4ff991e8ccd2b5ec3fcbed086c0610009a78f3da88fbc7f0d05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset.", "mimetype": "text/plain", "start_char_idx": 42239, "end_char_idx": 42399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d5bf85d-0352-439a-9d77-48eb65591a7b", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset.", "original_text": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a770b126-4503-4589-82bb-195c73bee83f", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "This represents a more complex non-linear manifold.\n - **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n", "original_text": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset."}, "hash": "c5b3c9db4dc9b60cf881c702297eab37c1ae9ea30f7874c54fb56df678c37847", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69e4e920-f833-4ec9-a6e3-f768d4fda22a", "node_type": "1", "metadata": {"window": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n", "original_text": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    "}, "hash": "b047b4b2d85e87a977033d329b32f95df8912710cb1cd2da658e466a4a382506", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n", "mimetype": "text/plain", "start_char_idx": 42399, "end_char_idx": 42516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69e4e920-f833-4ec9-a6e3-f768d4fda22a", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n", "original_text": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d5bf85d-0352-439a-9d77-48eb65591a7b", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - `Standard IF` is ineffective, as its rectangular partitions cannot capture the spiral structure.\n     - `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset.", "original_text": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n"}, "hash": "5187c68ece4b968c1b926606946a62a67bca93f9a0ae737478117afb287e6dae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91297eb0-7166-46f8-bc45-3d7813e00738", "node_type": "1", "metadata": {"window": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9. ", "original_text": "- `Standard IF` creates a coarse, blocky grid around the data.\n    "}, "hash": "497fb3ee9dfa420a702379639c1eeac54679126de44906b04c0f1e3e578dc6cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    ", "mimetype": "text/plain", "start_char_idx": 42516, "end_char_idx": 42621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91297eb0-7166-46f8-bc45-3d7813e00738", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9. ", "original_text": "- `Standard IF` creates a coarse, blocky grid around the data.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69e4e920-f833-4ec9-a6e3-f768d4fda22a", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Generalized IF` and `Extended IF` perform better but still struggle with the high curvature of the spiral.\n     - The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n", "original_text": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n    "}, "hash": "da5a03261d03c7eba3745f4a3e63e1aa6250c949e282c4bbd75803d9d7f8eff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f35a032-e7a2-4188-9e2f-b448a347625c", "node_type": "1", "metadata": {"window": "The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate. ", "original_text": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    "}, "hash": "f2c4ef5bb0770e47f97a07bf12f45a38576973cbe1d47ffd4aeeeb2978bb4dc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Standard IF` creates a coarse, blocky grid around the data.\n    ", "mimetype": "text/plain", "start_char_idx": 42621, "end_char_idx": 42688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f35a032-e7a2-4188-9e2f-b448a347625c", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate. ", "original_text": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91297eb0-7166-46f8-bc45-3d7813e00738", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` variants show a better ability to adapt their partitions to the local density of the spiral, although it remains a very challenging distribution for all methods.  The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9. ", "original_text": "- `Standard IF` creates a coarse, blocky grid around the data.\n    "}, "hash": "7bd61f42aa8495030bc43e62a27465ad09267d2cd27b4860262f7506c06b47e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f79b0bf6-f8e2-4def-b222-4589b72b6c09", "node_type": "1", "metadata": {"window": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement. ", "original_text": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n"}, "hash": "e7bef55b8070fbfc65baeef38a695c407c4dd2503a85e4c9762da8dff18671f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    ", "mimetype": "text/plain", "start_char_idx": 42688, "end_char_idx": 42812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f79b0bf6-f8e2-4def-b222-4589b72b6c09", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement. ", "original_text": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f35a032-e7a2-4188-9e2f-b448a347625c", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "The partitions attempt to follow the curve of the spiral arms.\n\n **Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate. ", "original_text": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n    "}, "hash": "9c417fd1622721c300fabfd89a31badf2b47e7ee797b7cd5d952080166bc2cad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efbc70ae-e7f3-431d-843f-e162c582332d", "node_type": "1", "metadata": {"window": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "**Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset."}, "hash": "3e9b6ce4f545bad3dbf37e3ad104599cfa077e73885041606447fa239226a491", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n", "mimetype": "text/plain", "start_char_idx": 42812, "end_char_idx": 42990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "efbc70ae-e7f3-431d-843f-e162c582332d", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "**Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f79b0bf6-f8e2-4def-b222-4589b72b6c09", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 9: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=3_0_climb_speed=1_5' dataset. **\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement. ", "original_text": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n"}, "hash": "402947d64f9216d8a650f38fe705873dc6157abb6eadf25731eee56b1c7ded15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "472b0b33-96eb-460a-9ff9-9f0dba18aaf1", "node_type": "1", "metadata": {"window": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "**\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n"}, "hash": "9729d701bfa89f2b6a22206c0865ddf259a696f2f2d5d76f99d7fe8ab87a97ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset.", "mimetype": "text/plain", "start_char_idx": 42990, "end_char_idx": 43151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "472b0b33-96eb-460a-9ff9-9f0dba18aaf1", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "**\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efbc70ae-e7f3-431d-843f-e162c582332d", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**\nThis figure follows the same structure as Figure 5, using a 3D helical (spring-like) structure projected onto 2D.\n - **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "**Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset."}, "hash": "7ae066e0689613342fadf5079064cc768f667985b2f51b5c64b42ce003ce2e9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d588c0d-a781-470d-acda-35bc584c31b2", "node_type": "1", "metadata": {"window": "- `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "- **Observations:** The trends are similar to Figure 9. "}, "hash": "1740d5ce011b9dbacf50dda8a581cc7fc7fd9e3e58bc61b45f92c85b5c7c49ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n", "mimetype": "text/plain", "start_char_idx": 43151, "end_char_idx": 43300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d588c0d-a781-470d-acda-35bc584c31b2", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "- **Observations:** The trends are similar to Figure 9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "472b0b33-96eb-460a-9ff9-9f0dba18aaf1", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- **Observations:**\n    - This dataset is complex due to its overlapping structure in 2D projection.\n     - `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "**\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n"}, "hash": "e13759bf889d9b8025bd23005638fcc486712a762e1e317266288b09c0b20ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "277de77c-e221-4e47-b253-290af4185f53", "node_type": "1", "metadata": {"window": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The axis-parallel cuts of `Standard IF` are clearly inadequate. "}, "hash": "e12edf843a5157ddc36813c2e232e83515995d39822f9ffe60f66ee15344f71f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- **Observations:** The trends are similar to Figure 9. ", "mimetype": "text/plain", "start_char_idx": 43300, "end_char_idx": 43356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "277de77c-e221-4e47-b253-290af4185f53", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The axis-parallel cuts of `Standard IF` are clearly inadequate. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d588c0d-a781-470d-acda-35bc584c31b2", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Standard IF` creates a coarse, blocky grid around the data.\n     - `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "- **Observations:** The trends are similar to Figure 9. "}, "hash": "f607f6c60d4c752928f1fa964696dad8c950c6bb2af0fa9a02dd8bf40f586f16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ecaee03-ed5f-41e4-ac73-bb6d4b5915e7", "node_type": "1", "metadata": {"window": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The oblique cuts of `EIF/GIF` are an improvement. "}, "hash": "0bac0201e2bbc0d2aa0b22802afe662ac10c9373b661e042d4a90467efde947e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The axis-parallel cuts of `Standard IF` are clearly inadequate. ", "mimetype": "text/plain", "start_char_idx": 43356, "end_char_idx": 43420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ecaee03-ed5f-41e4-ac73-bb6d4b5915e7", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The oblique cuts of `EIF/GIF` are an improvement. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "277de77c-e221-4e47-b253-290af4185f53", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- `Extended IF` and `Generalized IF` use slanted cuts that are somewhat better at isolating the dense spring structure.\n     - The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The axis-parallel cuts of `Standard IF` are clearly inadequate. "}, "hash": "7d075a84e0a07f588dba52160e249b7c0d55c4838f176ddf6904ebdd178eca84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "900d7e68-86f6-4da7-ae72-fd438dd30ed4", "node_type": "1", "metadata": {"window": "**Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold."}, "hash": "b88489e1470e2ee42e4260582d70e7ab290ef629851fba503dddfeca39a7d4e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The oblique cuts of `EIF/GIF` are an improvement. ", "mimetype": "text/plain", "start_char_idx": 43420, "end_char_idx": 43470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "900d7e68-86f6-4da7-ae72-fd438dd30ed4", "embedding": null, "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "**Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "910846ca-badb-47fa-bd30-b0a0d37ddcf2", "node_type": "4", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf"}, "hash": "98903f3e8f60f7d74d0dd29ea99feb86589c49356dca823b6839f73cedae5df1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ecaee03-ed5f-41e4-ac73-bb6d4b5915e7", "node_type": "1", "metadata": {"title": "Extended K-Means Isolation Forest", "authors": "Vlad Birsan", "year": 2025, "file_path": "ad-papers-pdf/extended_kmeans_isolation_forest.pdf", "window": "- The `K-Means IF` methods, by being density-aware, create partitions that follow the path of the spring, demonstrating superior performance in capturing this complex manifold.\n\n **Figure 10: Detailed visualization of the internal mechanisms for each anomaly detection method, exemplified on the 'spring-radius=4_0_climb_speed=2_5' dataset. **\nThis figure is similar to Figure 9 but with different parameters for the spring dataset, resulting in a wider, more spread-out helical structure.\n - **Observations:** The trends are similar to Figure 9.  The axis-parallel cuts of `Standard IF` are clearly inadequate.  The oblique cuts of `EIF/GIF` are an improvement.  The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "original_text": "The oblique cuts of `EIF/GIF` are an improvement. "}, "hash": "4258795b51da5b5b825e50270a4ac83b11f15f075a8b5fcb7c0660777c16302a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The clustering-based, density-aware partitions of the `K-Means IF` variants provide the most geometrically faithful isolation boundaries, adapting well to the shape of the data manifold.", "mimetype": "text/plain", "start_char_idx": 43470, "end_char_idx": 43656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]